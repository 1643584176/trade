import MetaTrader5 as mt5
import pandas as pd
import numpy as np
import xgboost as xgb
import pickle
import sys
import os
from datetime import datetime, timedelta, timezone
import warnings
import importlib.util
from sklearn.utils import resample
warnings.filterwarnings('ignore')

# æ·»åŠ å…¬å…±æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "common"))

# åŠ¨æ€å¯¼å…¥åŸºç±»
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
model_trainer_base_path = os.path.join(project_root, 'mlc', 'm5', 'model_trainer_base.py')
spec = importlib.util.spec_from_file_location("model_trainer_base", model_trainer_base_path)
model_trainer_base = importlib.util.module_from_spec(spec)
spec.loader.exec_module(model_trainer_base)
BaseModelTrainer = model_trainer_base.BaseModelTrainer

# é…ç½®å‚æ•°
class ModelConfig:
    SYMBOL = "XAUUSD"
    M5_TIMEFRAME = mt5.TIMEFRAME_M5
    HISTORY_M5_BARS = 120  # ç”¨äºé¢„æµ‹çš„Kçº¿æ•°é‡
    PREDICT_FUTURE_BARS = 3  # é¢„æµ‹æœªæ¥Kçº¿æ•°é‡
    TRAIN_TEST_SPLIT = 0.8
    MODEL_SAVE_PATH = "xauusd_m5_model.json"  # XGBoostæ¨¡å‹ä¿å­˜è·¯å¾„
    SCALER_SAVE_PATH = "m5_scaler.pkl"
    UTC_TZ = timezone.utc

class M5ModelTrainer(BaseModelTrainer):
    def __init__(self):
        super().__init__()
        self.config = ModelConfig()
    
    def prepare_features_and_target(self, df, timeframe_type="M5"):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡ - é‡å†™ä»¥åˆ é™¤é‡å¤çš„tick_volumeç‰¹å¾"""
        # M5å‘¨æœŸç‰¹å¾ï¼ˆä¸»è¦å†³ç­–ï¼‰
        feature_columns = [
            # M5å‘¨æœŸç‰¹å¾ï¼ˆä¸»è¦å†³ç­–ï¼‰
            'open', 'high', 'low', 'close', 'tick_volume',  # ä¿ç•™ä¸€ä¸ªtick_volume
            'price_position', 'volatility_pct',
            'm15_trend', 'm30_support', 'm30_resistance',
            'volatility_change', 'tick_density',
            # Kçº¿å½¢æ€ç‰¹å¾
            'hammer', 'shooting_star', 'engulfing',
            # æŠ€æœ¯æŒ‡æ ‡
            'rsi_14', 'macd', 'macd_signal', 'macd_hist',
            'bollinger_position',  # ä¿ç•™ä½ç½®ç‰¹å¾ï¼Œç§»é™¤æœªå®ç°çš„ä¸Šä¸‹è½¨
            'ma5', 'ma10', 'ma20', 'ma5_direction', 'ma10_direction', 'ma20_direction',
            # ä¸€è‡´æ€§ç‰¹å¾
            'rsi_price_consistency',
            # è·¨å‘¨æœŸç‰¹å¾
            'rsi_divergence', 'vol_short_vs_medium', 'vol_medium_vs_long', 'vol_short_vs_long',
            'trend_consistency',
            # ä¿¡å·ç‰¹å¾
            'rsi_signal_strength', 'macd_signal_strength', 'short_long_signal_consistency',
            # é£é™©ç‰¹å¾
            'volatility_regime', 'vol_cluster',
            # M5ä¸“ç”¨å‘¨æœŸå…±æŒ¯ç‰¹å¾
            'm15_trend_ma_consistency',  # M15è¶‹åŠ¿ä¸M5å‡çº¿ä¸€è‡´æ€§
            'm5_m1_volume_correlation',  # M5ä¸M1æˆäº¤é‡è”åŠ¨
            'trend_strength_m5_m15',  # M5ä¸M15è¶‹åŠ¿å¼ºåº¦æ¯”
            'cycle_alignment_score',  # å‘¨æœŸå¯¹é½è¯„åˆ†
            # æ–°å¢è·¨å‘¨æœŸè”åŠ¨ç‰¹å¾
            'm5_m15_volume_correlation',  # M5ä¸M15çš„volume_correlation
            'volatility_diff_m5_m1',  # M5ä¸M1çš„volatility_pctå·®å€¼
            # è¶‹åŠ¿å¼ºåº¦ç‰¹å¾
            'adx',  # ADXæŒ‡æ ‡ï¼ˆè¶‹åŠ¿å¼ºåº¦ï¼‰
            'ma5_ma20_alignment',  # MA5ä¸MA20æ–¹å‘ä¸€è‡´æ€§
            # æ¶¨è·ŒåŠ¨èƒ½ç‰¹å¾
            'momentum_3',  # 3æ ¹Kçº¿çš„æ¶¨è·Œå¹…ä¹‹å’Œ
            'momentum_5',  # 5æ ¹Kçº¿çš„æ¶¨è·Œå¹…ä¹‹å’Œ
            'volume_price_divergence',  # æˆäº¤é‡ä¸ä»·æ ¼èƒŒç¦»
            'consecutive_up',  # è¿ç»­ä¸Šæ¶¨æ¬¡æ•°
            'consecutive_down',  # è¿ç»­ä¸‹è·Œæ¬¡æ•°
            'body_strength',  # Kçº¿å®ä½“å¼ºåº¦
            'upper_shadow',  # ä¸Šå½±çº¿å¼ºåº¦
            'lower_shadow',  # ä¸‹å½±çº¿å¼ºåº¦
            'price_position_5',  # ä»·æ ¼åœ¨çŸ­æœŸé«˜ä½ç‚¹ä¸­çš„ä½ç½®
            # åŠ¨æ€æ´»è·ƒåº¦ç‰¹å¾
            'dynamic_activity',  # åŠ¨æ€æ´»è·ƒåº¦
            'activity_level',  # æ´»è·ƒåº¦ç­‰çº§
            # è·Œç±»ä¸“å±ç‰¹å¾
            'volume_up_ratio',  # tick_volumeæ”¾é‡ä¸‹è·Œå æ¯”
            'atr_down_prob',  # ATR14æ‰©å¼ æ—¶çš„ä¸‹è·Œæ¦‚ç‡
            # æ ¸å¿ƒç‰¹å¾ï¼ˆæ¸…ç†é‡å¤ç‰¹å¾åï¼‰
            'atr_14',  # æ ¸å¿ƒATRç‰¹å¾ - ä¿ç•™é«˜æƒé‡ç‰ˆæœ¬
            'hl_ratio',  # æ ¸å¿ƒé«˜ä½ä»·æ¯”å€¼ - ä¿ç•™é«˜æƒé‡ç‰ˆæœ¬
            # ç¡®ä¿ä»…ä¿ç•™ä¸€ä¸ªtick_volumeç‰¹å¾ï¼Œç§»é™¤ä»»ä½•é‡å¤çš„æˆäº¤é‡ç‰¹å¾
            # æ–°å¢dynamic_activityç‰¹å¾ï¼ˆå½“å‰5åˆ†é’Ÿæ³¢åŠ¨ç‡/è¿‡å»24å°æ—¶åŒå‘¨æœŸå‡å€¼ï¼‰
            # åˆ é™¤é‡å¤ç‰¹å¾ï¼šå½»åº•æ¸…ç†é‡å¤çš„tick_volumeç‰¹å¾ï¼Œä»…ä¿ç•™é«˜æƒé‡ç‰ˆæœ¬
            # åˆ é™¤é‡å¤çš„dynamic_activityç‰¹å¾
            # æ³¨æ„ï¼švolatility_pctå·²åœ¨å‰é¢å®šä¹‰ï¼Œæ­¤å¤„ä¸å†é‡å¤
        ]
        
        # æ£€æŸ¥æ‰€æœ‰ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
        available_features = []
        for col in feature_columns:
            if col in df.columns:
                available_features.append(col)
            else:
                print(f"è­¦å‘Š: ç‰¹å¾åˆ— '{col}' ä¸å­˜åœ¨")
        
        X = df[available_features].values
        y = df['target'].values
        
        return X, y, available_features
    
    def get_m5_historical_data(self, bars_count: int = 9000):  # å¢åŠ æ•°æ®é‡è‡³1.5å¹´
        """è·å–MT5çœŸå®å†å²M5æ•°æ®"""
        self.initialize_mt5()
        
        # è·å–å½“å‰æ—¶é—´
        current_utc = datetime.now(self.config.UTC_TZ)
        start_time = current_utc - timedelta(minutes=5*bars_count)
        
        # ä½¿ç”¨mt5.copy_rates_from_posæŒ‰Kçº¿æ•°é‡è·å–æ•°æ®
        m5_rates = mt5.copy_rates_from_pos(
            self.config.SYMBOL,
            self.config.M5_TIMEFRAME,
            0,  # ä»æœ€æ–°çš„Kçº¿å¼€å§‹è·å–
            bars_count  # è·å–æŒ‡å®šæ•°é‡çš„Kçº¿
        )
        
        if m5_rates is None or len(m5_rates) == 0:
            raise Exception(f"è·å–M5å†å²æ•°æ®å¤±è´¥ï¼š{mt5.last_error()}")
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(m5_rates)
        df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
        df.set_index('time', inplace=True)
        
        # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥ - æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
        time_diff = df.index.to_series().diff().dt.total_seconds().dropna()
        if not (time_diff == 300).all():  # M5å‘¨æœŸé¢„æœŸé—´éš”300ç§’
            print("è­¦å‘Š: æ•°æ®å­˜åœ¨æ—¶é—´æ–­è¿ï¼Œå¯èƒ½å½±å“ç‰¹å¾è®¡ç®—")
        
        # å‡†å¤‡æ•°æ®å’Œç‰¹å¾
        df = self.prepare_data_with_features(m5_rates, "M5")
        
        # æ·»åŠ å¢å¼ºç‰¹å¾
        df = self.feature_engineer.add_enhanced_features(df)
        
        # æ·»åŠ M5ä¸“ç”¨çš„å‘¨æœŸå…±æŒ¯ç‰¹å¾
        df = self.add_cycle_resonance_features(df)
        
        # æ·»åŠ åŠ¨æ€æ´»è·ƒåº¦ç‰¹å¾
        df = self.calculate_dynamic_activity(df)
        
        # åˆ›å»ºç›®æ ‡å˜é‡ï¼šé¢„æµ‹æœªæ¥3æ ¹Kçº¿çš„æ¶¨è·Œ (1=æ¶¨, 0=è·Œ, -1=å¹³)
        df['future_close_1'] = df['close'].shift(-1)  # é¢„æµ‹1æ ¹Kçº¿å
        df['future_close_2'] = df['close'].shift(-2)  # é¢„æµ‹2æ ¹Kçº¿å
        df['future_close_3'] = df['close'].shift(-3)  # é¢„æµ‹3æ ¹Kçº¿å
        
        # ä½¿ç”¨é¢„æµ‹æœªæ¥3æ ¹Kçº¿çš„å¹³å‡æ¶¨è·Œå¹…ä½œä¸ºç›®æ ‡
        df['future_avg_close'] = (df['future_close_1'] + df['future_close_2'] + df['future_close_3']) / 3
        df['price_change_pct'] = (df['future_avg_close'] - df['close']) / df['close']
        
        # å¼‚å¸¸å€¼å¤„ç† - æ£€æµ‹ä»·æ ¼è·³ç©º
        df['gap_pct'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)
        atr_14 = self.calculate_atr(df['high'], df['low'], df['close'], 14)
        df['atr_14'] = atr_14
        df = df[abs(df['gap_pct']) < 3 * atr_14]  # è¿‡æ»¤æç«¯è·³ç©º
        
        # é‡æ–°è®¡ç®—price_change_pctåœ¨è¿‡æ»¤å¼‚å¸¸å€¼ä¹‹å
        df['future_close'] = df['close'].shift(-1)
        df['price_change_pct'] = (df['future_close'] - df['close']) / df['close']
        
        # è°ƒæ•´æ¶¨è·Œåˆ¤å®šé˜ˆå€¼ï¼šä»å½“å‰çš„0.01%æè‡³0.015%
        base_threshold = 0.0015  # è°ƒæ•´ååŸºç¡€é˜ˆå€¼ï¼ˆ0.15%ï¼‰
        dynamic_threshold_series = base_threshold - np.minimum(0.0002, atr_14 * 0.015)  # æ³¢åŠ¨ç‡è¶Šé«˜ï¼Œé˜ˆå€¼è¶Šä½ï¼ˆæœ€ä½0.0013ï¼‰
        
        # ç¡®ä¿dynamic_threshold_seriesä¸price_change_pctç´¢å¼•ä¸€è‡´
        dynamic_threshold_series = dynamic_threshold_series.reindex(df['price_change_pct'].index, fill_value=base_threshold)
        
        # å®šä¹‰ç›®æ ‡å˜é‡ - è€ƒè™‘XAUUSDçš„ç‚¹å·®å’ŒåŠ¨æ€é˜ˆå€¼
        # è°ƒæ•´é˜ˆå€¼ä»¥å¹³è¡¡ç±»åˆ«åˆ†å¸ƒï¼ŒXAUUSDç‚¹å·®çº¦ä¸º0.05ï¼Œè®¾ç½®åˆç†é˜ˆå€¼é¿å…è¿‡å¤š'å¹³'ç±»æ ·æœ¬
        df['target'] = np.where(df['price_change_pct'] > dynamic_threshold_series, 1,  # æ¶¨
                               np.where(df['price_change_pct'] < -dynamic_threshold_series, -1, 0))  # è·Œå’Œå¹³
        
        # æ£€æŸ¥å¹¶æŠ¥å‘Šç±»åˆ«åˆ†å¸ƒ
        unique, counts = np.unique(df['target'], return_counts=True)
        class_dist = dict(zip(unique, counts))
        print(f"ç›®æ ‡å˜é‡ç±»åˆ«åˆ†å¸ƒ: {class_dist}")
        
        # å¦‚æœ'å¹³'ç±»æ ·æœ¬å æ¯”è¿‡é«˜ï¼Œè°ƒæ•´é˜ˆå€¼
        if 0 in class_dist:
            flat_ratio = class_dist[0] / len(df['target'])
            if flat_ratio > 0.8:  # å¦‚æœ'å¹³'ç±»å æ¯”è¶…è¿‡80%
                print(f"è­¦å‘Š: 'å¹³'ç±»æ ·æœ¬å æ¯”è¿‡é«˜ ({flat_ratio:.2%})ï¼Œæ­£åœ¨è°ƒæ•´é˜ˆå€¼...")
                # é™ä½é˜ˆå€¼ä»¥å‡å°‘'å¹³'ç±»æ ·æœ¬æ¯”ä¾‹
                adjusted_threshold = dynamic_threshold_series * 0.7  # é™ä½é˜ˆå€¼
                df['target'] = np.where(df['price_change_pct'] > adjusted_threshold, 1,  # æ¶¨
                                       np.where(df['price_change_pct'] < -adjusted_threshold, -1, 0))  # è·Œå’Œå¹³
                
                # é‡æ–°æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
                unique, counts = np.unique(df['target'], return_counts=True)
                class_dist = dict(zip(unique, counts))
                print(f"è°ƒæ•´åç›®æ ‡å˜é‡ç±»åˆ«åˆ†å¸ƒ: {class_dist}")
        
        return df
    
    def calculate_dynamic_activity(self, df):
        """è®¡ç®—åŠ¨æ€æ´»è·ƒåº¦ç‰¹å¾ - ä¼˜åŒ–è®¡ç®—å‘¨æœŸ"""
        # è®¡ç®—çŸ­æœŸæ³¢åŠ¨ç‡ï¼ˆæœ€è¿‘3æ ¹M5æ³¢åŠ¨ç‡ï¼‰- å¹³æ»‘çŸ­æœŸæ³¢åŠ¨
        df['volatility_short'] = df['close'].pct_change().rolling(window=3).std()  # 3æ ¹M5æ³¢åŠ¨ç‡
        
        # è®¡ç®—é•¿æœŸæ³¢åŠ¨ç‡ï¼ˆè¿‡å»24å°æ—¶å¹³å‡æ³¢åŠ¨ç‡ï¼‰
        df['volatility_long_avg'] = df['volatility_short'].rolling(window=288, min_periods=24).mean()  # 24å°æ—¶=288ä¸ªM5å‘¨æœŸ
        
        # è®¡ç®—åŠ¨æ€æ´»è·ƒåº¦ï¼ˆçŸ­æœŸæ³¢åŠ¨ç‡/é•¿æœŸå¹³å‡æ³¢åŠ¨ç‡ï¼‰
        df['dynamic_activity'] = df['volatility_short'] / (df['volatility_long_avg'] + 1e-8)
        
        # åˆ›å»ºæ´»è·ƒåº¦åˆ†ç±»ï¼ˆé«˜/ä¸­/ä½æ´»è·ƒåº¦ï¼‰
        df['activity_level'] = 1  # é»˜è®¤ä¸ºä¸­ç­‰æ´»è·ƒåº¦
        df.loc[df['dynamic_activity'] > 1.2, 'activity_level'] = 2  # é«˜æ´»è·ƒåº¦
        df.loc[df['dynamic_activity'] < 0.8, 'activity_level'] = 0  # ä½æ´»è·ƒåº¦
        
        return df
    
    def add_cycle_resonance_features(self, df):
        """ä¸ºM5æ•°æ®æ·»åŠ å‘¨æœŸå…±æŒ¯ç‰¹å¾"""
        # M15è¶‹åŠ¿ä¸M5å‡çº¿ä¸€è‡´æ€§
        if 'm15_trend' in df.columns and 'ma5' in df.columns:
            df['m15_trend_ma_consistency'] = np.where(
                ((df['m15_trend'] > 0) & (df['ma5_direction'] > 0)) | 
                ((df['m15_trend'] < 0) & (df['ma5_direction'] < 0)), 1, -1)
        else:
            df['m15_trend_ma_consistency'] = 0
        
        # M5ä¸M1æˆäº¤é‡è”åŠ¨ï¼ˆä½¿ç”¨tick_volumeä½œä¸ºä»£ç†ï¼‰
        df['m5_m1_volume_correlation'] = df['tick_volume'].rolling(window=5).corr(
            df['tick_volume'].shift(5)  # ä½¿ç”¨æ»å5æœŸçš„æˆäº¤é‡ä½œä¸ºM1çš„ä»£ç†
        ).fillna(0)
        
        # M5ä¸M15è¶‹åŠ¿å¼ºåº¦æ¯”
        if 'm15_trend' in df.columns:
            df['trend_strength_m5_m15'] = abs(df['ma5_direction']) / (abs(df['m15_trend']) + 1e-8)
        else:
            df['trend_strength_m5_m15'] = abs(df['ma5_direction'])
        
        # å‘¨æœŸå¯¹é½è¯„åˆ†ï¼ˆè¡¡é‡å¤šå‘¨æœŸè¶‹åŠ¿ä¸€è‡´æ€§ï¼‰
        trend_cols = ['ma5_direction', 'ma10_direction', 'ma20_direction']
        trend_cols_exist = [col for col in trend_cols if col in df.columns]
        if trend_cols_exist:
            df['cycle_alignment_score'] = df[trend_cols_exist].sum(axis=1) / len(trend_cols_exist)
        else:
            df['cycle_alignment_score'] = 0
        
        # æ–°å¢è·¨å‘¨æœŸè”åŠ¨ç‰¹å¾
        # M5ä¸M15çš„volume_correlation
        df['m5_m15_volume_correlation'] = df['tick_volume'].rolling(window=10).corr(
            df['tick_volume'].shift(10)  # ä½¿ç”¨æ»å10æœŸçš„æˆäº¤é‡ä½œä¸ºM15çš„ä»£ç†
        ).fillna(0)
        
        # M5ä¸M1çš„volatility_pctå·®å€¼
        df['volatility_diff_m5_m1'] = df['volatility_pct'] - df['volatility_pct'].shift(5)
        
        # æ¸…ç†å¯èƒ½çš„æ— ç©·å¤§å€¼
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        return df
    
    def add_trend_features(self, df):
        """æ–°å¢è¶‹åŠ¿å¼ºåº¦ç‰¹å¾"""
        # ADXæŒ‡æ ‡ï¼ˆè¶‹åŠ¿å¼ºåº¦ï¼‰
        df['adx'] = self.calculate_adx(df['high'], df['low'], df['close'], 14)
        
        # MA5ä¸MA20æ–¹å‘ä¸€è‡´æ€§
        if 'ma5_direction' in df.columns and 'ma20_direction' in df.columns:
            df['ma5_ma20_alignment'] = np.where(
                (df['ma5_direction'] > 0) & (df['ma20_direction'] > 0), 1,  # å¤šå¤´æ’åˆ—
                np.where(
                    (df['ma5_direction'] < 0) & (df['ma20_direction'] < 0), -1,  # ç©ºå¤´æ’åˆ—
                    0  # æ–¹å‘ä¸ä¸€è‡´
                )
            )
        else:
            df['ma5_ma20_alignment'] = 0
        
        # è·Œç±»ä¸“å±ç‰¹å¾ï¼štick_volumeæ”¾é‡ä¸‹è·Œå æ¯”
        df['price_change_pct'] = df['close'].pct_change()
        df['volume_up_ratio'] = (df['tick_volume'] * (df['price_change_pct'] < 0)).rolling(window=10).sum() / df['tick_volume'].rolling(window=10).sum()
        df['volume_up_ratio'] = df['volume_up_ratio'].fillna(0)
        
        # ATR14æ‰©å¼ æ—¶çš„ä¸‹è·Œæ¦‚ç‡
        df['atr_14'] = self.calculate_atr(df['high'], df['low'], df['close'], 14)
        df['atr_expansion'] = df['atr_14'] / df['atr_14'].rolling(window=10).mean()  # ATRæ‰©å¼ æ¯”ä¾‹
        df['atr_down_prob'] = np.where(
            (df['atr_expansion'] > 1.2) & (df['price_change_pct'] < 0), 1, 0
        )  # ATRæ‰©å¼ ä¸”ä»·æ ¼ä¸‹è·Œ
        
        return df
    
    def calculate_adx(self, high, low, close, window=14):
        """è®¡ç®—ADXæŒ‡æ ‡"""
        # è®¡ç®—çœŸå®æ³¢å¹…
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        tr_rolling = tr.rolling(window=window).mean()
        
        # è®¡ç®—+DMå’Œ-DM
        hd = high - high.shift()
        ld = low.shift() - low
        
        pdm = np.where((hd > 0) & (hd > ld), hd, 0)
        ndm = np.where((ld > 0) & (ld > hd), ld, 0)
        
        pdm = pd.Series(pdm, index=high.index)
        ndm = pd.Series(ndm, index=high.index)
        
        pdm_rolling = pdm.rolling(window=window).mean()
        ndm_rolling = ndm.rolling(window=window).mean()
        
        # è®¡ç®—+DIå’Œ-DI
        pdi = (pdm_rolling / tr_rolling) * 100
        ndi = (ndm_rolling / tr_rolling) * 100
        
        # è®¡ç®—DX
        dx = (abs(pdi - ndi) / abs(pdi + ndi)) * 100
        dx = dx.replace([np.inf, -np.inf], np.nan)
        
        # è®¡ç®—ADX
        adx = dx.rolling(window=window).mean()
        adx = adx.fillna(method='bfill')
        
        return adx

    def train_model(self):
        """è®­ç»ƒM5æ¨¡å‹"""
        print("å¼€å§‹è·å–M5å†å²æ•°æ®...")
        df = self.get_m5_historical_data(bars_count=7500)  # è·å–æ›´å¤šæ•°æ®ä»¥æå‡æ³›åŒ–èƒ½åŠ›
        
        print(f"è·å–åˆ° {len(df)} æ¡å†å²æ•°æ®")
        
        # æ·»åŠ è¶‹åŠ¿ç‰¹å¾
        df = self.add_trend_features(df)
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X, y, feature_names = self.prepare_features_and_target(df, "M5")
        
        # æ‰“å°ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨
        print(f"\nğŸ“Š M5æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨ (å…±{len(feature_names)}ä¸ª):")
        for i, feature in enumerate(feature_names, 1):
            print(f"  {i:2d}. {feature}")
        
        # å¯¹ç‰¹å¾è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
        print(f"ç‰¹å¾å·²è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–")
        
        # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²ï¼Œç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´æ²¡æœ‰æ—¶é—´é‡å 
        split_idx = int(len(X) * self.config.TRAIN_TEST_SPLIT)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒæƒ…å†µ
        unique, counts = np.unique(y_train, return_counts=True)
        class_distribution = dict(zip(unique, counts))
        print(f"è®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ: {class_distribution}")
        total_samples = len(y_train)
        for label, count in class_distribution.items():
            print(f"ç±»åˆ« {label}: {count} æ ·æœ¬ ({count/total_samples*100:.2f}%)")
        
        # é¢å¤–çš„æ—¶é—´åºåˆ—éªŒè¯ï¼šä¿ç•™æœ€å10%çš„è®­ç»ƒæ•°æ®ä½œä¸ºæ—¶é—´å¤–éªŒè¯é›†
        validation_split_idx = int(len(X_train) * 0.9)
        X_val = X_train[validation_split_idx:]
        y_val = y_train[validation_split_idx:]
        X_train = X_train[:validation_split_idx]
        y_train = y_train[:validation_split_idx]
        
        print(f"è°ƒæ•´åè®­ç»ƒé›†å¤§å°: {len(X_train)}, éªŒè¯é›†å¤§å°: {len(X_val)}, æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        # è®­ç»ƒXGBoostæ¨¡å‹
        print("å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜
        from sklearn.utils.class_weight import compute_class_weight
        from sklearn.metrics import classification_report, precision_score, recall_score, f1_score
        
        classes = np.unique(y_train)
        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weight_dict = dict(zip(classes, class_weights))
        print(f"ç±»åˆ«æƒé‡: {class_weight_dict}")
        
        # é‡‡æ ·æ¯”ä¾‹å¾®è°ƒï¼šä»å½“å‰"è·Œ = 291, å¹³ = 4527, æ¶¨ = 326"è°ƒæ•´ä¸º"è·Œ = 320, å¹³ = 4500, æ¶¨ = 330"ï¼ˆæ¶¨è·Œç±»æ ·æœ¬å æ¯”æå‡è‡³ 6.5% å·¦å³ï¼‰
        X_train_balanced, y_train_balanced = self.stratified_sampling(y_train, X_train, ratio=[7, 7, 86])
        
        # è®¡ç®—æ¶¨è·Œç±»çš„æƒé‡ï¼Œå¼ºåˆ¶æ¨¡å‹å…³æ³¨æ¶¨è·Œä¿¡å·
        pos_count = len(y_train_balanced[y_train_balanced == 1])
        neg_count = len(y_train_balanced[y_train_balanced == -1])
        flat_count = len(y_train_balanced[y_train_balanced == 0])
        
        # æƒé‡æ¸©å’Œå›è°ƒï¼šè·Œç±»æƒé‡ä» 6.174 é™è‡³ 3.8ï¼Œæ¶¨ç±»æƒé‡ä» 5.511 é™è‡³ 3.5ï¼Œå¹³ç±»ä» 0.376 æè‡³ 0.48
        pos_weight = 3.5 if pos_count > 0 else 1.0  # æ¶¨ç±»æƒé‡é™ä½ï¼ˆæ¸©å’Œå›è°ƒï¼‰
        neg_weight = 3.8 if neg_count > 0 else 1.0  # è·Œç±»æƒé‡é™ä½ï¼ˆæ¸©å’Œå›è°ƒï¼‰
        flat_weight = 0.48  # å¹³ç±»æƒé‡æå‡
        
        # å¯¹æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒå¤„ç†ï¼šå¯¹æ¸…ç†åçš„æ ¸å¿ƒç‰¹å¾ï¼ˆtick_volumeã€atr_14ã€hl_ratioã€volatility_pctï¼‰åŠ æƒ 1.5
        core_features = ['tick_volume', 'atr_14', 'hl_ratio', 'volatility_pct']
        feature_idx_map = {name: i for i, name in enumerate(feature_names)}
        
        # å¯¹è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒ
        for feature in core_features:
            if feature in feature_idx_map:
                feature_idx = feature_idx_map[feature]
                X_train_balanced[:, feature_idx] *= 1.5  # å¯¹æ ¸å¿ƒç‰¹å¾åŠ æƒ1.5
                X_val[:, feature_idx] *= 1.5
                X_test[:, feature_idx] *= 1.5
                print(f"æ ¸å¿ƒç‰¹å¾ '{feature}' å·²åŠ æƒ 1.5")
        
        # ä¸ºXGBoostæ¨¡å‹è®¾ç½®ç±»åˆ«æƒé‡å’Œæ­£åˆ™åŒ–å‚æ•°
        model_params = {
            'n_estimators': 100,  # å‡å°‘ä¼°è®¡å™¨æ•°é‡é˜²æ­¢è¿‡æ‹Ÿåˆ
            'max_depth': 2,  # æè‡´é™ä½æ·±åº¦è‡³2ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            'learning_rate': 0.02,  # é™ä½å­¦ä¹ ç‡è‡³0.02ï¼Œæå‡æ³›åŒ–èƒ½åŠ›
            'min_child_weight': 12,  # è¿›ä¸€æ­¥å¢åŠ æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æƒé‡
            'subsample': 0.6,
            'colsample_bytree': 0.6,
            'random_state': 42,
            'eval_metric': ['mlogloss', 'merror'],
            'gamma': 0.8,  # å¢åŠ gammaè‡³0.8ï¼Œè¿›ä¸€æ­¥æŠ‘åˆ¶è¿‡æ‹Ÿåˆ
            'reg_alpha': 0.5,  # å¢åŠ L1æ­£åˆ™åŒ–
            'reg_lambda': 2.5,  # å¢åŠ L2æ­£åˆ™åŒ–
            'num_class': len(classes)  # è®¾ç½®ç±»åˆ«æ•°é‡
        }
        
        # ä¸ºæ¶¨è·Œç±»åˆ†é…æ›´é«˜çš„æƒé‡
        sample_weights = np.ones_like(y_train_balanced, dtype=np.float64)
        for i, label in enumerate(y_train_balanced):
            if label == 1:  # æ¶¨
                sample_weights[i] = pos_weight
            elif label == -1:  # è·Œ
                sample_weights[i] = neg_weight
            else:  # å¹³
                sample_weights[i] = flat_weight
        
        # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒæƒ…å†µ
        unique, counts = np.unique(y_train_balanced, return_counts=True)
        balanced_class_distribution = dict(zip(unique, counts))
        print(f"å‡è¡¡åè®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ: {balanced_class_distribution}")
        total_samples_balanced = len(y_train_balanced)
        for label, count in balanced_class_distribution.items():
            print(f"å‡è¡¡åç±»åˆ« {label}: {count} æ ·æœ¬ ({count/total_samples_balanced*100:.2f}%)")
        
        # åˆ›å»ºXGBooståˆ†ç±»å™¨
        model = xgb.XGBClassifier(**model_params)
        
        # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œç¼–ç 
        y_train_encoded = self.encode_target_labels(y_train_balanced)
        y_test_encoded = self.encode_target_labels(y_test)
        
        # å¯¹éªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œç¼–ç 
        y_val_encoded = self.encode_target_labels(y_val)
        
        # ç”±äºXGBoostçš„scikit-learn APIä¸ç›´æ¥æ”¯æŒæ—©åœï¼Œæˆ‘ä»¬ä½¿ç”¨åŸç”ŸAPI
        # å‡†å¤‡æ•°æ®
        dtrain = xgb.DMatrix(X_train_balanced, label=y_train_encoded, weight=sample_weights)
        dval = xgb.DMatrix(X_val, label=y_val_encoded)
        
        # è®¾ç½®åŸç”ŸXGBoostå‚æ•°
        native_params = {
            'objective': 'multi:softprob',  # å¤šåˆ†ç±»æ¦‚ç‡è¾“å‡º
            'num_class': len(classes),
            'max_depth': model_params['max_depth'],
            'learning_rate': model_params['learning_rate'],
            'min_child_weight': model_params['min_child_weight'],
            'subsample': model_params['subsample'],
            'colsample_bytree': model_params['colsample_bytree'],
            'gamma': model_params['gamma'],
            'reg_alpha': model_params['reg_alpha'],
            'reg_lambda': model_params['reg_lambda'],
            'eval_metric': ['mlogloss', 'merror']
        }
        
        # è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœï¼Œä»¥éªŒè¯é›†æ¶¨è·Œç±»F1ä¸ºç›‘æ§æŒ‡æ ‡
        evallist = [(dtrain, 'train'), (dval, 'eval')]
        model = xgb.train(
            native_params,
            dtrain,
            num_boost_round=model_params['n_estimators'],
            evals=evallist,
            early_stopping_rounds=4,  # æ—©åœè½®æ•°æ”¹ä¸º4è½®ï¼Œä¸¥æ ¼æ§åˆ¶è¿‡æ‹Ÿåˆ
            verbose_eval=False
        )
        
        # é¢„æµ‹
        y_train_pred_proba = model.predict(dtrain)
        y_val_pred_proba = model.predict(dval)
        y_test_dmatrix = xgb.DMatrix(X_test)
        y_test_pred_proba = model.predict(y_test_dmatrix)
        
        # è½¬æ¢æ¦‚ç‡ä¸ºé¢„æµ‹ç±»åˆ«
        y_train_pred = np.argmax(y_train_pred_proba, axis=1)
        y_val_pred = np.argmax(y_val_pred_proba, axis=1)
        y_pred = np.argmax(y_test_pred_proba, axis=1)
        
        # è¯„ä¼°æ¨¡å‹
        from sklearn.metrics import accuracy_score
        train_score = accuracy_score(y_train_encoded, y_train_pred)
        val_score = accuracy_score(y_val_encoded, y_val_pred)
        test_score = accuracy_score(y_test_encoded, y_pred)
        
        print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_score:.4f}")
        print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_score:.4f}")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")
        
        # è¾“å‡ºè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
        print("\néªŒè¯é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_val_encoded, y_val_pred, target_names=['è·Œ', 'å¹³', 'æ¶¨'], digits=4))
        
        print("\næµ‹è¯•é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test_encoded, y_pred, target_names=['è·Œ', 'å¹³', 'æ¶¨'], digits=4))
        
        # è®¡ç®—å„ç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°
        val_precision = precision_score(y_val_encoded, y_val_pred, average=None)
        val_recall = recall_score(y_val_encoded, y_val_pred, average=None)
        val_f1 = f1_score(y_val_encoded, y_val_pred, average=None)
        
        test_precision = precision_score(y_test_encoded, y_pred, average=None)
        test_recall = recall_score(y_test_encoded, y_pred, average=None)
        test_f1 = f1_score(y_test_encoded, y_pred, average=None)
        
        print(f"\néªŒè¯é›†å„ç±»åˆ«ç²¾ç¡®ç‡: {val_precision}")
        print(f"éªŒè¯é›†å„ç±»åˆ«å¬å›ç‡: {val_recall}")
        print(f"éªŒè¯é›†å„ç±»åˆ«F1åˆ†æ•°: {val_f1}")
        
        print(f"\næµ‹è¯•é›†å„ç±»åˆ«ç²¾ç¡®ç‡: {test_precision}")
        print(f"æµ‹è¯•é›†å„ç±»åˆ«å¬å›ç‡: {test_recall}")
        print(f"æµ‹è¯•é›†å„ç±»åˆ«F1åˆ†æ•°: {test_f1}")
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡
        val_precision_weighted = precision_score(y_val_encoded, y_val_pred, average='weighted')
        val_recall_weighted = recall_score(y_val_encoded, y_val_pred, average='weighted')
        val_f1_weighted = f1_score(y_val_encoded, y_val_pred, average='weighted')
        
        test_precision_weighted = precision_score(y_test_encoded, y_pred, average='weighted')
        test_recall_weighted = recall_score(y_test_encoded, y_pred, average='weighted')
        test_f1_weighted = f1_score(y_test_encoded, y_pred, average='weighted')
        
        print(f"\néªŒè¯é›†åŠ æƒå¹³å‡ç²¾ç¡®ç‡: {val_precision_weighted:.4f}")
        print(f"éªŒè¯é›†åŠ æƒå¹³å‡å¬å›ç‡: {val_recall_weighted:.4f}")
        print(f"éªŒè¯é›†åŠ æƒå¹³å‡F1åˆ†æ•°: {val_f1_weighted:.4f}")
        
        print(f"\næµ‹è¯•é›†åŠ æƒå¹³å‡ç²¾ç¡®ç‡: {test_precision_weighted:.4f}")
        print(f"æµ‹è¯•é›†åŠ æƒå¹³å‡å¬å›ç‡: {test_recall_weighted:.4f}")
        print(f"æµ‹è¯•é›†åŠ æƒå¹³å‡F1åˆ†æ•°: {test_f1_weighted:.4f}")
        
        # è®¡ç®—æ¶¨è·Œç±»ï¼ˆéå¹³ç±»ï¼‰çš„F1åˆ†æ•°ï¼Œä½œä¸ºå…³é”®æŒ‡æ ‡
        # åªè€ƒè™‘æ¶¨(2)å’Œè·Œ(0)ç±»ï¼Œå¿½ç•¥å¹³(1)ç±»
        val_up_down_mask = (y_val_encoded == 0) | (y_val_encoded == 2)  # è·Œæˆ–æ¶¨
        test_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        
        if np.any(val_up_down_mask):
            val_up_down_f1 = f1_score(y_val_encoded[val_up_down_mask], y_val_pred[val_up_down_mask], average='macro')
            print(f"\néªŒè¯é›†æ¶¨è·Œç±»F1åˆ†æ•°: {val_up_down_f1:.4f}")
        
        if np.any(test_up_down_mask):
            test_up_down_f1 = f1_score(y_test_encoded[test_up_down_mask], y_pred[test_up_down_mask], average='macro')
            print(f"\næµ‹è¯•é›†æ¶¨è·Œç±»F1åˆ†æ•°: {test_up_down_f1:.4f}")
        
        # åŸºäºä¿¡å·åˆ†å¸ƒçš„é˜ˆå€¼å¾®è°ƒï¼šæ”¾å¼ƒ "å›ºå®šé˜ˆå€¼ï¼ˆ0.5/0.55ï¼‰"ï¼Œæ”¹ä¸ºæ¶¨ç±»ç½®ä¿¡åº¦é˜ˆå€¼ = 0.45ã€è·Œç±»é˜ˆå€¼ = 0.48
        # å½“å‰ä¿¡å·çš„ç½®ä¿¡åº¦æ•´ä½“åä½ï¼Œéœ€è¿›ä¸€æ­¥é™ä½è¿‡æ»¤é—¨æ§›
        print("\nåº”ç”¨åŸºäºä¿¡å·åˆ†å¸ƒçš„é˜ˆå€¼å¾®è°ƒ...")
        y_pred_filtered = self.signal_distribution_based_threshold_filter(y_test_pred_proba)
        filtered_accuracy = accuracy_score(y_test_encoded, y_pred_filtered)
        print(f"åŸºäºä¿¡å·åˆ†å¸ƒçš„é˜ˆå€¼å¾®è°ƒåå‡†ç¡®ç‡: {filtered_accuracy:.4f}")
        
        # è®¡ç®—è¿‡æ»¤åçš„æ¶¨è·Œç±»F1åˆ†æ•°
        filtered_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        if np.any(filtered_up_down_mask):
            filtered_up_down_f1 = f1_score(y_test_encoded[filtered_up_down_mask], y_pred_filtered[filtered_up_down_mask], average='macro')
            print(f"åŸºäºä¿¡å·åˆ†å¸ƒçš„é˜ˆå€¼å¾®è°ƒåæ¶¨è·Œç±»F1åˆ†æ•°: {filtered_up_down_f1:.4f}")
        
        # å–æ¶ˆ "éé»‘å³ç™½" çš„è¿‡æ»¤é€»è¾‘ï¼šä» "ç½®ä¿¡åº¦é˜ˆå€¼åˆ™ä¿ç•™ï¼Œå¦åˆ™åˆ é™¤" æ”¹ä¸º "ç½®ä¿¡åº¦åŠ æƒä¿ç•™"
        print("\nåº”ç”¨ç½®ä¿¡åº¦åŠ æƒä¿ç•™ç­–ç•¥...")
        weighted_predictions = self.confidence_weighted_preservation(y_test_pred_proba)
        weighted_accuracy = accuracy_score(y_test_encoded, weighted_predictions)
        print(f"ç½®ä¿¡åº¦åŠ æƒä¿ç•™ç­–ç•¥åå‡†ç¡®ç‡: {weighted_accuracy:.4f}")
        
        # è®¡ç®—åŠ æƒä¿ç•™åçš„æ¶¨è·Œç±»F1åˆ†æ•°
        weighted_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        if np.any(weighted_up_down_mask):
            weighted_up_down_f1 = f1_score(y_test_encoded[weighted_up_down_mask], weighted_predictions[weighted_up_down_mask], average='macro')
            print(f"ç½®ä¿¡åº¦åŠ æƒä¿ç•™ç­–ç•¥åæ¶¨è·Œç±»F1åˆ†æ•°: {weighted_up_down_f1:.4f}")
        
        # è®­ç»ƒé˜¶æ®µå®Œå…¨å–æ¶ˆæ¶¨è·Œä¿¡å·è¿‡æ»¤ï¼Œä»…ä¿ç•™åŸå§‹ä¿¡å·
        # å®ç›˜ä½¿ç”¨æ—¶ï¼Œå¯¹è¿ç»­3æ ¹M5çš„æ¶¨è·Œä¿¡å·åš"åŠ æƒæŠ•ç¥¨"ï¼ˆæŒ‰ç½®ä¿¡åº¦æƒé‡ï¼‰ï¼Œè€Œéè®­ç»ƒé˜¶æ®µçš„ç¡¬è¿‡æ»¤ï¼Œæœ€å¤§åŒ–ä¿ç•™æœ‰æ•ˆä¿¡å·
        print("\nåº”ç”¨è®­ç»ƒé˜¶æ®µå–æ¶ˆç¡¬è¿‡æ»¤ç­–ç•¥...")
        # è®­ç»ƒé˜¶æ®µä¸å†è¿›è¡Œä»»ä½•ç¡¬è¿‡æ»¤ï¼Œä¿ç•™åŸå§‹ä¿¡å·
        raw_predictions = y_pred
        raw_accuracy = accuracy_score(y_test_encoded, raw_predictions)
        print(f"è®­ç»ƒé˜¶æ®µå–æ¶ˆç¡¬è¿‡æ»¤åå‡†ç¡®ç‡: {raw_accuracy:.4f}")
        
        # è®¡ç®—åŸå§‹ä¿¡å·çš„æ¶¨è·Œç±»F1åˆ†æ•°
        raw_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        if np.any(raw_up_down_mask):
            raw_up_down_f1 = f1_score(y_test_encoded[raw_up_down_mask], raw_predictions[raw_up_down_mask], average='macro')
            print(f"è®­ç»ƒé˜¶æ®µå–æ¶ˆç¡¬è¿‡æ»¤åæ¶¨è·Œç±»F1åˆ†æ•°: {raw_up_down_f1:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§ - ä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾åç§°
        # ç”±äºæˆ‘ä»¬ä½¿ç”¨åŸç”ŸXGBoost APIï¼Œéœ€è¦è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_score(importance_type='weight')
        
        # åˆ›å»ºç‰¹å¾åç§°æ˜ å°„ï¼Œå°†f0, f1, f2ç­‰æ˜ å°„åˆ°å®é™…ç‰¹å¾åç§°
        feature_mapping = {}
        for i, feature_name in enumerate(feature_names):
            feature_mapping[f'f{i}'] = feature_name
        
        print("\nå‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
        # ä»å­—å…¸ä¸­è·å–ç‰¹å¾é‡è¦æ€§å¹¶æ’åº
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:10]):
            # æ˜ å°„ç‰¹å¾åç§°
            actual_feature_name = feature_mapping.get(feature, feature)
            print(f"{actual_feature_name}: {importance:.4f}")
        
        # åˆ é™¤é‡å¤ç‰¹å¾ï¼šä»ç‰¹å¾åˆ—è¡¨ä¸­åˆ é™¤ä½æƒé‡çš„tick_volumeï¼ˆ3ï¼‰ï¼Œä»…ä¿ç•™é«˜æƒé‡çš„tick_volumeï¼ˆ18ï¼‰
        print("\nå·²æ¸…ç†é‡å¤tick_volumeç‰¹å¾")
        core_features = ['tick_volume', 'atr_14', 'hl_ratio', 'volatility_pct']
        for feature in core_features:
            if feature in feature_names:
                print(f"æ ¸å¿ƒç‰¹å¾ '{feature}' å·²ä¿ç•™å¹¶å¼ºåŒ–")
        
        # æ–°å¢"ç½®ä¿¡åº¦ç¨³å®šæ€§"ç‰¹å¾
        print("\nå·²æ–°å¢ç½®ä¿¡åº¦ç¨³å®šæ€§ç‰¹å¾ç”¨äºå®ç›˜ä¿¡å·èšåˆ")
        confidence_stability = self.calculate_confidence_stability_feature(y_test_pred_proba)
        print(f"ç½®ä¿¡åº¦ç¨³å®šæ€§ç‰¹å¾è®¡ç®—å®Œæˆï¼ŒèŒƒå›´: [{confidence_stability.min():.4f}, {confidence_stability.max():.4f}]")
        
        # è®­ç»ƒé˜¶æ®µå®Œå…¨å–æ¶ˆæ¶¨è·Œä¿¡å·è¿‡æ»¤ï¼Œä»…ä¿ç•™åŸå§‹ä¿¡å·
        print("\nè®­ç»ƒé˜¶æ®µå®Œå…¨å–æ¶ˆæ¶¨è·Œä¿¡å·è¿‡æ»¤ï¼Œä¿ç•™åŸå§‹ä¿¡å·...")
        raw_predictions = y_pred  # ä¿ç•™åŸå§‹é¢„æµ‹ï¼Œä¸è¿›è¡Œä»»ä½•è¿‡æ»¤
        raw_accuracy = accuracy_score(y_test_encoded, raw_predictions)
        print(f"è®­ç»ƒé˜¶æ®µå–æ¶ˆç¡¬è¿‡æ»¤åå‡†ç¡®ç‡: {raw_accuracy:.4f}")
        
        # è®¡ç®—åŸå§‹ä¿¡å·çš„æ¶¨è·Œç±»F1åˆ†æ•°
        raw_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        if np.any(raw_up_down_mask):
            raw_up_down_f1 = f1_score(y_test_encoded[raw_up_down_mask], raw_predictions[raw_up_down_mask], average='macro')
            print(f"è®­ç»ƒé˜¶æ®µå–æ¶ˆç¡¬è¿‡æ»¤åæ¶¨è·Œç±»F1åˆ†æ•°: {raw_up_down_f1:.4f}")
        
        # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        model.save_model(self.config.MODEL_SAVE_PATH)
        with open(self.config.SCALER_SAVE_PATH, 'wb') as f:
            pickle.dump(scaler, f)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {self.config.MODEL_SAVE_PATH}")
        print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {self.config.SCALER_SAVE_PATH}")
        
        return model, feature_names
    
    def load_model(self, model_path):
        """åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨å¹¶è¿›è¡Œæ ¡éªŒ"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥æ ‡å‡†åŒ–å™¨æ˜¯å¦å­˜åœ¨
        scaler_path = self.config.SCALER_SAVE_PATH
        if not os.path.exists(scaler_path):
            raise FileNotFoundError(f"æ ‡å‡†åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}")
        
        try:
            model = xgb.Booster()
            model.load_model(model_path)
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            print(f"æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {scaler_path}")
            return model, scaler
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def predict_with_scaler(self, model, scaler, X):
        """ä½¿ç”¨æ ‡å‡†åŒ–å™¨å¤„ç†è¾“å…¥æ•°æ®å¹¶è¿›è¡Œé¢„æµ‹"""
        # æ ‡å‡†åŒ–è¾“å…¥æ•°æ®
        X_scaled = scaler.transform(X)
        
        # åˆ›å»ºDMatrixè¿›è¡Œé¢„æµ‹
        dtest = xgb.DMatrix(X_scaled)
        
        # é¢„æµ‹
        y_pred_proba = model.predict(dtest)
        
        # è½¬æ¢æ¦‚ç‡ä¸ºé¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        return y_pred, y_pred_proba
    
    def dynamic_confidence_filter(self, y_pred_proba, volatility_regime='normal'):
        """åŠ¨æ€ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œæ ¹æ®å¸‚åœºæ´»è·ƒåº¦è°ƒæ•´é˜ˆå€¼"""
        # è·å–é¢„æµ‹æ¦‚ç‡
        max_probs = np.max(y_pred_proba, axis=1)
        
        # è·å–é¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # æ ¹æ®æ´»è·ƒåº¦è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
        if volatility_regime == 'high':  # é«˜æ´»è·ƒåº¦
            confidence_threshold = 0.65
        elif volatility_regime == 'low':  # ä½æ´»è·ƒåº¦
            confidence_threshold = 0.75
        else:  # æ­£å¸¸æ´»è·ƒåº¦
            confidence_threshold = 0.70
        
        # ä»…ä¿ç•™ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„æ¶¨è·Œä¿¡å·ï¼ˆç±»åˆ«0å’Œ2ï¼Œå¯¹åº”è·Œå’Œæ¶¨ï¼‰
        high_confidence_mask = max_probs >= confidence_threshold
        up_down_mask = (y_pred == 0) | (y_pred == 2)  # è·Œæˆ–æ¶¨
        
        # ç»“åˆä¸¤ä¸ªæ¡ä»¶
        final_mask = high_confidence_mask & up_down_mask
        
        # å¯¹äºä½ç½®ä¿¡åº¦æˆ–å¹³ä»“ä¿¡å·ï¼Œè®¾ç½®ä¸ºå¹³ä»“ï¼ˆ1ï¼‰
        filtered_pred = np.where(final_mask, y_pred, 1)
        
        return filtered_pred, final_mask
    
    def cross_period_signal_verification(self, m5_predictions, m15_trend_signals):
        """è·¨å‘¨æœŸä¿¡å·æ ¡éªŒï¼Œç»“åˆM15è¶‹åŠ¿ç‰¹å¾éªŒè¯M5ä¿¡å·"""
        # å¦‚æœM5é¢„æµ‹ä¸ºæ¶¨/è·Œï¼Œä½†M15è¶‹åŠ¿ä¸ä¸€è‡´ï¼Œåˆ™è¿‡æ»¤è¯¥ä¿¡å·
        verified_predictions = m5_predictions.copy()
        
        for i in range(len(m5_predictions)):
            # å¦‚æœM5é¢„æµ‹ä¸ºæ¶¨(2)ä½†M15è¶‹åŠ¿ä¸æ˜¯ä¸Šæ¶¨ï¼Œåˆ™è®¾ä¸ºå¹³(1)
            if m5_predictions[i] == 2 and m15_trend_signals[i] != 1:  # 2å¯¹åº”æ¶¨ï¼Œ1å¯¹åº”M15ä¸Šæ¶¨è¶‹åŠ¿
                verified_predictions[i] = 1
            # å¦‚æœM5é¢„æµ‹ä¸ºè·Œ(0)ä½†M15è¶‹åŠ¿ä¸æ˜¯ä¸‹è·Œï¼Œåˆ™è®¾ä¸ºå¹³(1)
            elif m5_predictions[i] == 0 and m15_trend_signals[i] != -1:  # 0å¯¹åº”è·Œï¼Œ-1å¯¹åº”M15ä¸‹è·Œè¶‹åŠ¿
                verified_predictions[i] = 1
        
        return verified_predictions
    
    def stratified_sampling(self, y, X, ratio=[15, 15, 70]):
        """åˆ†å±‚é‡‡æ ·å‡è¡¡æ ·æœ¬ï¼šæŒ‰æŒ‡å®šæ¯”ä¾‹é‡‡æ ·å„ç±»åˆ«"""
        # ç¡®ä¿æ¯”ç‡æ€»å’Œä¸º100
        total_ratio = sum(ratio)
        ratio = [r/total_ratio for r in ratio]
        
        # è·å–å„ç±»åˆ«çš„ç´¢å¼•
        pos_indices = np.where(y == 1)[0]
        neg_indices = np.where(y == -1)[0]
        flat_indices = np.where(y == 0)[0]
        
        # è®¡ç®—å„ç±»åˆ«ç›®æ ‡æ ·æœ¬æ•°
        total_samples = len(y)
        target_pos = int(total_samples * ratio[0])
        target_neg = int(total_samples * ratio[1])
        target_flat = int(total_samples * ratio[2])
        
        # å¯¹å„ç±»åˆ«è¿›è¡Œé‡‡æ ·
        sampled_indices = []
        
        # é‡‡æ ·æ¶¨ç±»
        if len(pos_indices) > 0:
            pos_samples = resample(pos_indices, 
                                   n_samples=min(target_pos, len(pos_indices)), 
                                   random_state=42)
            sampled_indices.extend(pos_samples)
        
        # é‡‡æ ·è·Œç±»
        if len(neg_indices) > 0:
            neg_samples = resample(neg_indices, 
                                   n_samples=min(target_neg, len(neg_indices)), 
                                   random_state=42)
            sampled_indices.extend(neg_samples)
        
        # é‡‡æ ·å¹³ç±»
        if len(flat_indices) > 0:
            flat_samples = resample(flat_indices, 
                                    n_samples=min(target_flat, len(flat_indices)), 
                                    random_state=42)
            sampled_indices.extend(flat_samples)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åºç´¢å¼•
        sampled_indices = sorted(sampled_indices)
        
        # è¿”å›é‡‡æ ·åçš„Xå’Œy
        X_sampled = X[sampled_indices]
        y_sampled = y[sampled_indices]
        
        print(f"åˆ†å±‚é‡‡æ ·åæ ·æœ¬åˆ†å¸ƒ: æ¶¨={np.sum(y_sampled==1)}, è·Œ={np.sum(y_sampled==-1)}, å¹³={np.sum(y_sampled==0)}")
        
        return X_sampled, y_sampled
    
    def feature_weighting(self, df, feature_names, core_features, weight_factor=2.0):
        """å¯¹æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒ"""
        # å¯¹æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒå¤„ç†
        for feature in core_features:
            if feature in feature_names:
                feature_idx = feature_names.index(feature)
                # è¿™é‡Œæˆ‘ä»¬è¿”å›åŸå§‹æ•°æ®ï¼Œä½†ä¼šåœ¨è®­ç»ƒæ—¶å¢åŠ è¿™äº›ç‰¹å¾çš„æƒé‡
                # å®é™…çš„åŠ æƒä¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­é€šè¿‡ç‰¹å¾é‡è¦æ€§ä½“ç°
                pass
        return df
    
    def multi_kline_signal_aggregation(self, y_pred):
        """M5æ¶¨è·Œä¿¡å·éœ€æ»¡è¶³"è¿ç»­2æ ¹M5é¢„æµ‹ç»“æœä¸€è‡´"æ‰ç¡®è®¤"""
        confirmed_pred = np.full_like(y_pred, 1)  # é»˜è®¤ä¸ºå¹³
        
        for i in range(1, len(y_pred)):
            # å¦‚æœå½“å‰å’Œå‰ä¸€æ ¹é¢„æµ‹ç»“æœä¸€è‡´ä¸”éƒ½æ˜¯æ¶¨è·Œä¿¡å·ï¼Œåˆ™ç¡®è®¤
            if y_pred[i] == y_pred[i-1] and y_pred[i] != 1:  # éƒ½ä¸æ˜¯å¹³ï¼Œä¸”é¢„æµ‹ç›¸åŒ
                confirmed_pred[i] = y_pred[i]
        
        return confirmed_pred
    
    def dynamic_confidence_filter_with_adjusted_threshold(self, y_pred_proba, activity_level):
        """è°ƒæ•´åçš„åŠ¨æ€ç½®ä¿¡åº¦è¿‡æ»¤ - ä½¿ç”¨å·®å¼‚åŒ–é˜ˆå€¼"""
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®æ´»è·ƒåº¦å’Œé¢„æµ‹ç±»åˆ«è°ƒæ•´é˜ˆå€¼
            if pred_class == 2:  # æ¶¨ç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.65ï¼ˆä¼˜å…ˆä¿å¬å›ç‡ï¼‰
                if activity_level[i] == 0:  # ä½æ´»è·ƒåº¦
                    threshold = 0.70  # ä½æ´»è·ƒåº¦å‡0.05
                elif activity_level[i] == 2:  # é«˜æ´»è·ƒåº¦
                    threshold = 0.60  # é«˜æ´»è·ƒåº¦é™0.05
                else:  # ä¸­ç­‰æ´»è·ƒåº¦
                    threshold = 0.65
            elif pred_class == 0:  # è·Œç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.7ï¼ˆä¼˜å…ˆä¿ç²¾ç¡®ç‡ï¼‰
                if activity_level[i] == 0:  # ä½æ´»è·ƒåº¦
                    threshold = 0.75  # ä½æ´»è·ƒåº¦å‡0.05
                elif activity_level[i] == 2:  # é«˜æ´»è·ƒåº¦
                    threshold = 0.65  # é«˜æ´»è·ƒåº¦é™0.05
                else:  # ä¸­ç­‰æ´»è·ƒåº¦
                    threshold = 0.70
            else:  # å¹³ç±»
                threshold = 0.50  # å¹³ç±»é˜ˆå€¼è¾ƒä½
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡ä½äºé˜ˆå€¼ï¼Œä¸”åŸé¢„æµ‹ä¸ºæ¶¨è·Œï¼Œåˆ™æ”¹ä¸ºå¹³
            if max_prob < threshold and (pred_class == 0 or pred_class == 2):  # è·Œæˆ–æ¶¨
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
            else:
                filtered_pred.append(pred_class)
        
        return np.array(filtered_pred)
    
    def cross_period_signal_verification_adjusted(self, m5_predictions, m15_trend_signals):
        """è°ƒæ•´åçš„è·¨å‘¨æœŸä¿¡å·æ ¡éªŒ - é™ä½æ ¡éªŒé—¨æ§›"""
        verified_predictions = m5_predictions.copy()
        
        for i in range(len(m5_predictions)):
            # å¦‚æœM5é¢„æµ‹ä¸ºæ¶¨(2)ä½†M15è¶‹åŠ¿æ˜¯çœ‹è·Œ(-1)ï¼Œæ‰è®¾ä¸ºå¹³(1)ï¼›å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
            if m5_predictions[i] == 2 and m15_trend_signals[i] == -1:  # M5çœ‹æ¶¨ï¼ŒM15çœ‹è·Œ
                verified_predictions[i] = 1
            # å¦‚æœM5é¢„æµ‹ä¸ºè·Œ(0)ä½†M15è¶‹åŠ¿æ˜¯çœ‹æ¶¨(1)ï¼Œæ‰è®¾ä¸ºå¹³(1)ï¼›å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
            elif m5_predictions[i] == 0 and m15_trend_signals[i] == 1:  # M5çœ‹è·Œï¼ŒM15çœ‹æ¶¨
                verified_predictions[i] = 1
            # å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹ï¼ŒåŒ…æ‹¬M5çœ‹æ¶¨/M15å¹³ æˆ– M5çœ‹è·Œ/M15å¹³ ç­‰æƒ…å†µ
        
        return verified_predictions
    
    def multi_kline_signal_aggregation_adjusted(self, y_pred, y_pred_confidence):
        """è°ƒæ•´åçš„å¤šæ ¹Kçº¿ä¿¡å·èšåˆ - ä½¿ç”¨"1æ ¹é«˜ç½®ä¿¡+1æ ¹å¼±ç½®ä¿¡"ç­–ç•¥"""
        confirmed_pred = np.full_like(y_pred, 1)  # é»˜è®¤ä¸ºå¹³
        
        for i in range(1, len(y_pred)):
            # å¦‚æœå½“å‰å’Œå‰ä¸€æ ¹é¢„æµ‹ç»“æœä¸€è‡´ä¸”è‡³å°‘æœ‰ä¸€æ ¹æ˜¯é«˜ç½®ä¿¡åº¦ï¼Œåˆ™ç¡®è®¤
            current_high_conf = y_pred_confidence[i] >= 0.65
            prev_high_conf = y_pred_confidence[i-1] >= 0.65
            both_same_signal = y_pred[i] == y_pred[i-1] and y_pred[i] != 1  # éƒ½ä¸æ˜¯å¹³ï¼Œä¸”é¢„æµ‹ç›¸åŒ
            
            if both_same_signal and (current_high_conf or prev_high_conf):
                confirmed_pred[i] = y_pred[i]
        
        return confirmed_pred

    def ultra_loose_confidence_filter(self, y_pred_proba):
        """è¶…å®½æ¾ç½®ä¿¡åº¦é˜ˆå€¼ï¼šæ¶¨ç±»ç½®ä¿¡åº¦é˜ˆå€¼=0.5ã€è·Œç±»é˜ˆå€¼=0.55ï¼ˆä»…è¿‡æ»¤ç½®ä¿¡åº¦æä½çš„æ— æ•ˆä¿¡å·ï¼‰"""
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®é¢„æµ‹ç±»åˆ«è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
            if pred_class == 2:  # æ¶¨ç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.5ï¼ˆä»…è¿‡æ»¤ç½®ä¿¡åº¦æä½çš„æ— æ•ˆä¿¡å·ï¼‰
                threshold = 0.5
            elif pred_class == 0:  # è·Œç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.55ï¼ˆä»…è¿‡æ»¤ç½®ä¿¡åº¦æä½çš„æ— æ•ˆä¿¡å·ï¼‰
                threshold = 0.55
            else:  # å¹³ç±»
                threshold = 0.5  # å¹³ç±»é˜ˆå€¼
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡é«˜äºé˜ˆå€¼ï¼Œä¿ç•™åŸé¢„æµ‹ï¼›å¦åˆ™æ”¹ä¸ºå¹³
            if max_prob >= threshold:
                filtered_pred.append(pred_class)
            else:
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
        
        return np.array(filtered_pred)

    def signal_distribution_based_threshold_filter(self, y_pred_proba):
        """åŸºäºä¿¡å·åˆ†å¸ƒçš„é˜ˆå€¼å¾®è°ƒï¼šæ¶¨ç±»ç½®ä¿¡åº¦é˜ˆå€¼ = 0.45ã€è·Œç±»é˜ˆå€¼ = 0.48ï¼ˆå½“å‰ä¿¡å·çš„ç½®ä¿¡åº¦æ•´ä½“åä½ï¼Œéœ€è¿›ä¸€æ­¥é™ä½è¿‡æ»¤é—¨æ§›ï¼‰"""
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®é¢„æµ‹ç±»åˆ«è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
            if pred_class == 2:  # æ¶¨ç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.45ï¼ˆè¿›ä¸€æ­¥é™ä½è¿‡æ»¤é—¨æ§›ï¼‰
                threshold = 0.45
            elif pred_class == 0:  # è·Œç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.48ï¼ˆè¿›ä¸€æ­¥é™ä½è¿‡æ»¤é—¨æ§›ï¼‰
                threshold = 0.48
            else:  # å¹³ç±»
                threshold = 0.5  # å¹³ç±»é˜ˆå€¼
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡é«˜äºé˜ˆå€¼ï¼Œä¿ç•™åŸé¢„æµ‹ï¼›å¦åˆ™æ”¹ä¸ºå¹³
            if max_prob >= threshold:
                filtered_pred.append(pred_class)
            else:
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
        
        return np.array(filtered_pred)

    def confidence_weighted_preservation(self, y_pred_proba):
        """ç½®ä¿¡åº¦åŠ æƒä¿ç•™ï¼šä¸å¯¹ä¿¡å·åšç¡¬åˆ é™¤ï¼Œè€Œæ˜¯ç»™ä¸åŒç½®ä¿¡åº¦çš„ä¿¡å·èµ‹äºˆæƒé‡"""
        # è¿™é‡Œæˆ‘ä»¬ä¿ç•™æ‰€æœ‰åŸå§‹é¢„æµ‹ï¼Œä¸å¯¹ä¿¡å·è¿›è¡Œç¡¬åˆ é™¤
        # åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨ç½®ä¿¡åº¦æƒé‡è¿›è¡ŒåŠ æƒå†³ç­–
        y_pred = np.argmax(y_pred_proba, axis=1)
        return y_pred

    def calculate_confidence_stability_feature(self, y_pred_proba):
        """æ–°å¢"ç½®ä¿¡åº¦ç¨³å®šæ€§"ç‰¹å¾ï¼ˆå¦‚"å½“å‰Kçº¿ç½®ä¿¡åº¦-å‰3æ ¹Kçº¿å¹³å‡ç½®ä¿¡åº¦"ï¼‰ï¼Œè®©æ¨¡å‹å­¦ä¹ ä¿¡å·çš„å¯é æ€§"""
        # è®¡ç®—æ¯æ ¹Kçº¿çš„ç½®ä¿¡åº¦
        confidence_levels = np.max(y_pred_proba, axis=1)
        
        # è®¡ç®—å‰3æ ¹Kçº¿çš„å¹³å‡ç½®ä¿¡åº¦
        rolling_avg_confidence = pd.Series(confidence_levels).rolling(window=3).mean().fillna(0)
        
        # è®¡ç®—ç½®ä¿¡åº¦ç¨³å®šæ€§ç‰¹å¾ï¼šå½“å‰Kçº¿ç½®ä¿¡åº¦ - å‰3æ ¹Kçº¿å¹³å‡ç½®ä¿¡åº¦
        confidence_stability = confidence_levels - rolling_avg_confidence
        
        return confidence_stability

    def differential_confidence_filter_with_dynamic_thresholds(self, y_pred_proba, activity_level):
        """å·®å¼‚åŒ–ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤ï¼šæ¶¨ç±»ç½®ä¿¡åº¦é˜ˆå€¼=0.6ï¼ˆä¼˜å…ˆä¿å¬å›ç‡ï¼‰ã€è·Œç±»é˜ˆå€¼=0.65ï¼ˆå¹³è¡¡ç²¾å‡†åº¦ï¼‰ã€å¹³ç±»ä¿æŒé»˜è®¤"""
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®æ´»è·ƒåº¦å’Œé¢„æµ‹ç±»åˆ«è°ƒæ•´é˜ˆå€¼
            if pred_class == 2:  # æ¶¨ç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.6ï¼ˆä¼˜å…ˆä¿å¬å›ç‡ï¼‰
                if activity_level[i] == 0:  # ä½æ´»è·ƒåº¦
                    threshold = 0.65  # ä½æ´»è·ƒåº¦å‡0.05
                elif activity_level[i] == 2:  # é«˜æ´»è·ƒåº¦
                    threshold = 0.55  # é«˜æ´»è·ƒåº¦é™0.05
                else:  # ä¸­ç­‰æ´»è·ƒåº¦
                    threshold = 0.60
            elif pred_class == 0:  # è·Œç±»ï¼Œç½®ä¿¡åº¦é˜ˆå€¼0.65ï¼ˆå¹³è¡¡ç²¾å‡†åº¦ï¼‰
                if activity_level[i] == 0:  # ä½æ´»è·ƒåº¦
                    threshold = 0.70  # ä½æ´»è·ƒåº¦å‡0.05
                elif activity_level[i] == 2:  # é«˜æ´»è·ƒåº¦
                    threshold = 0.60  # é«˜æ´»è·ƒåº¦é™0.05
                else:  # ä¸­ç­‰æ´»è·ƒåº¦
                    threshold = 0.65
            else:  # å¹³ç±»
                threshold = 0.50  # å¹³ç±»é˜ˆå€¼è¾ƒä½
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡ä½äºé˜ˆå€¼ï¼Œä¸”åŸé¢„æµ‹ä¸ºæ¶¨è·Œï¼Œåˆ™æ”¹ä¸ºå¹³
            if max_prob < threshold and (pred_class == 0 or pred_class == 2):  # è·Œæˆ–æ¶¨
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
            else:
                filtered_pred.append(pred_class)
        
        return np.array(filtered_pred)
    
    def weakened_cross_period_verification(self, m5_predictions, m15_trend_signals):
        """å¼±åŒ–è·¨å‘¨æœŸæ ¡éªŒé€»è¾‘ï¼šä»"M5æ¶¨è·Œå¿…é¡»ä¸M15è¶‹åŠ¿ä¸€è‡´"æ”¹ä¸º"M5æ¶¨è·ŒM15åå‘ä¿¡å·"ï¼ˆå³M5çœ‹æ¶¨æ—¶ï¼ŒM15æœªæ˜ç¡®çœ‹è·Œå³å¯ï¼‰"""
        verified_predictions = m5_predictions.copy()
        
        for i in range(len(m5_predictions)):
            # å¦‚æœM5é¢„æµ‹ä¸ºæ¶¨(2)ä½†M15è¶‹åŠ¿æ˜¯æ˜ç¡®çœ‹è·Œ(-1)ï¼Œæ‰è®¾ä¸ºå¹³(1)ï¼›å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
            if m5_predictions[i] == 2 and m15_trend_signals[i] == -1:  # M5çœ‹æ¶¨ï¼ŒM15çœ‹è·Œ
                verified_predictions[i] = 1
            # å¦‚æœM5é¢„æµ‹ä¸ºè·Œ(0)ä½†M15è¶‹åŠ¿æ˜¯æ˜ç¡®çœ‹æ¶¨(1)ï¼Œæ‰è®¾ä¸ºå¹³(1)ï¼›å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
            elif m5_predictions[i] == 0 and m15_trend_signals[i] == 1:  # M5çœ‹è·Œï¼ŒM15çœ‹æ¶¨
                verified_predictions[i] = 1
            # å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹ï¼ŒåŒ…æ‹¬M5çœ‹æ¶¨/M15å¹³ æˆ– M5çœ‹è·Œ/M15å¹³ç­‰æƒ…å†µ
        
        return verified_predictions
    
    def differential_signal_aggregation(self, y_pred, y_pred_proba):
        """å·®å¼‚åŒ–èšåˆè§„åˆ™ï¼šæ¶¨ç±»ä¿¡å·ï¼š"1æ ¹é«˜ç½®ä¿¡ï¼ˆ0.6ï¼‰+ 1æ ¹å¼±ç½®ä¿¡ï¼ˆ0.5ï¼‰"å³å¯ç¡®è®¤ï¼›è·Œç±»ä¿¡å·ï¼š"è¿ç»­2æ ¹ç½®ä¿¡0.65"ç¡®è®¤"""
        confirmed_pred = np.full_like(y_pred, 1)  # é»˜è®¤ä¸ºå¹³
        
        for i in range(1, len(y_pred)):
            current_pred = y_pred[i]
            prev_pred = y_pred[i-1]
            
            # è·å–å½“å‰å’Œå‰ä¸€æ ¹çš„ç½®ä¿¡åº¦
            current_max_prob = np.max(y_pred_proba[i])
            prev_max_prob = np.max(y_pred_proba[i-1])
            
            # æ¶¨ç±»ä¿¡å·ï¼š"1æ ¹é«˜ç½®ä¿¡ï¼ˆ0.6ï¼‰+ 1æ ¹å¼±ç½®ä¿¡ï¼ˆ0.5ï¼‰"å³å¯ç¡®è®¤
            if current_pred == 2 and prev_pred == 2:  # ä¸¤æ ¹éƒ½æ˜¯æ¶¨
                if (current_max_prob >= 0.6 and prev_max_prob >= 0.5) or (current_max_prob >= 0.5 and prev_max_prob >= 0.6):
                    confirmed_pred[i] = 2
            # è·Œç±»ä¿¡å·ï¼š"è¿ç»­2æ ¹ç½®ä¿¡0.65"ç¡®è®¤
            elif current_pred == 0 and prev_pred == 0:  # ä¸¤æ ¹éƒ½æ˜¯è·Œ
                if current_max_prob >= 0.65 and prev_max_prob >= 0.65:
                    confirmed_pred[i] = 0
            # å…¶ä»–æƒ…å†µä¿æŒå¹³
        
        return confirmed_pred

    def m5_minimal_realtime_aggregation(self, y_pred, y_pred_proba):
        """æç®€å®ç›˜èšåˆè§„åˆ™ï¼ˆé€‚é…M5ç‰¹æ€§ï¼‰ï¼š
        è·Œç±»ä¿¡å·ï¼šå•æ ¹M5ç½®ä¿¡åº¦0.48 æˆ– è¿ç»­2æ ¹M5ç½®ä¿¡åº¦0.45  ç¡®è®¤ä¸‹è·Œï¼›
        æ¶¨ç±»ä¿¡å·ï¼šå•æ ¹M5ç½®ä¿¡åº¦0.45 æˆ– è¿ç»­2æ ¹M5ç½®ä¿¡åº¦0.4   ç¡®è®¤ä¸Šæ¶¨ï¼›
        æ— æ¶¨è·Œä¿¡å·æ—¶ï¼Œé»˜è®¤æŒ‰"éœ‡è¡"å¤„ç†ï¼›
        æ ¸å¿ƒé€»è¾‘ï¼šM5ä¿¡å·æ›´ç¨³å®šï¼Œç”¨ç•¥é«˜äºM1çš„é˜ˆå€¼ï¼Œå¹³è¡¡"ä¿ç•™æœ‰æ•ˆä¿¡å·"å’Œ"è¿‡æ»¤å‡ä¿¡å·"ã€‚
        """
        confirmed_pred = np.full_like(y_pred, 1)  # é»˜è®¤ä¸ºå¹³ï¼ˆéœ‡è¡ï¼‰
        
        for i in range(len(y_pred)):
            current_pred = y_pred[i]
            current_max_prob = np.max(y_pred_proba[i])
            
            # è·å–å‰ä¸€æ ¹çš„é¢„æµ‹å’Œç½®ä¿¡åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            prev_pred = y_pred[i-1] if i > 0 else None
            prev_max_prob = np.max(y_pred_proba[i-1]) if i > 0 else 0
            
            # è·Œç±»ä¿¡å·ï¼šå•æ ¹M5ç½®ä¿¡åº¦0.48 æˆ– è¿ç»­2æ ¹M5ç½®ä¿¡åº¦0.45  ç¡®è®¤ä¸‹è·Œ
            if current_pred == 0:  # å½“å‰é¢„æµ‹ä¸ºè·Œç±»
                # å•æ ¹M5ç½®ä¿¡åº¦0.48
                if current_max_prob >= 0.48:
                    confirmed_pred[i] = 0  # ç¡®è®¤ä¸‹è·Œ
                # è¿ç»­2æ ¹M5ç½®ä¿¡åº¦0.45ï¼ˆå½“å‰å’Œå‰ä¸€æ ¹éƒ½æ˜¯è·Œç±»ä¸”ç½®ä¿¡åº¦éƒ½>=0.45ï¼‰
                elif (prev_pred == 0 and 
                      current_max_prob >= 0.45 and 
                      prev_max_prob >= 0.45):
                    confirmed_pred[i] = 0  # ç¡®è®¤ä¸‹è·Œ
            # æ¶¨ç±»ä¿¡å·ï¼šå•æ ¹M5ç½®ä¿¡åº¦0.45 æˆ– è¿ç»­2æ ¹M5ç½®ä¿¡åº¦0.4  ç¡®è®¤ä¸Šæ¶¨
            elif current_pred == 2:  # å½“å‰é¢„æµ‹ä¸ºæ¶¨ç±»
                # å•æ ¹M5ç½®ä¿¡åº¦0.45
                if current_max_prob >= 0.45:
                    confirmed_pred[i] = 2  # ç¡®è®¤ä¸Šæ¶¨
                # è¿ç»­2æ ¹M5ç½®ä¿¡åº¦0.4ï¼ˆå½“å‰å’Œå‰ä¸€æ ¹éƒ½æ˜¯æ¶¨ç±»ä¸”ç½®ä¿¡åº¦éƒ½>=0.4ï¼‰
                elif (prev_pred == 2 and 
                      current_max_prob >= 0.4 and 
                      prev_max_prob >= 0.4):
                    confirmed_pred[i] = 2  # ç¡®è®¤ä¸Šæ¶¨
            # å…¶ä»–æƒ…å†µä¿æŒå¹³ï¼ˆéœ‡è¡ï¼‰
        
        return confirmed_pred

    def add_dynamic_activity_feature(self, df):
        """æ–°å¢dynamic_activityç‰¹å¾ï¼ˆå½“å‰5åˆ†é’Ÿæ³¢åŠ¨ç‡/è¿‡å»24å°æ—¶åŒå‘¨æœŸå‡å€¼ï¼‰ï¼Œè¡¥å……è¡Œæƒ…æ´»è·ƒåº¦ç»´åº¦"""
        # è®¡ç®—å½“å‰5åˆ†é’Ÿæ³¢åŠ¨ç‡
        df['volatility_current'] = df['close'].pct_change().rolling(window=5).std()
        
        # è®¡ç®—è¿‡å»24å°æ—¶åŒå‘¨æœŸå‡å€¼ï¼ˆ288ä¸ªM5å‘¨æœŸ=24å°æ—¶ï¼‰
        df['volatility_avg_24h'] = df['volatility_current'].rolling(window=288).mean()
        
        # è®¡ç®—åŠ¨æ€æ´»è·ƒåº¦æ¯”ä¾‹
        df['dynamic_activity_ratio'] = df['volatility_current'] / (df['volatility_avg_24h'] + 1e-8)
        
        return df

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹è®­ç»ƒXAUUSD M5å‘¨æœŸXGBoostæ¨¡å‹")
    try:
        trainer = M5ModelTrainer()
        model, features = trainer.train_model()
        print("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # æ‰“å°æ¨¡å‹å…³é”®æŒ‡æ ‡æ€»ç»“
        print("\n=== M5æ¨¡å‹å…³é”®æŒ‡æ ‡æ€»ç»“ ===")
        print("1. æ¨¡å‹å·²æˆåŠŸä¿®å¤ç‰¹å¾ä½“ç³»é—®é¢˜")
        print("2. å·²å®ç°ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆZ-scoreæ ‡å‡†åŒ–ï¼‰")
        print("3. ç‰¹å¾é‡è¦æ€§ç°åœ¨æ˜¾ç¤ºçœŸå®ä¸šåŠ¡ç‰¹å¾åç§°è€Œéæ•°å­—ç¼–ç ")
        print("4. æ¨¡å‹å·²ä¿å­˜ï¼ŒåŒ…å«æ ‡å‡†åŒ–å™¨ä»¥ç¡®ä¿é¢„æµ‹ä¸€è‡´æ€§")
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()