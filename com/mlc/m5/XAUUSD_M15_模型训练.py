import MetaTrader5 as mt5
import pandas as pd
import numpy as np
import xgboost as xgb
import pickle
import sys
import os
from datetime import datetime, timedelta, timezone
import warnings
import importlib.util
from sklearn.utils import resample
warnings.filterwarnings('ignore')

# æ·»åŠ å…¬å…±æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "common"))

# åŠ¨æ€å¯¼å…¥åŸºç±»
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
model_trainer_base_path = os.path.join(project_root, 'mlc', 'm5', 'model_trainer_base.py')
spec = importlib.util.spec_from_file_location("model_trainer_base", model_trainer_base_path)
model_trainer_base = importlib.util.module_from_spec(spec)
spec.loader.exec_module(model_trainer_base)
BaseModelTrainer = model_trainer_base.BaseModelTrainer

# é…ç½®å‚æ•°
class M15ModelConfig:
    SYMBOL = "XAUUSD"
    M15_TIMEFRAME = mt5.TIMEFRAME_M15
    HISTORY_M15_BARS = 200  # ç”¨äºé¢„æµ‹çš„M15 Kçº¿æ•°é‡ï¼ˆ200æ ¹ï¼‰
    PREDICT_FUTURE_BARS = 4  # é¢„æµ‹æœªæ¥Kçº¿æ•°é‡ï¼ˆ1-4æ ¹ï¼‰
    TRAIN_TEST_SPLIT = 0.8
    MODEL_SAVE_PATH = "xauusd_m15_model.json"  # XGBoostæ¨¡å‹ä¿å­˜è·¯å¾„
    SCALER_SAVE_PATH = "m15_scaler.pkl"
    UTC_TZ = timezone.utc

class M15ModelTrainer(BaseModelTrainer):
    def __init__(self):
        super().__init__()
        self.config = M15ModelConfig()
    
    def get_m15_historical_data(self, bars_count: int = 547*24*4):  # 1.5å¹´çš„M15æ•°æ®
        """è·å–MT5çœŸå®å†å²M15æ•°æ®"""
        self.initialize_mt5()
        
        # è·å–å½“å‰æ—¶é—´
        current_utc = datetime.now(self.config.UTC_TZ)
        start_time = current_utc - timedelta(minutes=15*bars_count)  # M15æ•°æ®ï¼Œæ¯æ ¹Kçº¿15åˆ†é’Ÿ
        
        # ä½¿ç”¨mt5.copy_rates_from_posæŒ‰Kçº¿æ•°é‡è·å–æ•°æ®
        m15_rates = mt5.copy_rates_from_pos(
            self.config.SYMBOL,
            self.config.M15_TIMEFRAME,
            0,  # ä»æœ€æ–°çš„Kçº¿å¼€å§‹è·å–
            bars_count  # è·å–æŒ‡å®šæ•°é‡çš„Kçº¿
        )
        
        if m15_rates is None or len(m15_rates) == 0:
            raise Exception(f"è·å–M15å†å²æ•°æ®å¤±è´¥ï¼š{mt5.last_error()}")
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(m15_rates)
        df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
        df.set_index('time', inplace=True)
        
        # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥ - æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
        time_diff = df.index.to_series().diff().dt.total_seconds().dropna()
        if not (time_diff == 900).all():  # M15å‘¨æœŸé¢„æœŸé—´éš”900ç§’
            print("è­¦å‘Š: æ•°æ®å­˜åœ¨æ—¶é—´æ–­è¿ï¼Œå¯èƒ½å½±å“ç‰¹å¾è®¡ç®—")
        
        # å‡†å¤‡æ•°æ®å’Œç‰¹å¾
        df = self.prepare_data_with_features(m15_rates, "M15")
        
        # æ·»åŠ M15ä¸“ç”¨çš„è¶‹åŠ¿ç‰¹å¾
        df = self.add_trend_features(df)
        
        # åˆ›å»ºç›®æ ‡å˜é‡ï¼šé¢„æµ‹æœªæ¥3-5æ ¹Kçº¿çš„è¶‹åŠ¿æ–¹å‘ (1=æ¶¨, 0=è·Œ, -1=å¹³)
        df['future_close_1'] = df['close'].shift(-1)  # é¢„æµ‹1æ ¹Kçº¿å
        df['future_close_2'] = df['close'].shift(-2)  # é¢„æµ‹2æ ¹Kçº¿å
        df['future_close_3'] = df['close'].shift(-3)  # é¢„æµ‹3æ ¹Kçº¿å
        df['future_close_4'] = df['close'].shift(-4)  # é¢„æµ‹4æ ¹Kçº¿å
        df['future_close_5'] = df['close'].shift(-5)  # é¢„æµ‹5æ ¹Kçº¿å
        
        # ä½¿ç”¨é¢„æµ‹æœªæ¥3-5æ ¹Kçº¿çš„å¹³å‡æ¶¨è·Œå¹…ä½œä¸ºç›®æ ‡ï¼ˆè¶‹åŠ¿ç¡®è®¤ï¼‰
        df['future_avg_close'] = (df['future_close_1'] + df['future_close_2'] + df['future_close_3'] + df['future_close_4'] + df['future_close_5']) / 5
        df['price_change_pct'] = (df['future_avg_close'] - df['close']) / df['close']
        
        # å¼‚å¸¸å€¼å¤„ç† - æ£€æµ‹ä»·æ ¼è·³ç©º
        df['gap_pct'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)
        atr_14 = self.calculate_atr(df['high'], df['low'], df['close'], 14)
        df['atr_14'] = atr_14
        df = df[abs(df['gap_pct']) < 3 * atr_14]  # è¿‡æ»¤æç«¯è·³ç©º
        
        # é‡æ–°è®¡ç®—price_change_pctåœ¨è¿‡æ»¤å¼‚å¸¸å€¼ä¹‹å
        df['future_close_1'] = df['close'].shift(-1)
        df['price_change_pct'] = (df['future_close_1'] - df['close']) / df['close']
        
        # è®¡ç®—åŸºäºæ³¢åŠ¨ç‡çš„åŠ¨æ€é˜ˆå€¼ï¼ˆé€‚åˆM15è¶‹åŠ¿ç¡®è®¤ï¼‰
        base_threshold = 0.0018  # è°ƒæ•´ååŸºç¡€é˜ˆå€¼ï¼ˆ0.18%ï¼‰ï¼Œæ›´é€‚åˆM15å‘¨æœŸ
        dynamic_threshold_series = base_threshold - np.minimum(0.0003, atr_14 * 0.015)  # æ³¢åŠ¨ç‡è¶Šé«˜ï¼Œé˜ˆå€¼è¶Šä½ï¼ˆæœ€ä½0.0015ï¼‰
        
        # ç¡®ä¿dynamic_threshold_seriesä¸price_change_pctç´¢å¼•ä¸€è‡´
        dynamic_threshold_series = dynamic_threshold_series.reindex(df['price_change_pct'].index, fill_value=base_threshold)
        
        # å®šä¹‰ç›®æ ‡å˜é‡ - M15å‘¨æœŸå¯èƒ½æ³¢åŠ¨æ›´å¤§ï¼Œä½¿ç”¨æ›´å¤§é˜ˆå€¼ï¼Œè€ƒè™‘XAUUSDçš„ç‚¹å·®å’ŒåŠ¨æ€é˜ˆå€¼
        # è°ƒæ•´é˜ˆå€¼ä»¥å¹³è¡¡ç±»åˆ«åˆ†å¸ƒï¼ŒXAUUSDç‚¹å·®çº¦ä¸º0.05ï¼Œè®¾ç½®åˆç†é˜ˆå€¼é¿å…è¿‡å¤š'å¹³'ç±»æ ·æœ¬
        df['target'] = np.where(df['price_change_pct'] > dynamic_threshold_series, 1,  # æ¶¨
                               np.where(df['price_change_pct'] < -dynamic_threshold_series, -1, 0))  # è·Œå’Œå¹³
        
        # æ£€æŸ¥å¹¶æŠ¥å‘Šç±»åˆ«åˆ†å¸ƒ
        unique, counts = np.unique(df['target'], return_counts=True)
        class_dist = dict(zip(unique, counts))
        print(f"ç›®æ ‡å˜é‡ç±»åˆ«åˆ†å¸ƒ: {class_dist}")
        
        # å¦‚æœ'å¹³'ç±»æ ·æœ¬å æ¯”è¿‡é«˜ï¼Œè°ƒæ•´é˜ˆå€¼
        if 0 in class_dist:
            flat_ratio = class_dist[0] / len(df['target'])
            if flat_ratio > 0.8:  # å¦‚æœ'å¹³'ç±»å æ¯”è¶…è¿‡80%
                print(f"è­¦å‘Š: 'å¹³'ç±»æ ·æœ¬å æ¯”è¿‡é«˜ ({flat_ratio:.2%})ï¼Œæ­£åœ¨è°ƒæ•´é˜ˆå€¼...")
                # é™ä½é˜ˆå€¼ä»¥å‡å°‘'å¹³'ç±»æ ·æœ¬æ¯”ä¾‹
                adjusted_threshold = dynamic_threshold_series * 0.7  # é™ä½é˜ˆå€¼
                df['target'] = np.where(df['price_change_pct'] > adjusted_threshold, 1,  # æ¶¨
                                       np.where(df['price_change_pct'] < -adjusted_threshold, -1, 0))  # è·Œå’Œå¹³
                
                # é‡æ–°æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
                unique, counts = np.unique(df['target'], return_counts=True)
                class_dist = dict(zip(unique, counts))
                print(f"è°ƒæ•´åç›®æ ‡å˜é‡ç±»åˆ«åˆ†å¸ƒ: {class_dist}")
        
        return df
    
    def add_trend_features(self, df):
        """ä¸ºM15æ•°æ®æ·»åŠ è¶‹åŠ¿ç‰¹å¾"""
        # ADXæŒ‡æ ‡ï¼ˆè¶‹åŠ¿å¼ºåº¦ï¼‰
        df['adx'] = self.calculate_adx(df['high'], df['low'], df['close'], 14)
        
        # å‡çº¿æ’åˆ—ä¸€è‡´æ€§ï¼ˆå¤šå¤´/ç©ºå¤´æ’åˆ—ï¼‰
        ma_cols = ['ma5', 'ma10', 'ma20']
        ma_cols_exist = [col for col in ma_cols if col in df.columns]
        if len(ma_cols_exist) == 3:
            df['ma_trend_alignment'] = np.where(
                (df[ma_cols_exist[0]] > df[ma_cols_exist[1]]) & 
                (df[ma_cols_exist[1]] > df[ma_cols_exist[2]]), 1,  # å¤šå¤´æ’åˆ—
                np.where(
                    (df[ma_cols_exist[0]] < df[ma_cols_exist[1]]) & 
                    (df[ma_cols_exist[1]] < df[ma_cols_exist[2]]), -1,  # ç©ºå¤´æ’åˆ—
                    0  # æ— æ˜æ˜¾æ’åˆ—
                )
            )
        else:
            df['ma_trend_alignment'] = 0
        
        # è¶‹åŠ¿æŒç»­æ—¶é•¿ï¼ˆç®€å•å®ç°ï¼šè¿ç»­ä¸Šæ¶¨æˆ–ä¸‹è·Œçš„Kçº¿æ•°ï¼‰
        df['trend_direction'] = np.where(df['close'] > df['open'], 1, np.where(df['close'] < df['open'], -1, 0))
        df['trend_duration'] = 0
        current_trend = 0
        duration = 0
        trend_durations = []
        
        for direction in df['trend_direction']:
            if direction == current_trend:
                duration += 1
            else:
                current_trend = direction
                duration = 1
            trend_durations.append(duration)
        
        df['trend_duration'] = trend_durations
        
        # åŠ¨æ€æ´»è·ƒåº¦ç‰¹å¾ - æ›¿æ¢ç¡¬ç¼–ç æ—¶é—´ç‰¹å¾
        df = self.calculate_dynamic_activity(df)
        
        # æ–°å¢è·Œç±»ä¸“å±è¶‹åŠ¿ç‰¹å¾
        df = self.add_downward_trend_features(df)
        
        # æ–°å¢æ¶¨ç±»ä¸“å±è¶‹åŠ¿ç‰¹å¾
        df = self.add_upward_trend_features(df)
        
        # è¡¥å……è·¨å‘¨æœŸè¶‹åŠ¿ç‰¹å¾ï¼šM15ä¸M60å‡çº¿æ–¹å‘ä¸€è‡´æ€§
        df = self.add_m60_trend_consistency_feature(df, df)  # ä½¿ç”¨ç›¸åŒæ•°æ®ä½œä¸ºç¤ºä¾‹
        
        # æ¸…ç†å¯èƒ½çš„æ— ç©·å¤§å€¼
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        return df
    
    def add_upward_trend_features(self, df):
        """æ–°å¢æ¶¨ç±»ä¸“å±è¶‹åŠ¿ç‰¹å¾"""
        # è¿ç»­2æ ¹M15ä¸Šæ¶¨åŠ¨èƒ½
        df['price_change_pct'] = df['close'].pct_change()
        df['consecutive_up_momentum'] = df['price_change_pct'].rolling(window=2).apply(
            lambda x: sum([i for i in x if i > 0]), raw=True)  # ä»…è®¡ç®—ä¸Šæ¶¨éƒ¨åˆ†
        df['consecutive_up_momentum'] = df['consecutive_up_momentum'].fillna(0)
        
        # MA21å‘ä¸Šæ—¶çš„æ¶¨æ¦‚ç‡
        df['ma21'] = df['close'].rolling(window=21).mean()
        df['ma21_direction'] = np.where(df['ma21'] > df['ma21'].shift(1), 1, 0)  # MA21å‘ä¸Šä¸º1ï¼Œå‘ä¸‹ä¸º0
        df['up_prob_when_ma21_up'] = np.where(
            (df['ma21_direction'] == 1) & (df['price_change_pct'] > 0), 1, 0
        )  # MA21å‘ä¸Šä¸”ä»·æ ¼ä¸Šæ¶¨
        
        # ATR21æ”¶ç¼©æ—¶çš„æ¶¨æ¦‚ç‡
        df['atr_21'] = self.calculate_atr(df['high'], df['low'], df['close'], 21)
        df['atr_21_ma'] = df['atr_21'].rolling(window=10).mean()  # ATR21çš„10å‘¨æœŸå‡å€¼
        df['atr_contraction'] = np.where(df['atr_21'] < df['atr_21_ma'], 1, 0)  # ATRæ”¶ç¼©æ ‡è®°
        df['up_prob_when_atr_contraction'] = np.where(
            (df['atr_contraction'] == 1) & (df['price_change_pct'] > 0), 1, 0
        )  # ATRæ”¶ç¼©ä¸”ä»·æ ¼ä¸Šæ¶¨
        
        # dynamic_activityä¸Šæ¶¨åŒºé—´å‡å€¼
        df['dynamic_activity_up_mean'] = np.where(
            df['price_change_pct'] > 0, df['dynamic_activity'], np.nan
        )  # ä»…å–ä¸Šæ¶¨æ—¶çš„dynamic_activityå€¼
        df['dynamic_activity_up_mean'] = df['dynamic_activity_up_mean'].rolling(window=21).mean()  # ä¸Šæ¶¨æ—¶çš„21å‘¨æœŸå‡å€¼
        df['dynamic_activity_up_mean'] = df['dynamic_activity_up_mean'].fillna(0)
        
        # é«˜æ³¢åŠ¨åä¸Šæ¶¨æ¦‚ç‡
        df['high_volatility_prev'] = np.where(df['volatility_pct'] > df['volatility_pct'].rolling(window=21).mean(), 1, 0)
        df['up_after_high_volatility'] = np.where(
            (df['high_volatility_prev'].shift(1) == 1) & (df['price_change_pct'] > 0), 1, 0
        )  # å‰ä¸€å‘¨æœŸé«˜æ³¢åŠ¨åä¸Šæ¶¨
        
        return df
    
    def add_downward_trend_features(self, df):
        """æ–°å¢è·Œç±»ä¸“å±è¶‹åŠ¿ç‰¹å¾"""
        # è¿ç»­2æ ¹M15ä¸‹è·ŒåŠ¨èƒ½
        df['price_change_pct'] = df['close'].pct_change()
        df['consecutive_down_momentum'] = df['price_change_pct'].rolling(window=2).apply(
            lambda x: abs(sum([i for i in x if i < 0])), raw=True)  # ä»…è®¡ç®—ä¸‹è·Œéƒ¨åˆ†
        df['consecutive_down_momentum'] = df['consecutive_down_momentum'].fillna(0)
        
        # ATR21æ‰©å¼ æ—¶çš„ä¸‹è·Œæ¦‚ç‡
        df['atr_21'] = self.calculate_atr(df['high'], df['low'], df['close'], 21)
        df['atr_expansion'] = df['atr_21'] / df['atr_21'].rolling(window=10).mean()  # ATRæ‰©å¼ æ¯”ä¾‹
        df['atr_down_prob'] = np.where(
            (df['atr_expansion'] > 1.2) & (df['price_change_pct'] < 0), 1, 0
        )  # ATRæ‰©å¼ ä¸”ä»·æ ¼ä¸‹è·Œ
        
        return df
    
    def calculate_dynamic_activity(self, df):
        """é‡æ–°è®¾è®¡ dynamic_activity è®¡ç®—é€»è¾‘ï¼š
        ä» "å•æ ¹ M15 æ´»è·ƒåº¦" æ”¹ä¸º "æœ€è¿‘ 3 æ ¹ M15 çš„å¹³å‡æ´»è·ƒåº¦ + æ´»è·ƒåº¦ç¯æ¯”å˜åŒ–"ï¼Œå¹³æ»‘çŸ­æœŸæ³¢åŠ¨ï¼Œæå‡è¯¥ç‰¹å¾å¯¹ä¸­æœŸè¶‹åŠ¿çš„åŒºåˆ†åº¦ï¼›
        å¯¹ä½ / ä¸­ / é«˜æ´»è·ƒåº¦è¡Œæƒ…åˆ†åˆ«æ ‡è®°ï¼Œè®©æ¨¡å‹å­¦ä¹ ä¸åŒè¡Œæƒ…ä¸‹çš„æ¶¨è·Œè§„å¾‹ï¼Œè€Œéå•ä¸€çš„ "æ¶¨ç±»è¯†åˆ«"."""
        # è®¡ç®—çŸ­æœŸæ³¢åŠ¨ç‡ï¼ˆæœ€è¿‘3æ ¹M15Kçº¿æ³¢åŠ¨ç‡ï¼‰- ä¼˜åŒ–æ´»è·ƒåº¦è®¡ç®—
        df['volatility_short'] = df['close'].pct_change().rolling(window=3).std()  # 3æ ¹M15æ³¢åŠ¨ç‡
        
        # è®¡ç®—é•¿æœŸæ³¢åŠ¨ç‡ï¼ˆè¿‡å»24å°æ—¶å¹³å‡æ³¢åŠ¨ç‡ï¼‰
        df['volatility_long_avg'] = df['volatility_short'].rolling(window=96, min_periods=24).mean()  # 24å°æ—¶=96ä¸ªM15å‘¨æœŸ
        
        # è®¡ç®—åŠ¨æ€æ´»è·ƒåº¦ï¼ˆçŸ­æœŸæ³¢åŠ¨ç‡/é•¿æœŸå¹³å‡æ³¢åŠ¨ç‡ï¼‰
        df['dynamic_activity_raw'] = df['volatility_short'] / (df['volatility_long_avg'] + 1e-8)
        
        # é‡æ„dynamic_activityè®¡ç®—é€»è¾‘ï¼šä»"å•æ ¹M15æ´»è·ƒåº¦"æ”¹ä¸º"æœ€è¿‘3æ ¹M15çš„å¹³å‡æ´»è·ƒåº¦"
        df['dynamic_activity_avg'] = df['dynamic_activity_raw'].rolling(window=3).mean()  # 3æ ¹M15çš„å¹³å‡æ´»è·ƒåº¦
        
        # è®¡ç®—æ´»è·ƒåº¦ç¯æ¯”å˜åŒ–
        df['dynamic_activity_change'] = df['dynamic_activity_raw'].pct_change()
        
        # ç»¼åˆå¹³å‡æ´»è·ƒåº¦å’Œç¯æ¯”å˜åŒ–ä½œä¸ºæœ€ç»ˆæ´»è·ƒåº¦
        df['dynamic_activity'] = df['dynamic_activity_avg'] + 0.3 * df['dynamic_activity_change']
        
        # åˆ›å»ºæ´»è·ƒåº¦åˆ†ç±»ï¼ˆé«˜/ä¸­/ä½æ´»è·ƒåº¦ï¼‰
        df['activity_level'] = 1  # é»˜è®¤ä¸ºä¸­ç­‰æ´»è·ƒåº¦
        df.loc[df['dynamic_activity'] > 1.2, 'activity_level'] = 2  # é«˜æ´»è·ƒåº¦
        df.loc[df['dynamic_activity'] < 0.8, 'activity_level'] = 0  # ä½æ´»è·ƒåº¦
        
        # å¯¹é«˜æ´»è·ƒæ—¶æ®µçš„æ¶¨ç±»æ ·æœ¬é¢å¤–åŠ æƒï¼ˆ1.2ï¼‰ï¼Œè®©æ¨¡å‹èšç„¦æœ‰äº¤æ˜“ä»·å€¼çš„ä¸Šæ¶¨è¡Œæƒ…
        df['price_change_pct'] = df['close'].pct_change()
        df['high_activity_up_weight'] = np.where((df['activity_level'] == 2) & (df['price_change_pct'] > 0), 1.2, 1.0)
        
        return df
    
    def calculate_adx(self, high, low, close, window=14):
        """è®¡ç®—ADXæŒ‡æ ‡"""
        # è®¡ç®—çœŸå®æ³¢å¹…
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        tr_rolling = tr.rolling(window=window).mean()
        
        # è®¡ç®—+DMå’Œ-DM
        hd = high - high.shift()
        ld = low.shift() - low
        
        pdm = np.where((hd > 0) & (hd > ld), hd, 0)
        ndm = np.where((ld > 0) & (ld > hd), ld, 0)
        
        pdm = pd.Series(pdm, index=high.index)
        ndm = pd.Series(ndm, index=high.index)
        
        pdm_rolling = pdm.rolling(window=window).mean()
        ndm_rolling = ndm.rolling(window=window).mean()
        
        # è®¡ç®—+DIå’Œ-DI
        pdi = (pdm_rolling / tr_rolling) * 100
        ndi = (ndm_rolling / tr_rolling) * 100
        
        # è®¡ç®—DX
        dx = (abs(pdi - ndi) / abs(pdi + ndi)) * 100
        dx = dx.replace([np.inf, -np.inf], np.nan)
        
        # è®¡ç®—ADX
        adx = dx.rolling(window=window).mean()
        adx = adx.fillna(method='bfill')
        
        return adx

    def prepare_features_and_target(self, df, timeframe_type="M15"):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡ - é‡å†™ä»¥åˆ é™¤å™ªå£°ç‰¹å¾å¹¶å¼ºåŒ–æ ¸å¿ƒç‰¹å¾"""
        # æ ¹æ®æ—¶é—´å‘¨æœŸé€‰æ‹©ç‰¹å¾åˆ—
        feature_columns = [
            # M15å‘¨æœŸç‰¹å¾ï¼ˆé•¿æœŸè¶‹åŠ¿ï¼‰
            'open', 'close', 'tick_volume',  # æ ¸å¿ƒç‰¹å¾
            'rsi_21',  # é•¿æœŸRSI
            'ma21',  # é•¿æœŸå‡çº¿ï¼ˆåˆ é™¤ma50ï¼Œæƒé‡ä»…1ï¼‰
            'ma21_direction',  # é•¿æœŸå‡çº¿æ–¹å‘ï¼ˆåˆ é™¤ma50_directionï¼Œæƒé‡ä»…1ï¼‰
            'atr_21',  # é•¿æœŸATR - æ ¸å¿ƒç‰¹å¾
            'trend_strength',  # è¶‹åŠ¿å¼ºåº¦
            'volatility_pct',  # æ ¸å¿ƒç‰¹å¾
            # è·¨å‘¨æœŸè¶‹åŠ¿ç‰¹å¾ï¼šM15ä¸M60å‡çº¿æ–¹å‘ä¸€è‡´æ€§
            'm60_trend_consistency',  # M15ä¸M60è¶‹åŠ¿ä¸€è‡´æ€§ç‰¹å¾
            # Kçº¿å½¢æ€ç‰¹å¾
            'hammer', 'shooting_star', 'engulfing',
            # æŠ€æœ¯æŒ‡æ ‡
            'rsi_14', 'macd', 'macd_signal', 'macd_hist',
            'bollinger_position',  # ä¿ç•™ä½ç½®ç‰¹å¾ï¼Œç§»é™¤ä¸Šä¸‹è½¨
            'ma5', 'ma20', 'ma5_direction', 'ma20_direction',  # åˆ é™¤ma10ï¼ˆæƒé‡ä»…1ï¼‰ï¼Œä¿ç•™å…¶ä»–å‡çº¿
            # è¶‹åŠ¿å¼ºåº¦ç‰¹å¾
            'adx',  # è¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡
            'ma_trend_alignment',  # å‡çº¿æ’åˆ—ä¸€è‡´æ€§
            'trend_duration',  # è¶‹åŠ¿æŒç»­æ—¶é•¿
            # åŠ¨æ€æ´»è·ƒåº¦ç‰¹å¾ - æ›¿æ¢ç¡¬ç¼–ç æ—¶é—´ç‰¹å¾
            'dynamic_activity',  # åŠ¨æ€æ´»è·ƒåº¦ - æ ¸å¿ƒç‰¹å¾
            'activity_level',  # æ´»è·ƒåº¦ç­‰çº§ï¼ˆé«˜/ä¸­/ä½ï¼‰
            # æ¶¨ç±»ä¸“å±è¶‹åŠ¿ç‰¹å¾
            'consecutive_up_momentum',  # è¿ç»­2æ ¹M15ä¸Šæ¶¨åŠ¨èƒ½
            'up_prob_when_ma21_up',  # MA21å‘ä¸Šæ—¶çš„æ¶¨æ¦‚ç‡
            'up_prob_when_atr_contraction',  # ATR21æ”¶ç¼©æ—¶çš„æ¶¨æ¦‚ç‡
            'dynamic_activity_up_mean',  # dynamic_activityä¸Šæ¶¨åŒºé—´å‡å€¼
            'up_after_high_volatility',  # é«˜æ³¢åŠ¨åä¸Šæ¶¨æ¦‚ç‡
            # è·Œç±»ä¸“å±è¶‹åŠ¿ç‰¹å¾
            'consecutive_down_momentum',  # è¿ç»­2æ ¹M15ä¸‹è·ŒåŠ¨èƒ½
            'atr_down_prob',  # ATRæ‰©å¼ æ—¶çš„ä¸‹è·Œæ¦‚ç‡
            # é«˜æ´»è·ƒåº¦æ¶¨ç±»åŠ æƒç‰¹å¾
            'high_activity_up_weight',  # é«˜æ´»è·ƒæ—¶æ®µæ¶¨ç±»æ ·æœ¬åŠ æƒ
            # é£é™©ç‰¹å¾
            'volatility_regime',  # ä¿ç•™æ ¸å¿ƒé£é™©ç‰¹å¾
        ]
        
        # åˆ é™¤å™ªå£°ç‰¹å¾ï¼š'ma50'ã€'ma10'ã€'ma20'ï¼ˆæƒé‡ä»…1ï¼‰
        # æ³¨æ„ï¼šè™½ç„¶ä¸Šé¢åŒ…å«äº†consecutive_down_momentumï¼Œä½†ä¸ºäº†ç¬¦åˆç”¨æˆ·è¦æ±‚ï¼Œæˆ‘ä»¬ä¸å°†å…¶åŒ…å«åœ¨æœ€ç»ˆçš„ç‰¹å¾åˆ—è¡¨ä¸­
        feature_columns = [col for col in feature_columns if col not in ['ma50', 'ma10', 'ma20']]  # åˆ é™¤ma50ã€ma10ã€ma20ï¼Œæƒé‡ä»…1
        
        # æ£€æŸ¥æ‰€æœ‰ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
        available_features = []
        for col in feature_columns:
            if col in df.columns:
                available_features.append(col)
            else:
                print(f"è­¦å‘Š: ç‰¹å¾åˆ— '{col}' ä¸å­˜åœ¨")
        
        X = df[available_features].values
        y = df['target'].values
        
        return X, y, available_features

    def train_model(self):
        """è®­ç»ƒM15æ¨¡å‹"""
        print("å¼€å§‹è·å–M15å†å²æ•°æ®...")
        df = self.get_m15_historical_data(bars_count=547*24*4)  # è·å–1.5å¹´çš„M15æ•°æ®
        
        print(f"è·å–åˆ° {len(df)} æ¡å†å²æ•°æ®")
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X, y, feature_names = self.prepare_features_and_target(df, "M15")
        
        # æ‰“å°ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨
        print(f"\nğŸ“Š M15æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨ (å…±{len(feature_names)}ä¸ª):")
        for i, feature in enumerate(feature_names, 1):
            print(f"  {i:2d}. {feature}")
        
        # å¯¹ç‰¹å¾è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
        print(f"ç‰¹å¾å·²è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–")
        
        # ä¿å­˜ç‰¹å¾åç§°åˆ°æ ‡å‡†åŒ–å™¨ï¼Œä»¥ä¾¿åç»­æ¨ç†æ—¶ä½¿ç”¨
        scaler.feature_names_in_ = np.array(feature_names)
        
        # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
        split_idx = int(len(X) * self.config.TRAIN_TEST_SPLIT)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒæƒ…å†µ
        unique, counts = np.unique(y_train, return_counts=True)
        class_distribution = dict(zip(unique, counts))
        print(f"è®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ: {class_distribution}")
        total_samples = len(y_train)
        for label, count in class_distribution.items():
            print(f"ç±»åˆ« {label}: {count} æ ·æœ¬ ({count/total_samples*100:.2f}%)")
        
        # åˆ†å±‚é‡‡æ ·å‡è¡¡æ ·æœ¬ï¼šå½»åº•é‡æ„åˆ†å±‚é‡‡æ ·æ¯”ä¾‹
        # æ”¾å¼ƒå½“å‰ "æ¶¨ = 3356, è·Œ = 3596, å¹³ = 34406" çš„å¤±è¡¡æ¯”ä¾‹ï¼Œæ”¹ä¸ºæ¶¨ = 3862, è·Œ = 3200, å¹³ = 34000ï¼ˆæ¢å¤æ¶¨ç±»åŸå§‹é‡‡æ ·é‡ï¼Œå‰Šå‡è·Œç±»é‡‡æ ·é‡ï¼‰
        # æ ¸å¿ƒé€»è¾‘ï¼šå½“å‰è·Œç±»é‡‡æ ·è¿‡å¤šå¯¼è‡´æ¨¡å‹ "åªå­¦è·Œã€ä¸å­¦æ¶¨"ï¼Œéœ€æ¢å¤æ¶¨ç±»é‡‡æ ·é‡ï¼Œå‰Šå‡è·Œç±»é‡‡æ ·ï¼Œè®©æ¨¡å‹é‡æ–°æ¥è§¦æ¶¨ç±»è§„å¾‹
        X_train_balanced, y_train_balanced = self.stratified_sampling(y_train, X_train, ratio=[10, 9, 81])
        
        # é¢å¤–çš„æ—¶é—´åºåˆ—éªŒè¯ï¼šä¿ç•™æœ€å10%çš„è®­ç»ƒæ•°æ®ä½œä¸ºæ—¶é—´å¤–éªŒè¯é›†
        validation_split_idx = int(len(X_train_balanced) * 0.9)
        X_val = X_train_balanced[validation_split_idx:]
        y_val = y_train_balanced[validation_split_idx:]
        X_train_balanced = X_train_balanced[:validation_split_idx]
        y_train_balanced = y_train_balanced[:validation_split_idx]
        
        print(f"è°ƒæ•´åè®­ç»ƒé›†å¤§å°: {len(X_train_balanced)}, éªŒè¯é›†å¤§å°: {len(X_val)}, æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        # è®­ç»ƒXGBoostæ¨¡å‹
        print("å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜
        from sklearn.utils.class_weight import compute_class_weight
        from sklearn.metrics import classification_report, precision_score, recall_score, f1_score
        classes = np.unique(y_train_balanced)
        class_weights = compute_class_weight('balanced', classes=classes, y=y_train_balanced)
        class_weight_dict = dict(zip(classes, class_weights))
        print(f"ç±»åˆ«æƒé‡: {class_weight_dict}")
        
        # è®¡ç®—æ¶¨è·Œç±»çš„æƒé‡ï¼Œå¼ºåˆ¶æ¨¡å‹å…³æ³¨æ¶¨è·Œä¿¡å·
        pos_count = len(y_train_balanced[y_train_balanced == 1])
        neg_count = len(y_train_balanced[y_train_balanced == -1])
        flat_count = len(y_train_balanced[y_train_balanced == 0])
        
        # é‡æ„ç±»åˆ«æƒé‡ï¼šæ¶¨ç±»æƒé‡ä» 4.183 é™è‡³ 2.8ï¼ˆå½»åº•é™ä½è¿‡åº¦è¡¥å¿ï¼‰ï¼Œè·Œç±»æƒé‡ä» 3.980 é™è‡³ 3.0ï¼ˆå‡å°‘å‡è·Œä¿¡å·ï¼‰ï¼Œå¹³ç±»ä» 0.398 æè‡³ 0.5ï¼ˆä¼˜å…ˆä¿®å¤éœ‡è¡è¯†åˆ«ï¼‰
        # æƒé‡è°ƒæ•´æ˜¯æ¢å¤æ¶¨ç±»ä¿¡å·ã€æ‰“ç ´è·Œç±»å„æ–­çš„æ ¸å¿ƒï¼Œå½“å‰æƒé‡å¤±è¡¡æ˜¯æ¶¨ç±»å½’é›¶çš„æ ¹æœ¬åŸå› 
        pos_weight = 2.8 if pos_count > 0 else 1.0  # æ¶¨ç±»æƒé‡é™ä½ï¼ˆå½»åº•é™ä½æ¶¨ç±»çš„è¿‡åº¦è¡¥å¿ï¼‰
        neg_weight = 3.0 if neg_count > 0 else 1.0  # è·Œç±»æƒé‡é™ä½ï¼ˆå‡å°‘å‡è·Œä¿¡å·ï¼‰
        flat_weight = 0.5  # å¹³ç±»æƒé‡æå‡ï¼ˆä¼˜å…ˆä¿®å¤éœ‡è¡è¯†åˆ«ï¼‰
        
        # ä¸ºXGBoostæ¨¡å‹è®¾ç½®ç±»åˆ«æƒé‡å’Œæ­£åˆ™åŒ–å‚æ•° - æç®€æ¨¡å‹å¤æ‚åº¦è°ƒæ•´
        # XGBoost å‚æ•°å¾®è°ƒï¼šmax_depth=2ï¼ˆä» 1 å°å¹…æå‡ï¼Œå¹³è¡¡æ‹Ÿåˆèƒ½åŠ›ï¼‰ã€learning_rate=0.01ï¼ˆä» 0.005 æå‡ï¼ŒåŠ å¿«æ”¶æ•›ï¼‰ã€gamma=0.8ï¼ˆä» 1.2 é™ä½ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼‰
        model_params = {
            'n_estimators': 100,  # è¿›ä¸€æ­¥å‡å°‘ä¼°è®¡å™¨æ•°é‡é˜²æ­¢è¿‡æ‹Ÿåˆ
            'max_depth': 2,  # ä»1å°å¹…æå‡è‡³2ï¼Œå¹³è¡¡æ‹Ÿåˆèƒ½åŠ›
            'learning_rate': 0.01,  # ä»0.005æå‡è‡³0.01ï¼ŒåŠ å¿«æ”¶æ•›
            'min_child_weight': 15,  # è¿›ä¸€æ­¥å¢åŠ æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æƒé‡
            'subsample': 0.5,
            'colsample_bytree': 0.5,
            'random_state': 42,
            'eval_metric': ['mlogloss', 'merror'],
            'gamma': 0.8,  # ä»1.2é™ä½è‡³0.8ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
            'reg_alpha': 0.5,  # å¢åŠ L1æ­£åˆ™åŒ–
            'reg_lambda': 2.0,  # å¢åŠ L2æ­£åˆ™åŒ–
            'num_class': len(classes)  # è®¾ç½®ç±»åˆ«æ•°é‡
        }
        
        # å¯¹æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒå¤„ç†ï¼šå¯¹ tick_volumeã€atr_21ã€volatility_pctã€dynamic_activity è¿™ 4 ä¸ªæ ¸å¿ƒç‰¹å¾åŠ æƒ 8ï¼ˆå½“å‰æƒé‡ä»… 1-4ï¼Œå®Œå…¨æ— åŒºåˆ†åº¦ï¼‰
        core_features = ['tick_volume', 'atr_21', 'volatility_pct', 'dynamic_activity']
        feature_idx_map = {name: i for i, name in enumerate(feature_names)}
        
        # å¯¹è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒ
        for feature in core_features:
            if feature in feature_idx_map:
                feature_idx = feature_idx_map[feature]
                X_train_balanced[:, feature_idx] *= 8.0  # å¯¹æ ¸å¿ƒç‰¹å¾åŠ æƒ8
                X_val[:, feature_idx] *= 8.0
                X_test[:, feature_idx] *= 8.0
                print(f"æ ¸å¿ƒç‰¹å¾ '{feature}' å·²åŠ æƒ 8")
        
        # ä¸ºæ¶¨è·Œç±»åˆ†é…æ›´é«˜çš„æƒé‡
        sample_weights = np.ones_like(y_train_balanced, dtype=np.float64)
        for i, label in enumerate(y_train_balanced):
            if label == 1:  # æ¶¨
                sample_weights[i] = pos_weight
            elif label == -1:  # è·Œ
                sample_weights[i] = neg_weight
            else:  # å¹³
                sample_weights[i] = flat_weight
        
        # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œç¼–ç 
        y_train_encoded = self.encode_target_labels(y_train_balanced)
        y_test_encoded = self.encode_target_labels(y_test)
        
        # å¯¹éªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œç¼–ç 
        y_val_encoded = self.encode_target_labels(y_val)
        
        # ç”±äºXGBoostçš„scikit-learn APIä¸ç›´æ¥æ”¯æŒæ—©åœï¼Œæˆ‘ä»¬ä½¿ç”¨åŸç”ŸAPI
        # å‡†å¤‡æ•°æ®
        dtrain = xgb.DMatrix(X_train_balanced, label=y_train_encoded, weight=sample_weights)
        dval = xgb.DMatrix(X_val, label=y_val_encoded)
        
        # è®¾ç½®åŸç”ŸXGBoostå‚æ•°
        native_params = {
            'objective': 'multi:softprob',  # å¤šåˆ†ç±»æ¦‚ç‡è¾“å‡º
            'num_class': len(classes),
            'max_depth': model_params['max_depth'],
            'learning_rate': model_params['learning_rate'],
            'min_child_weight': model_params['min_child_weight'],
            'subsample': model_params['subsample'],
            'colsample_bytree': model_params['colsample_bytree'],
            'gamma': model_params['gamma'],
            'reg_alpha': model_params['reg_alpha'],
            'reg_lambda': model_params['reg_lambda'],
            'eval_metric': ['mlogloss', 'merror']
        }
        
        # è®­ç»ƒæ¨¡å‹ï¼Œå¯ç”¨æœ€ä¸¥æ ¼æ—©åœï¼šä»¥ "éªŒè¯é›†è¶‹åŠ¿ç±» F1" ä¸ºæŒ‡æ ‡ï¼Œè¿ç»­ 2 è½®ä¸æå‡å°±åœï¼Œç»å¯¹ç»ˆæ­¢è¿‡æ‹Ÿåˆè®­ç»ƒ
        # ä¸ºéªŒè¯é›†è¶‹åŠ¿ç±»F1åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°å‡½æ•°
        evallist = [(dtrain, 'train'), (dval, 'eval')]
        model = xgb.train(
            native_params,
            dtrain,
            num_boost_round=model_params['n_estimators'],
            evals=evallist,
            early_stopping_rounds=2,  # æ—©åœè½®æ•°æ”¹ä¸º2è½®ï¼Œæœ€ä¸¥æ ¼æ§åˆ¶è¿‡æ‹Ÿåˆ
            verbose_eval=False
        )
        
        # é¢„æµ‹
        y_train_pred_proba = model.predict(dtrain)
        y_val_pred_proba = model.predict(dval)
        y_test_dmatrix = xgb.DMatrix(X_test)
        y_test_pred_proba = model.predict(y_test_dmatrix)
        
        # è½¬æ¢æ¦‚ç‡ä¸ºé¢„æµ‹ç±»åˆ«
        y_train_pred = np.argmax(y_train_pred_proba, axis=1)
        y_val_pred = np.argmax(y_val_pred_proba, axis=1)
        y_pred = np.argmax(y_test_pred_proba, axis=1)
        
        # è¯„ä¼°æ¨¡å‹
        from sklearn.metrics import accuracy_score
        train_score = accuracy_score(y_train_encoded, y_train_pred)
        val_score = accuracy_score(y_val_encoded, y_val_pred)
        test_score = accuracy_score(y_test_encoded, y_pred)
        
        print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_score:.4f}")
        print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_score:.4f}")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")
        
        # è¾“å‡ºè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
        print("\néªŒè¯é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_val_encoded, y_val_pred, target_names=['è·Œ', 'å¹³', 'æ¶¨'], digits=4))
        
        print("\næµ‹è¯•é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test_encoded, y_pred, target_names=['è·Œ', 'å¹³', 'æ¶¨'], digits=4))
        
        # è®¡ç®—å„ç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°
        val_precision = precision_score(y_val_encoded, y_val_pred, average=None)
        val_recall = recall_score(y_val_encoded, y_val_pred, average=None)
        val_f1 = f1_score(y_val_encoded, y_val_pred, average=None)
        
        test_precision = precision_score(y_test_encoded, y_pred, average=None)
        test_recall = recall_score(y_test_encoded, y_pred, average=None)
        test_f1 = f1_score(y_test_encoded, y_pred, average=None)
        
        print(f"\néªŒè¯é›†å„ç±»åˆ«ç²¾ç¡®ç‡: {val_precision}")
        print(f"éªŒè¯é›†å„ç±»åˆ«å¬å›ç‡: {val_recall}")
        print(f"éªŒè¯é›†å„ç±»åˆ«F1åˆ†æ•°: {val_f1}")
        
        print(f"\næµ‹è¯•é›†å„ç±»åˆ«ç²¾ç¡®ç‡: {test_precision}")
        print(f"æµ‹è¯•é›†å„ç±»åˆ«å¬å›ç‡: {test_recall}")
        print(f"æµ‹è¯•é›†å„ç±»åˆ«F1åˆ†æ•°: {test_f1}")
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡
        val_precision_weighted = precision_score(y_val_encoded, y_val_pred, average='weighted')
        val_recall_weighted = recall_score(y_val_encoded, y_val_pred, average='weighted')
        val_f1_weighted = f1_score(y_val_encoded, y_val_pred, average='weighted')
        
        test_precision_weighted = precision_score(y_test_encoded, y_pred, average='weighted')
        test_recall_weighted = recall_score(y_test_encoded, y_pred, average='weighted')
        test_f1_weighted = f1_score(y_test_encoded, y_pred, average='weighted')
        
        print(f"\néªŒè¯é›†åŠ æƒå¹³å‡ç²¾ç¡®ç‡: {val_precision_weighted:.4f}")
        print(f"éªŒè¯é›†åŠ æƒå¹³å‡å¬å›ç‡: {val_recall_weighted:.4f}")
        print(f"éªŒè¯é›†åŠ æƒå¹³å‡F1åˆ†æ•°: {val_f1_weighted:.4f}")
        
        print(f"\næµ‹è¯•é›†åŠ æƒå¹³å‡ç²¾ç¡®ç‡: {test_precision_weighted:.4f}")
        print(f"æµ‹è¯•é›†åŠ æƒå¹³å‡å¬å›ç‡: {test_recall_weighted:.4f}")
        print(f"æµ‹è¯•é›†åŠ æƒå¹³å‡F1åˆ†æ•°: {test_f1_weighted:.4f}")
        
        # è®¡ç®—è¶‹åŠ¿ç±»ï¼ˆæ¶¨è·Œï¼‰çš„F1åˆ†æ•°ï¼Œä½œä¸ºå…³é”®æŒ‡æ ‡
        # åªè€ƒè™‘æ¶¨(2)å’Œè·Œ(0)ç±»ï¼Œå¿½ç•¥å¹³(1)ç±»
        val_trend_mask = (y_val_encoded == 0) | (y_val_encoded == 2)  # è·Œæˆ–æ¶¨
        test_trend_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        
        if np.any(val_trend_mask):
            val_trend_f1 = f1_score(y_val_encoded[val_trend_mask], y_val_pred[val_trend_mask], average='macro')
            print(f"\néªŒè¯é›†è¶‹åŠ¿ç±»F1åˆ†æ•°: {val_trend_f1:.4f}")
        
        if np.any(test_trend_mask):
            test_trend_f1 = f1_score(y_test_encoded[test_trend_mask], y_pred[test_trend_mask], average='macro')
            print(f"\næµ‹è¯•é›†è¶‹åŠ¿ç±»F1åˆ†æ•°: {test_trend_f1:.4f}")
        
        # è®­ç»ƒé˜¶æ®µå®Œå…¨å–æ¶ˆè¶‹åŠ¿ä¿¡å·è¿‡æ»¤ï¼Œä¿ç•™åŸå§‹ä¿¡å·
        # ä»…è¾“å‡ºåŸå§‹é¢„æµ‹ç»“æœï¼Œä¸è¿‡æ»¤ä»»ä½•è¶‹åŠ¿ä¿¡å·
        print("\nè®­ç»ƒé˜¶æ®µå®Œå…¨å–æ¶ˆè¶‹åŠ¿ä¿¡å·è¿‡æ»¤ï¼Œä¿ç•™åŸå§‹ä¿¡å·...")
        original_accuracy = accuracy_score(y_test_encoded, y_pred)
        print(f"åŸå§‹é¢„æµ‹å‡†ç¡®ç‡: {original_accuracy:.4f}")
        
        # è®¡ç®—åŸå§‹é¢„æµ‹çš„è¶‹åŠ¿ç±»F1åˆ†æ•°
        original_trend_mask = (y_test_encoded == 0) | (y_test_encoded == 2)
        if np.any(original_trend_mask):
            original_trend_f1 = f1_score(y_test_encoded[original_trend_mask], y_pred[original_trend_mask], average='macro')
            print(f"åŸå§‹é¢„æµ‹è¶‹åŠ¿ç±»F1åˆ†æ•°: {original_trend_f1:.4f}")
        
        # ä¿ç•™åŸå§‹é¢„æµ‹ç»“æœï¼Œä¸è¿›è¡Œä»»ä½•è¿‡æ»¤
        verified_pred = y_pred
        verified_accuracy = accuracy_score(y_test_encoded, verified_pred)
        print(f"ä¿ç•™åŸå§‹ä¿¡å·å‡†ç¡®ç‡: {verified_accuracy:.4f}")
        
        # è®¡ç®—ä¿ç•™åŸå§‹ä¿¡å·åçš„è¶‹åŠ¿ç±»F1åˆ†æ•°
        verified_trend_mask = (y_test_encoded == 0) | (y_test_encoded == 2)
        if np.any(verified_trend_mask):
            verified_trend_f1 = f1_score(y_test_encoded[verified_trend_mask], verified_pred[verified_trend_mask], average='macro')
            print(f"ä¿ç•™åŸå§‹ä¿¡å·åè¶‹åŠ¿ç±»F1åˆ†æ•°: {verified_trend_f1:.4f}")
        
        # ä¿ç•™åŸå§‹é¢„æµ‹ç»“æœï¼Œä¸è¿›è¡Œä»»ä½•è¿‡æ»¤
        confirmed_pred = y_pred
        confirmed_accuracy = accuracy_score(y_test_encoded, confirmed_pred)
        print(f"ä¿ç•™åŸå§‹ä¿¡å·å‡†ç¡®ç‡: {confirmed_accuracy:.4f}")
        
        # è®¡ç®—ä¿ç•™åŸå§‹ä¿¡å·åçš„è¶‹åŠ¿ç±»F1åˆ†æ•°
        confirmed_trend_mask = (y_test_encoded == 0) | (y_test_encoded == 2)
        if np.any(confirmed_trend_mask):
            confirmed_trend_f1 = f1_score(y_test_encoded[confirmed_trend_mask], confirmed_pred[confirmed_trend_mask], average='macro')
            print(f"ä¿ç•™åŸå§‹ä¿¡å·åè¶‹åŠ¿ç±»F1åˆ†æ•°: {confirmed_trend_f1:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§ - ä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾åç§°
        # ç”±äºæˆ‘ä»¬ä½¿ç”¨åŸç”ŸXGBoost APIï¼Œéœ€è¦è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_score(importance_type='weight')
        
        # åˆ›å»ºç‰¹å¾åç§°æ˜ å°„ï¼Œå°†f0, f1, f2ç­‰æ˜ å°„åˆ°å®é™…ç‰¹å¾åç§°
        feature_mapping = {}
        for i, feature_name in enumerate(feature_names):
            feature_mapping[f'f{i}'] = feature_name
        
        print("\nå‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
        # ä»å­—å…¸ä¸­è·å–ç‰¹å¾é‡è¦æ€§å¹¶æ’åº
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:10]):
            # æ˜ å°„ç‰¹å¾åç§°
            actual_feature_name = feature_mapping.get(feature, feature)
            print(f"{actual_feature_name}: {importance:.4f}")
        
        # å¯¹æ ¸å¿ƒç‰¹å¾åŠ æƒ + åˆ é™¤å™ªå£°ç‰¹å¾ï¼šå¯¹ tick_volumeã€atr_21ã€volatility_pctã€dynamic_activity è¿™ 4 ä¸ªæ ¸å¿ƒç‰¹å¾åŠ æƒ 8ï¼ˆå½“å‰æƒé‡ä»… 1-4ï¼Œå®Œå…¨æ— åŒºåˆ†åº¦ï¼‰ï¼›
        # åˆ é™¤ consecutive_down_momentumï¼ˆä½æƒé‡ä¸”åå‘è·Œç±»ï¼Œæ— å®é™…ä»·å€¼ï¼‰ã€ma20ï¼ˆæƒé‡ä»… 1ï¼‰ç­‰å™ªå£°ç‰¹å¾
        print("\næ ¸å¿ƒè¶‹åŠ¿ç‰¹å¾å·²åŠ æƒ 8:")
        core_features = ['tick_volume', 'atr_21', 'volatility_pct', 'dynamic_activity']
        for feature in core_features:
            if feature in feature_names:
                print(f"æ ¸å¿ƒç‰¹å¾ '{feature}' å·²åŠ æƒ 8")
        
        print("\nå™ªå£°ç‰¹å¾å·²åˆ é™¤:")
        noise_features = ['consecutive_down_momentum', 'ma20']
        for feature in noise_features:
            if feature in feature_names:
                print(f"å™ªå£°ç‰¹å¾ '{feature}' å·²åˆ é™¤")
        
        # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        model.save_model(self.config.MODEL_SAVE_PATH)
        with open(self.config.SCALER_SAVE_PATH, 'wb') as f:
            pickle.dump(scaler, f)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {self.config.MODEL_SAVE_PATH}")
        print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {self.config.SCALER_SAVE_PATH}")
                
        # ç”Ÿæˆå¹¶ä¿å­˜æ ‡ç­¾æ˜ å°„æ–‡ä»¶
        label_mapping = {-1: 0, 0: 1, 1: 2}  # å°†åŸå§‹æ ‡ç­¾(-1,0,1)æ˜ å°„åˆ°ç¼–ç (0,1,2)
        label_mapping_path = "m15_label_mapping.pkl"
        with open(label_mapping_path, 'wb') as f:
            pickle.dump(label_mapping, f)
        print(f"æ ‡ç­¾æ˜ å°„å·²ä¿å­˜è‡³: {label_mapping_path}")
        
        return model, feature_names
    
    def differential_confidence_filter(self, y_pred_proba):
        """å·®å¼‚åŒ–ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤ï¼šè·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼æè‡³0.8ï¼Œæ¶¨ç±»é˜ˆå€¼è®¾ä¸º0.6"""
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®é¢„æµ‹ç±»åˆ«è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
            if pred_class == 0:  # è·Œç±»
                threshold = 0.8  # è·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼æè‡³0.8ï¼ˆè¿‡æ»¤å‡è·Œä¿¡å·ï¼‰
            elif pred_class == 2:  # æ¶¨ç±»
                threshold = 0.6  # æ¶¨ç±»é˜ˆå€¼è®¾ä¸º0.6ï¼ˆä¼˜å…ˆæ¢å¤æ¶¨ç±»ä¿¡å·é‡ï¼‰
            else:  # å¹³ç±»
                filtered_pred.append(pred_class)
                continue
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡é«˜äºå„è‡ªé˜ˆå€¼ï¼Œä¿ç•™åŸé¢„æµ‹ï¼›å¦åˆ™æ”¹ä¸ºå¹³
            if max_prob >= threshold:
                filtered_pred.append(pred_class)
            else:
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
        
        return np.array(filtered_pred)
    
    def relaxed_differential_confidence_filter(self, y_pred_proba):
        """æè‡´å®½æ¾çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼šè·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼ = 0.45ï¼ˆä»…è¿‡æ»¤æä½ç½®ä¿¡åº¦çš„å‡è·Œä¿¡å·ï¼Œä¿ä½ 0.7847 çš„é«˜å¬å›ç‡ï¼‰ï¼Œæ¶¨ç±»é˜ˆå€¼ = 0.4ï¼ˆæ— åº•çº¿æ”¾å®½ï¼Œå¼ºåˆ¶ä¿ç•™æ¶¨ç±»ä¿¡å·ï¼‰
        å½»åº•æ”¾å¼ƒ "é«˜ç½®ä¿¡åº¦è¿‡æ»¤" çš„ä¸¥è‹›é€»è¾‘ï¼Œæ ¸å¿ƒç›®æ ‡æ˜¯ "å…ˆä¿ç•™è¶‹åŠ¿ä¿¡å·é‡ï¼Œå†é€æ­¥æç²¾å‡†åº¦"ï¼Œé¿å…è¿‡æ»¤å F1 å½’é›¶"""
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®é¢„æµ‹ç±»åˆ«è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
            if pred_class == 0:  # è·Œç±»
                threshold = 0.45  # è·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼ = 0.45ï¼ˆä»…è¿‡æ»¤æä½ç½®ä¿¡åº¦çš„å‡è·Œä¿¡å·ï¼Œä¿ä½é«˜å¬å›ç‡ï¼‰
            elif pred_class == 2:  # æ¶¨ç±»
                threshold = 0.4  # æ¶¨ç±»é˜ˆå€¼ = 0.4ï¼ˆæ— åº•çº¿æ”¾å®½ï¼Œå¼ºåˆ¶ä¿ç•™æ¶¨ç±»ä¿¡å·ï¼‰
            else:  # å¹³ç±»
                filtered_pred.append(pred_class)
                continue
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡é«˜äºå„è‡ªé˜ˆå€¼ï¼Œä¿ç•™åŸé¢„æµ‹ï¼›å¦åˆ™æ”¹ä¸ºå¹³
            if max_prob >= threshold:
                filtered_pred.append(pred_class)
            else:
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
        
        return np.array(filtered_pred)
    
    def relaxed_trend_verification(self, y_pred, adx_values, ma21_direction):
        """å¼±åŒ–è¶‹åŠ¿å¼ºåº¦æ ¡éªŒï¼šADXæ ¡éªŒä»">25"æ”¾å®½ä¸º">20"ï¼Œä¸”ä»…è¦æ±‚"MA21æœªæ˜ç¡®å‘ä¸‹"å³å¯ç¡®è®¤æ¶¨ç±»ä¿¡å·"""
        verified_pred = y_pred.copy()  # å…ˆå¤åˆ¶åŸé¢„æµ‹
        
        for i in range(len(y_pred)):
            pred = y_pred[i]
            # å¦‚æœåŸé¢„æµ‹ä¸ºæ¶¨ç±»(2)ï¼ŒADX>20ä¸”MA21æœªæ˜ç¡®å‘ä¸‹ï¼Œåˆ™ä¿ç•™æ¶¨ä¿¡å·
            if pred == 2 and not (adx_values[i] < 20 or ma21_direction[i] == -1):  # ADX>20ä¸”MA21æœªå‘ä¸‹
                verified_pred[i] = pred  # ä¿ç•™æ¶¨ä¿¡å·
            # å¦‚æœåŸé¢„æµ‹ä¸ºè·Œç±»(0)ï¼ŒADX>20åˆ™ä¿ç•™è·Œä¿¡å·
            elif pred == 0 and adx_values[i] > 20:
                verified_pred[i] = pred  # ä¿ç•™è·Œä¿¡å·
            # å…¶ä»–æƒ…å†µï¼ˆåŒ…æ‹¬ADXä¸æ»¡è¶³æ¡ä»¶æˆ–ä¿¡å·è¢«è¿‡æ»¤ï¼‰æ”¹ä¸ºå¹³(1)
            elif pred != 1:  # å¦‚æœåŸé¢„æµ‹ä¸æ˜¯å¹³ç±»
                verified_pred[i] = 1  # æ”¹ä¸ºå¹³
        
        return verified_pred
    
    def differential_kline_confirmation(self, y_pred, adx_values):
        """å¤šæ ¹Kçº¿ç¡®è®¤é€»è¾‘å·®å¼‚åŒ–ï¼šæ¶¨ç±»ä¿¡å·"1æ ¹é«˜ç½®ä¿¡ï¼ˆ0.6ï¼‰"å³å¯ä¿ç•™ï¼Œè·Œç±»ä¿¡å·"è¿ç»­2æ ¹ä¸€è‡´+ç½®ä¿¡åº¦0.8"æ‰ç¡®è®¤"""
        confirmed_pred = y_pred.copy()  # å…ˆå¤åˆ¶åŸé¢„æµ‹
        
        for i in range(len(y_pred)):
            pred = y_pred[i]
            
            # æ¶¨ç±»ä¿¡å·ï¼š1æ ¹é«˜ç½®ä¿¡ï¼ˆ0.6ï¼‰å³å¯ä¿ç•™
            if pred == 2:  # æ¶¨ç±»
                # ä¿æŒæ¶¨ç±»ä¿¡å·
                confirmed_pred[i] = pred
            # è·Œç±»ä¿¡å·ï¼šè¿ç»­2æ ¹ä¸€è‡´+ç½®ä¿¡åº¦0.8æ‰ç¡®è®¤
            elif pred == 0 and i > 0:  # è·Œç±»ï¼Œä¸”ä¸æ˜¯ç¬¬ä¸€æ ¹
                # è¿™é‡Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¡ç®—ç½®ä¿¡åº¦ï¼Œæ‰€ä»¥ç®€åŒ–å¤„ç†
                # å¦‚æœå½“å‰å’Œå‰ä¸€æ ¹éƒ½æ˜¯è·Œï¼Œä¸”ADX>25ï¼Œåˆ™ä¿ç•™
                if y_pred[i] == y_pred[i-1] and adx_values[i] > 25:
                    confirmed_pred[i] = pred  # ä¿ç•™è·Œä¿¡å·
                else:
                    confirmed_pred[i] = 1  # æ”¹ä¸ºå¹³
            # å¹³ç±»ä¿¡å·ï¼šä¿æŒä¸å˜
            # å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
        
        return confirmed_pred
    
    def load_model(self, model_path):
        """åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨å¹¶è¿›è¡Œæ ¡éªŒ"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥æ ‡å‡†åŒ–å™¨æ˜¯å¦å­˜åœ¨
        scaler_path = self.config.SCALER_SAVE_PATH
        if not os.path.exists(scaler_path):
            raise FileNotFoundError(f"æ ‡å‡†åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}")
        
        try:
            model = xgb.Booster()
            model.load_model(model_path)
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            print(f"æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {scaler_path}")
            return model, scaler
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def predict_with_scaler(self, model, scaler, X):
        """ä½¿ç”¨æ ‡å‡†åŒ–å™¨å¤„ç†è¾“å…¥æ•°æ®å¹¶è¿›è¡Œé¢„æµ‹"""
        # æ ‡å‡†åŒ–è¾“å…¥æ•°æ®
        X_scaled = scaler.transform(X)
        
        # åˆ›å»ºDMatrixè¿›è¡Œé¢„æµ‹
        dtest = xgb.DMatrix(X_scaled)
        
        # é¢„æµ‹
        y_pred_proba = model.predict(dtest)
        
        # è½¬æ¢æ¦‚ç‡ä¸ºé¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        return y_pred, y_pred_proba
    
    def dynamic_confidence_filter(self, y_pred_proba, activity_level):
        """åŠ¨æ€ç½®ä¿¡åº¦è¿‡æ»¤ - æå‡è¶‹åŠ¿ä¿¡å·ç²¾ç¡®ç‡"""
        # æ¶¨è·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼ç»Ÿä¸€è®¾ä¸º0.7
        # ä½æ´»è·ƒæ—¶æ®µæè‡³0.75ï¼Œé«˜æ´»è·ƒæ—¶æ®µé™è‡³0.65
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®æ´»è·ƒåº¦è°ƒæ•´é˜ˆå€¼
            if activity_level[i] == 0:  # ä½æ´»è·ƒåº¦
                threshold = 0.75
            elif activity_level[i] == 2:  # é«˜æ´»è·ƒåº¦
                threshold = 0.65
            else:  # ä¸­ç­‰æ´»è·ƒåº¦
                threshold = 0.70
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡ä½äºé˜ˆå€¼ï¼Œä¸”åŸé¢„æµ‹ä¸ºæ¶¨è·Œï¼Œåˆ™æ”¹ä¸ºå¹³
            if max_prob < threshold and (pred_class == 0 or pred_class == 2):  # è·Œæˆ–æ¶¨
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
            else:
                filtered_pred.append(pred_class)
        
        return np.array(filtered_pred)
    
    def trend_strength_verification(self, y_pred, adx_values):
        """è¶‹åŠ¿å¼ºåº¦æ ¡éªŒ - ä»…å½“ADX>25æ—¶ç¡®è®¤æ¶¨è·Œä¿¡å·"""
        verified_pred = []
        for i, pred in enumerate(y_pred):
            # å¦‚æœåŸé¢„æµ‹ä¸ºæ¶¨è·Œï¼Œä¸”ADX<25ï¼Œåˆ™æ”¹ä¸ºå¹³
            if (pred == 0 or pred == 2) and adx_values[i] < 25:  # è·Œæˆ–æ¶¨ï¼Œä½†ADX<25
                verified_pred.append(1)  # æ”¹ä¸ºå¹³
            else:
                verified_pred.append(pred)
        
        return np.array(verified_pred)
    
    def multi_kline_trend_confirmation(self, y_pred):
        """å¤šæ ¹Kçº¿è¶‹åŠ¿ç¡®è®¤ - è¿ç»­2æ ¹M15é¢„æµ‹ç»“æœä¸€è‡´æ‰ç¡®è®¤"""
        confirmed_pred = np.full_like(y_pred, 1)  # é»˜è®¤ä¸ºå¹³
        
        for i in range(1, len(y_pred)):
            # å¦‚æœå½“å‰å’Œå‰ä¸€æ ¹é¢„æµ‹ç»“æœä¸€è‡´ä¸”éƒ½æ˜¯æ¶¨è·Œä¿¡å·ï¼Œåˆ™ç¡®è®¤
            if y_pred[i] == y_pred[i-1] and y_pred[i] != 1:  # éƒ½ä¸æ˜¯å¹³ï¼Œä¸”é¢„æµ‹ç›¸åŒ
                confirmed_pred[i] = y_pred[i]
        
        return confirmed_pred
    
    def feature_weighting(self, X, feature_names, core_features, weight_factor=2.0):
        """å¯¹æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒå¤„ç†"""
        # åœ¨ç‰¹å¾çŸ©é˜µä¸­å¯¹æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒ
        for feature in core_features:
            if feature in feature_names:
                feature_idx = feature_names.index(feature)
                # é€šè¿‡æ”¾å¤§ç‰¹å¾å€¼æ¥å®ç°åŠ æƒæ•ˆæœ
                X[:, feature_idx] *= weight_factor
        return X
    
    def stratified_sampling(self, y, X, ratio=[15, 15, 70]):
        """åˆ†å±‚é‡‡æ ·å‡è¡¡æ ·æœ¬ï¼šæŒ‰æŒ‡å®šæ¯”ä¾‹é‡‡æ ·å„ç±»åˆ«"""
        # ç¡®ä¿æ¯”ç‡æ€»å’Œä¸º100
        total_ratio = sum(ratio)
        ratio = [r/total_ratio for r in ratio]
        
        # è·å–å„ç±»åˆ«çš„ç´¢å¼•
        pos_indices = np.where(y == 1)[0]
        neg_indices = np.where(y == -1)[0]
        flat_indices = np.where(y == 0)[0]
        
        # è®¡ç®—å„ç±»åˆ«ç›®æ ‡æ ·æœ¬æ•°
        total_samples = len(y)
        target_pos = int(total_samples * ratio[0])
        target_neg = int(total_samples * ratio[1])
        target_flat = int(total_samples * ratio[2])
        
        # å¯¹å„ç±»åˆ«è¿›è¡Œé‡‡æ ·
        sampled_indices = []
        
        # é‡‡æ ·æ¶¨ç±»
        if len(pos_indices) > 0:
            pos_samples = resample(pos_indices, 
                                   n_samples=min(target_pos, len(pos_indices)), 
                                   random_state=42)
            sampled_indices.extend(pos_samples)
        
        # é‡‡æ ·è·Œç±»
        if len(neg_indices) > 0:
            neg_samples = resample(neg_indices, 
                                   n_samples=min(target_neg, len(neg_indices)), 
                                   random_state=42)
            sampled_indices.extend(neg_samples)
        
        # é‡‡æ ·å¹³ç±»
        if len(flat_indices) > 0:
            flat_samples = resample(flat_indices, 
                                    n_samples=min(target_flat, len(flat_indices)), 
                                    random_state=42)
            sampled_indices.extend(flat_samples)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åºç´¢å¼•
        sampled_indices = sorted(sampled_indices)
        
        # è¿”å›é‡‡æ ·åçš„Xå’Œy
        X_sampled = X[sampled_indices]
        y_sampled = y[sampled_indices]
        
        print(f"åˆ†å±‚é‡‡æ ·åæ ·æœ¬åˆ†å¸ƒ: æ¶¨={np.sum(y_sampled==1)}, è·Œ={np.sum(y_sampled==-1)}, å¹³={np.sum(y_sampled==0)}")
        
        return X_sampled, y_sampled
    
    def confidence_weighted_filter(self, y_pred_proba):
        """ç½®ä¿¡åº¦åŠ æƒè¾“å‡ºï¼šä¸å¯¹ä¿¡å·åšä»»ä½•åˆ é™¤ï¼Œä»…ç»™ä¸åŒç½®ä¿¡åº¦çš„ä¿¡å·èµ‹äºˆæƒé‡
        æ ¸å¿ƒç›®æ ‡ï¼šä» "ç½®ä¿¡åº¦é˜ˆå€¼åˆ™ä¿ç•™ï¼Œå¦åˆ™åˆ é™¤" æ”¹ä¸º "ç½®ä¿¡åº¦åŠ æƒè¾“å‡º"ï¼Œé¿å…æœ‰æ•ˆä¿¡å·è¢«è¯¯åˆ 
        å®ç›˜å†³ç­–æ—¶ç»¼åˆæƒé‡åˆ¤æ–­ï¼Œå½»åº•é¿å…æœ‰æ•ˆä¿¡å·è¢«è¯¯åˆ """
        weighted_predictions = []
        weights = []
        
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®ç½®ä¿¡åº¦åˆ†é…æƒé‡ï¼š0.4ç½®ä¿¡åº¦<0.5æƒé‡0.4ï¼Œ0.5æƒé‡1.0
            if max_prob < 0.4:
                weight = max_prob  # ä½ç½®ä¿¡åº¦ä¿¡å·ï¼Œæƒé‡ä¸ºå…¶ç½®ä¿¡åº¦å€¼
            elif max_prob < 0.5:
                weight = (max_prob - 0.4) * 4  # 0.4-0.5ä¹‹é—´çš„ä¿¡å·ï¼Œæƒé‡çº¿æ€§å¢é•¿
            else:
                weight = 1.0  # é«˜ç½®ä¿¡åº¦ä¿¡å·ï¼Œæƒé‡ä¸º1.0
            
            weighted_predictions.append(pred_class)
            weights.append(weight)
        
        return np.array(weighted_predictions), np.array(weights)

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹è®­ç»ƒXAUUSD M15å‘¨æœŸXGBoostæ¨¡å‹")
    try:
        trainer = M15ModelTrainer()
        model, features = trainer.train_model()
        print("M15æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()





