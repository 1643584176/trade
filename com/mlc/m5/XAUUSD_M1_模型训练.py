import MetaTrader5 as mt5
import pandas as pd
import numpy as np
import xgboost as xgb
import pickle
import sys
import os
from datetime import datetime, timedelta, timezone
import warnings
import importlib.util
from sklearn.utils import resample
warnings.filterwarnings('ignore')

# æ·»åŠ å…¬å…±æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "common"))

# åŠ¨æ€å¯¼å…¥åŸºç±»
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
model_trainer_base_path = os.path.join(project_root, 'mlc', 'm5', 'model_trainer_base.py')
spec = importlib.util.spec_from_file_location("model_trainer_base", model_trainer_base_path)
model_trainer_base = importlib.util.module_from_spec(spec)
spec.loader.exec_module(model_trainer_base)
BaseModelTrainer = model_trainer_base.BaseModelTrainer

# é…ç½®å‚æ•°
class M1ModelConfig:
    SYMBOL = "XAUUSD"
    M1_TIMEFRAME = mt5.TIMEFRAME_M1
    M5_TIMEFRAME = mt5.TIMEFRAME_M5
    M15_TIMEFRAME = mt5.TIMEFRAME_M15
    HISTORY_M1_BARS = 50  # ç”¨äºé¢„æµ‹çš„M1 Kçº¿æ•°é‡ï¼ˆ30-50æ ¹ï¼‰
    PREDICT_FUTURE_BARS = 3  # é¢„æµ‹æœªæ¥Kçº¿æ•°é‡
    TRAIN_TEST_SPLIT = 0.8
    MODEL_SAVE_PATH = "xauusd_m1_model.json"  # XGBoostæ¨¡å‹ä¿å­˜è·¯å¾„
    SCALER_SAVE_PATH = "m1_scaler.pkl"
    UTC_TZ = timezone.utc

class M1ModelTrainer(BaseModelTrainer):
    def __init__(self):
        super().__init__()
        self.config = M1ModelConfig()
    
    def prepare_features_and_target(self, df, timeframe_type="M1"):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡ - é‡å†™ä»¥åˆ é™¤é‡å¤çš„atr_7ç‰¹å¾"""
        # M1å‘¨æœŸç‰¹å¾ï¼ˆçŸ­æœŸæ³¢åŠ¨ï¼‰
        feature_columns = [
            # M1å‘¨æœŸç‰¹å¾ï¼ˆçŸ­æœŸæ³¢åŠ¨ï¼‰
            'open', 'high', 'low', 'close', 'tick_volume',
            'rsi_7',  # çŸ­æœŸRSI
            'ma3', 'ma7',  # çŸ­æœŸå‡çº¿
            'atr_7',  # çŸ­æœŸATR - æ ¸å¿ƒç‰¹å¾ï¼ˆä»…ä¿ç•™æ­¤ç‰ˆæœ¬ï¼Œåˆ é™¤é‡å¤çš„ï¼‰
            'volatility_pct',
            'hour_of_day', 'is_peak_hour',
            # Kçº¿å½¢æ€ç‰¹å¾
            'hammer', 'shooting_star', 'engulfing',
            # æŠ€æœ¯æŒ‡æ ‡
            'rsi_14', 'macd', 'macd_hist',
            'bollinger_position',
            'ma5', 'ma10', 'ma20', 'ma10_direction', 'ma20_direction',
            # ä¸€è‡´æ€§ç‰¹å¾
            'rsi_price_consistency',
            # è·¨å‘¨æœŸç‰¹å¾
            'rsi_divergence', 'vol_short_vs_medium', 'vol_medium_vs_long', 'vol_short_vs_long',
            'trend_consistency',
            # ä¿¡å·ç‰¹å¾
            'rsi_signal_strength', 'short_long_signal_consistency',
            # é£é™©ç‰¹å¾
            'volatility_regime', 'vol_cluster',
            # M1ä¸“ç”¨å¾®è§‚ç‰¹å¾
            'tick_vol_zscore',  # Tickæˆäº¤é‡è„‰å†²
            'up_down_count_10',  # 1åˆ†é’Ÿå†…æ¶¨è·Œæ¬¡æ•°
            'hl_spread_zscore',  # é«˜ä½ä»·å·®z-score
            'volatility_intensity',  # ä»·æ ¼æ³¢åŠ¨å¼ºåº¦
            'ma5_deviation',  # çŸ­æœŸåç¦»åº¦
            'volume_impulse',  # æˆäº¤é‡è„‰å†²ç‰¹å¾ï¼ˆå½“å‰æˆäº¤é‡/å‰3æ ¹å‡å€¼ï¼‰
            'price_direction_consistency',  # æ¶¨è·Œå»¶ç»­æ€§ç‰¹å¾
            'dynamic_activity',  # åŠ¨æ€æ´»è·ƒåº¦ç‰¹å¾
            'high_activity',  # é«˜æ´»è·ƒåº¦æ ‡è®°
            'up_momentum_3',  # è¿ç»­3æ ¹M1æ¶¨è·Œå¹…ä¹‹å’Œï¼ˆä»…è®¡ç®—ä¸Šæ¶¨ï¼‰
            'down_momentum_3',  # è¿ç»­3æ ¹M1ä¸‹è·ŒåŠ¨èƒ½ï¼ˆæ–°å¢è·Œç±»åŠ¨èƒ½ç‰¹å¾ï¼‰
            'down_volume_ratio',  # è·Œæ—¶æˆäº¤é‡å æ¯”ï¼ˆæ–°å¢è·Œç±»åŠ¨èƒ½ç‰¹å¾ï¼‰
            # æ¶¨è·ŒåŠ¨èƒ½ç‰¹å¾
            'momentum_3',  # 3æ ¹Kçº¿çš„æ¶¨è·Œå¹…ä¹‹å’Œ
            'momentum_5',  # 5æ ¹Kçº¿çš„æ¶¨è·Œå¹…ä¹‹å’Œ
            'volume_price_divergence',  # æˆäº¤é‡ä¸ä»·æ ¼èƒŒç¦»
            'consecutive_up',  # è¿ç»­ä¸Šæ¶¨æ¬¡æ•°
            'consecutive_down',  # è¿ç»­ä¸‹è·Œæ¬¡æ•°
            # æ–°å¢æ¶¨ç±»ä¸“å±ç‰¹å¾
            'volume_up_ratio',  # æˆäº¤é‡æ”¾é‡å æ¯”
            'up_momentum_5',  # 5æ ¹Kçº¿ä»…è®¡ç®—ä¸Šæ¶¨éƒ¨åˆ†çš„å¼ºåº¦
            'volume_up_ratio_enhanced',  # volume_up_ratio å¼ºåŒ–ç‰ˆ
            'activity_trend_up',  # activity_trend ä¸Šæ¶¨è¶‹åŠ¿
            'ma5_deviation_up',  # ma5_deviation å‘ä¸Šåç¦»
            # æ–°å¢è·Œç±»ä¸“å±ç‰¹å¾
            'down_momentum_5',  # 5æ ¹Kçº¿ä»…è®¡ç®—ä¸‹è·Œéƒ¨åˆ†çš„å¼ºåº¦
            'down_volume_impulse',  # æ”¾é‡ä¸‹è·Œå æ¯”
            # æ–°å¢é«˜æ´»è·ƒåº¦æ¶¨ç±»åŠ æƒç‰¹å¾
            'high_activity_up_weight',  # é«˜æ´»è·ƒæ—¶æ®µæ¶¨ç±»æ ·æœ¬åŠ æƒ
            # dynamic_activity ç‰¹å¾ä¼˜åŒ–
            'activity_trend',  # æ´»è·ƒåº¦è¶‹åŠ¿ç‰¹å¾
            # æ–°å¢æ¶¨è·Œæ´»è·ƒåº¦å·®å¼‚ç‰¹å¾
            'up_down_activity_diff',  # æ¶¨è·Œæ´»è·ƒåº¦å·®å¼‚
            # æ ¸å¿ƒæ³¢åŠ¨ç‰¹å¾ - åªä¿ç•™ä¸€ä¸ªatr_7ï¼Œåˆ é™¤é‡å¤çš„

            # æ–°å¢è·Œç±»ä¸“å±ç‰¹å¾
            'activity_trend_down',  # æ´»è·ƒåº¦è¶‹åŠ¿ä¸‹è·Œåˆ†é‡
            'ma5_deviation_down',  # ma5_deviation å‘ä¸‹åç¦»
            # åˆ é™¤é‡å¤ç‰¹å¾ï¼šç§»é™¤é‡å¤çš„ atr_7, tick_volume, bollinger_position, up_momentum_5
        ]
        
        # æ£€æŸ¥æ‰€æœ‰ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
        available_features = []
        for col in feature_columns:
            if col in df.columns:
                available_features.append(col)
            else:
                print(f"è­¦å‘Š: ç‰¹å¾åˆ— '{col}' ä¸å­˜åœ¨")
        
        X = df[available_features].values
        y = df['target'].values
        
        return X, y, available_features
    
    def get_m1_historical_data(self, bars_count: int = 60*24*60):  # 60å¤©çš„M1æ•°æ®
        """è·å–MT5çœŸå®å†å²M1æ•°æ®"""
        self.initialize_mt5()
        
        # ä½¿ç”¨mt5.copy_rates_from_posæŒ‰Kçº¿æ•°é‡è·å–æ•°æ®
        m1_rates = mt5.copy_rates_from_pos(
            self.config.SYMBOL,
            self.config.M1_TIMEFRAME,
            0,  # ä»æœ€æ–°çš„Kçº¿å¼€å§‹è·å–
            bars_count  # è·å–æŒ‡å®šæ•°é‡çš„Kçº¿
        )
        
        if m1_rates is None or len(m1_rates) == 0:
            raise Exception(f"è·å–M1å†å²æ•°æ®å¤±è´¥ï¼š{mt5.last_error()}")
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(m1_rates)
        df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
        df.set_index('time', inplace=True)
        
        # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥ - æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
        time_diff = df.index.to_series().diff().dt.total_seconds().dropna()
        if not (time_diff == 60).all():  # M1å‘¨æœŸé¢„æœŸé—´éš”60ç§’
            print("è­¦å‘Š: æ•°æ®å­˜åœ¨æ—¶é—´æ–­è¿ï¼Œå¯èƒ½å½±å“ç‰¹å¾è®¡ç®—")
        
        # å‡†å¤‡æ•°æ®å’Œç‰¹å¾
        df = self.prepare_data_with_features(m1_rates, "M1")
        
        # æ·»åŠ M1ä¸“ç”¨çš„å¾®è§‚äº¤æ˜“ç‰¹å¾
        df = self.add_micro_features(df)
        
        # åˆ›å»ºç›®æ ‡å˜é‡ï¼šé¢„æµ‹æœªæ¥1-3æ ¹Kçº¿çš„æ¶¨è·Œ (1=æ¶¨, 0=è·Œ, -1=å¹³)
        df['future_close_1'] = df['close'].shift(-1)  # é¢„æµ‹1æ ¹Kçº¿å
        df['future_close_2'] = df['close'].shift(-2)  # é¢„æµ‹2æ ¹Kçº¿å
        df['future_close_3'] = df['close'].shift(-3)  # é¢„æµ‹3æ ¹Kçº¿å
        
        # ä½¿ç”¨é¢„æµ‹æœªæ¥1-3æ ¹Kçº¿çš„å¹³å‡æ¶¨è·Œå¹…ä½œä¸ºç›®æ ‡
        # è®¡ç®—æœªæ¥1-3æ ¹Kçº¿çš„å¹³å‡ä»·æ ¼
        df['future_avg_close'] = (df['future_close_1'] + df['future_close_2'] + df['future_close_3']) / 3
        df['price_change_pct'] = (df['future_avg_close'] - df['close']) / df['close']
        
        # å¼‚å¸¸å€¼å¤„ç† - æ£€æµ‹ä»·æ ¼è·³ç©º
        df['gap_pct'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)
        atr_14 = self.calculate_atr(df['high'], df['low'], df['close'], 14)
        df['atr_14'] = atr_14
        df = df[abs(df['gap_pct']) < 3 * atr_14]  # è¿‡æ»¤æç«¯è·³ç©º
        
        # é‡æ–°è®¡ç®—price_change_pctåœ¨è¿‡æ»¤å¼‚å¸¸å€¼ä¹‹å
        df['future_close_1'] = df['close'].shift(-1)
        df['price_change_pct'] = (df['future_close_1'] - df['close']) / df['close']
        
        # è®¡ç®—åŸºäºæ³¢åŠ¨ç‡çš„åŠ¨æ€é˜ˆå€¼
        atr_14 = self.calculate_atr(df['high'], df['low'], df['close'], 14)
        df['atr_14'] = atr_14
        
        # æ ¹æ®ATRåŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œæ³¢åŠ¨ç‡é«˜åˆ™é™ä½é˜ˆå€¼è¦æ±‚
        base_threshold = 0.0006  # è°ƒæ•´ååŸºç¡€é˜ˆå€¼ï¼ˆ0.06%ï¼‰ï¼Œé€‚åº¦æ”¾å®½ä»¥å‡å°‘å¹³ç±»å æ¯”ï¼Œå¢åŠ æ¶¨è·Œç±»æ ·æœ¬
        dynamic_threshold_series = base_threshold - np.minimum(0.0002, atr_14 * 0.015)  # æ³¢åŠ¨ç‡è¶Šé«˜ï¼Œé˜ˆå€¼è¶Šä½ï¼ˆæœ€ä½0.0005ï¼‰
        
        # ç¡®ä¿dynamic_threshold_seriesä¸price_change_pctç´¢å¼•ä¸€è‡´
        dynamic_threshold_series = dynamic_threshold_series.reindex(df['price_change_pct'].index, fill_value=base_threshold)
        
        # å®šä¹‰ç›®æ ‡å˜é‡ - è€ƒè™‘XAUUSDçš„ç‚¹å·®å’ŒåŠ¨æ€é˜ˆå€¼
        # è°ƒæ•´é˜ˆå€¼ä»¥å¹³è¡¡ç±»åˆ«åˆ†å¸ƒï¼ŒXAUUSDç‚¹å·®çº¦ä¸º0.05ï¼Œè®¾ç½®åˆç†é˜ˆå€¼é¿å…è¿‡å¤š'å¹³'ç±»æ ·æœ¬
        df['target'] = np.where(df['price_change_pct'] > dynamic_threshold_series, 1,  # æ¶¨
                               np.where(df['price_change_pct'] < -dynamic_threshold_series, -1, 0))  # è·Œå’Œå¹³
        
        # æ£€æŸ¥å¹¶æŠ¥å‘Šç±»åˆ«åˆ†å¸ƒ
        unique, counts = np.unique(df['target'], return_counts=True)
        class_dist = dict(zip(unique, counts))
        print(f"ç›®æ ‡å˜é‡ç±»åˆ«åˆ†å¸ƒ: {class_dist}")
        
        # å¦‚æœ'å¹³'ç±»æ ·æœ¬å æ¯”è¿‡é«˜ï¼Œè°ƒæ•´é˜ˆå€¼
        if 0 in class_dist:
            flat_ratio = class_dist[0] / len(df['target'])
            if flat_ratio > 0.8:  # å¦‚æœ'å¹³'ç±»å æ¯”è¶…è¿‡80%
                print(f"è­¦å‘Š: 'å¹³'ç±»æ ·æœ¬å æ¯”è¿‡é«˜ ({flat_ratio:.2%})ï¼Œæ­£åœ¨è°ƒæ•´é˜ˆå€¼...")
                # é™ä½é˜ˆå€¼ä»¥å‡å°‘'å¹³'ç±»æ ·æœ¬æ¯”ä¾‹
                adjusted_threshold = dynamic_threshold_series * 0.7  # é™ä½é˜ˆå€¼
                df['target'] = np.where(df['price_change_pct'] > adjusted_threshold, 1,  # æ¶¨
                                       np.where(df['price_change_pct'] < -adjusted_threshold, -1, 0))  # è·Œå’Œå¹³
                
                # é‡æ–°æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
                unique, counts = np.unique(df['target'], return_counts=True)
                class_dist = dict(zip(unique, counts))
                print(f"è°ƒæ•´åç›®æ ‡å˜é‡ç±»åˆ«åˆ†å¸ƒ: {class_dist}")
        
        return df
    
    def add_micro_features(self, df):
        """ä¸ºM1æ•°æ®æ·»åŠ å¾®è§‚äº¤æ˜“ç‰¹å¾"""
        # Tickæˆäº¤é‡è„‰å†²ç‰¹å¾
        df['tick_vol_zscore'] = (df['tick_volume'] - df['tick_volume'].rolling(window=10).mean()) / df['tick_volume'].rolling(window=10).std()
        df['tick_vol_zscore'] = df['tick_vol_zscore'].fillna(0)
        
        # æˆäº¤é‡è„‰å†²ç‰¹å¾ï¼ˆå½“å‰æˆäº¤é‡ / å‰3æ ¹å‡å€¼ï¼Œæ›´é€‚åˆM1è¶…çŸ­æœŸå‘¨æœŸï¼‰
        df['volume_impulse'] = df['tick_volume'] / df['tick_volume'].rolling(window=3).mean()
        df['volume_impulse'] = df['volume_impulse'].fillna(1.0)  # ç”¨1.0å¡«å……åˆå§‹å€¼
        
        # æ¶¨è·Œå»¶ç»­æ€§ç‰¹å¾ï¼ˆè¿ç»­2æ ¹M1çš„æ¶¨è·Œå¹…æ–¹å‘æ˜¯å¦ä¸€è‡´ï¼‰
        df['price_change'] = df['close'].pct_change()
        df['price_direction'] = np.where(df['price_change'] > 0, 1, np.where(df['price_change'] < 0, -1, 0))
        df['price_direction_consistency'] = (df['price_direction'] == df['price_direction'].shift(1)).astype(int)
        
        # 1åˆ†é’Ÿå†…æ¶¨è·Œæ¬¡æ•°ç‰¹å¾ï¼ˆé€šè¿‡ä»·æ ¼å˜åŒ–æ–¹å‘ç»Ÿè®¡ï¼‰
        df['price_change'] = df['close'].diff()
        df['price_direction'] = np.where(df['price_change'] > 0, 1, np.where(df['price_change'] < 0, -1, 0))
        df['up_down_count_10'] = df['price_direction'].rolling(window=10).sum().abs()
        
        # ç›˜å£ä¹°å–ä»·å·®ç‰¹å¾ï¼ˆé€šè¿‡é«˜ä½ä»·å·®å¼‚è¿‘ä¼¼ï¼‰
        df['high_low_spread'] = (df['high'] - df['low']) / df['close']
        df['hl_spread_zscore'] = (df['high_low_spread'] - df['high_low_spread'].rolling(window=20).mean()) / df['high_low_spread'].rolling(window=20).std()
        df['hl_spread_zscore'] = df['hl_spread_zscore'].fillna(0)
        
        # ä»·æ ¼æ³¢åŠ¨å¼ºåº¦ç‰¹å¾
        df['volatility_intensity'] = abs(df['close'] - df['open']) / df['close']
        
        # çŸ­æœŸè¶‹åŠ¿å¼ºåº¦ï¼ˆåŸºäºç§»åŠ¨å¹³å‡åç¦»åº¦ï¼‰
        df['ma5_deviation'] = abs(df['close'] - df['close'].rolling(window=5).mean()) / df['close']
        df['ma5_trend_strength'] = (df['close'] - df['close'].rolling(window=5).mean()) / df['close']
        
        # æ¸…ç†å¯èƒ½çš„æ— ç©·å¤§å€¼
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        # æ›´æ–°dynamic_activityç‰¹å¾ï¼šä¿ç•™ "æœ€è¿‘ 5 æ ¹ M1 å¹³å‡æ´»è·ƒåº¦"ï¼Œæ–°å¢ "æ¶¨ / è·Œæ´»è·ƒåº¦å·®å¼‚" ç‰¹å¾
        df['volatility_5m'] = df['close'].pct_change().rolling(window=5).std()  # 5åˆ†é’Ÿæ³¢åŠ¨ç‡
        df['volatility_60m_avg'] = df['volatility_5m'].rolling(window=12).mean()  # 60åˆ†é’Ÿï¼ˆ12ä¸ª5åˆ†é’Ÿï¼‰å¹³å‡æ³¢åŠ¨ç‡
        df['dynamic_activity_raw'] = df['volatility_5m'] / (df['volatility_60m_avg'] + 1e-8)  # é˜²æ­¢é™¤é›¶
        df['dynamic_activity'] = df['dynamic_activity_raw'].rolling(window=5).mean()  # æœ€è¿‘5æ ¹M1å¹³å‡æ´»è·ƒåº¦
        
        # æ–°å¢"æ¶¨/è·Œæ´»è·ƒåº¦å·®å¼‚"ç‰¹å¾
        df['price_change_direction'] = np.where(df['close'] > df['open'], 1, np.where(df['close'] < df['open'], -1, 0))
        df['up_activity'] = df['dynamic_activity_raw'] * (df['price_change_direction'] == 1).astype(int)  # ä¸Šæ¶¨æ—¶æ´»è·ƒåº¦
        df['down_activity'] = df['dynamic_activity_raw'] * (df['price_change_direction'] == -1).astype(int)  # ä¸‹è·Œæ—¶æ´»è·ƒåº¦
        df['up_down_activity_diff'] = df['up_activity'].rolling(window=5).mean() - df['down_activity'].rolling(window=5).mean()  # æ¶¨è·Œæ´»è·ƒåº¦å·®å¼‚
        
        df['high_activity'] = (df['dynamic_activity'] > 1.2).astype(int)  # é«˜æ´»è·ƒåº¦æ ‡è®°
        
        # å¯¹é«˜æ´»è·ƒæ—¶æ®µçš„æ¶¨ç±»æ ·æœ¬é¢å¤–åŠ æƒï¼ˆ1.1ï¼‰ï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«é«˜æ³¢åŠ¨ä¸‹çš„ä¸Šæ¶¨ä¿¡å·
        df['high_activity_up_weight'] = df['high_activity'] * (df['price_change_pct'] > 0).astype(int) * 1.1
        
        # æ–°å¢æ¶¨ç±»åŠ¨èƒ½ç‰¹å¾ï¼ˆæ¶¨ç±»ä¸“å±ç‰¹å¾è¡¥å……ï¼‰
        # è¿ç»­3æ ¹M1æ¶¨è·Œå¹…ä¹‹å’Œï¼ˆä»…è®¡ç®—ä¸Šæ¶¨ï¼‰
        df['price_change_pct'] = df['close'].pct_change()
        df['up_momentum_3'] = df['price_change_pct'].rolling(window=3).apply(lambda x: sum([i for i in x if i > 0]), raw=True)  # ä»…è®¡ç®—ä¸Šæ¶¨éƒ¨åˆ†
        df['up_momentum_3'] = df['up_momentum_3'].fillna(0)
        
        # volume_up_ratio å¼ºåŒ–ç‰ˆ
        df['volume_up_ratio_enhanced'] = df['tick_volume'] / df['tick_volume'].rolling(window=10).mean()  # æˆäº¤é‡ç›¸å¯¹å‡å€¼çš„æ¯”å€¼
        df['volume_up_impulse_enhanced'] = df['volume_up_ratio_enhanced'] * (df['price_change_pct'] > 0).astype(int)  # æ”¾é‡ä¸Šæ¶¨å æ¯”
        
        # activity_trend ä¸Šæ¶¨è¶‹åŠ¿
        df['activity_trend_up'] = df['dynamic_activity'] - df['dynamic_activity'].shift(5)  # å½“å‰æ´»è·ƒåº¦ - å‰5æ ¹å¹³å‡æ´»è·ƒåº¦
        df['activity_trend_up'] = df['activity_trend_up'].fillna(0)
        
        # ma5_deviation å‘ä¸Šåç¦»
        df['ma5_deviation_up'] = np.where(df['ma5_trend_strength'] > 0, df['ma5_deviation'], 0)  # ä»…å½“è¶‹åŠ¿å‘ä¸Šæ—¶è€ƒè™‘åç¦»åº¦
        
        # å¼ºåŒ–è·Œç±»åŠ¨èƒ½ç‰¹å¾ï¼šè¿ç»­3æ ¹M1ä¸‹è·ŒåŠ¨èƒ½ + è·Œæ—¶æˆäº¤é‡å æ¯”
        df['down_momentum_3'] = df['price_change_pct'].rolling(window=3).apply(lambda x: abs(sum([i for i in x if i < 0])), raw=True)  # ä»…è®¡ç®—ä¸‹è·Œéƒ¨åˆ†
        df['down_momentum_3'] = df['down_momentum_3'].fillna(0)
        
        # è·Œæ—¶æˆäº¤é‡å æ¯”
        df['price_direction'] = np.where(df['price_change_pct'] < 0, 1, 0)  # ä»·æ ¼ä¸‹è·Œæ ‡è®°
        df['down_volume_ratio'] = df['tick_volume'] * df['price_direction']  # è·Œæ—¶æˆäº¤é‡
        df['down_volume_ratio'] = df['down_volume_ratio'].rolling(window=10).sum() / df['tick_volume'].rolling(window=10).sum()  # è·Œæ—¶æˆäº¤é‡å æ¯”
        df['down_volume_ratio'] = df['down_volume_ratio'].fillna(0)
        
        # æ–°å¢æ¶¨ç±»ä¸“å±ç‰¹å¾ï¼švolume_impulse æ”¾é‡ä¸Šæ¶¨å æ¯”
        df['volume_up_ratio'] = df['tick_volume'] / df['tick_volume'].rolling(window=10).mean()  # æˆäº¤é‡ç›¸å¯¹å‡å€¼çš„æ¯”å€¼
        df['up_volume_impulse'] = df['volume_up_ratio'] * (df['price_change_pct'] > 0).astype(int)  # æ”¾é‡ä¸Šæ¶¨å æ¯”
        
        # æ–°å¢æ¶¨ç±»ä¸“å±ç‰¹å¾ï¼šmomentum_5 ä¸Šæ¶¨å¼ºåº¦
        df['up_momentum_5'] = df['price_change_pct'].rolling(window=5).apply(lambda x: sum([i for i in x if i > 0]), raw=True)  # 5æ ¹Kçº¿ä»…è®¡ç®—ä¸Šæ¶¨éƒ¨åˆ†
        df['up_momentum_5'] = df['up_momentum_5'].fillna(0)
        
        # æ–°å¢è·Œç±»ä¸“å±ç‰¹å¾ï¼šdown_momentum_5
        df['down_momentum_5'] = df['price_change_pct'].rolling(window=5).apply(lambda x: abs(sum([i for i in x if i < 0])), raw=True)  # 5æ ¹Kçº¿ä»…è®¡ç®—ä¸‹è·Œéƒ¨åˆ†
        df['down_momentum_5'] = df['down_momentum_5'].fillna(0)
        
        # æ–°å¢è·Œç±»ä¸“å±ç‰¹å¾ï¼švolume_down_ratio
        df['volume_down_ratio'] = df['tick_volume'] / df['tick_volume'].rolling(window=10).mean()  # æˆäº¤é‡ç›¸å¯¹å‡å€¼çš„æ¯”å€¼
        df['down_volume_impulse'] = df['volume_down_ratio'] * (df['price_change_pct'] < 0).astype(int)  # æ”¾é‡ä¸‹è·Œå æ¯”
        
        # dynamic_activity ç‰¹å¾ä¼˜åŒ–ï¼šæ–°å¢"æ´»è·ƒåº¦è¶‹åŠ¿"ç‰¹å¾
        df['activity_trend'] = df['dynamic_activity'] - df['dynamic_activity'].shift(5)  # å½“å‰æ´»è·ƒåº¦ - å‰5æ ¹å¹³å‡æ´»è·ƒåº¦
        df['activity_trend'] = df['activity_trend'].fillna(0)
        
        # æ–°å¢è·Œç±»ä¸“å±ç‰¹å¾ï¼šactivity_trend ä¸‹è·Œè¶‹åŠ¿
        df['activity_trend_down'] = np.where(df['activity_trend'] < 0, abs(df['activity_trend']), 0)  # ä»…å½“æ´»è·ƒåº¦è¶‹åŠ¿å‘ä¸‹æ—¶è€ƒè™‘
        
        # æ–°å¢è·Œç±»ä¸“å±ç‰¹å¾ï¼šma5_deviation å‘ä¸‹åç¦»
        df['ma5_deviation_down'] = np.where(df['ma5_trend_strength'] < 0, df['ma5_deviation'], 0)  # ä»…å½“è¶‹åŠ¿å‘ä¸‹æ—¶è€ƒè™‘åç¦»åº¦
        
        return df

    def train_model(self):
        """è®­ç»ƒM1æ¨¡å‹"""
        print("å¼€å§‹è·å–M1å†å²æ•°æ®...")
        df = self.get_m1_historical_data(bars_count=60*24*60)  # è·å–60å¤©çš„M1æ•°æ®
        
        print(f"è·å–åˆ° {len(df)} æ¡å†å²æ•°æ®")
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X, y, feature_names = self.prepare_features_and_target(df, "M1")
        
        # æ‰“å°ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨
        print(f"\nğŸ“Š M1æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨ (å…±{len(feature_names)}ä¸ª):")
        for i, feature in enumerate(feature_names, 1):
            print(f"  {i:2d}. {feature}")
        
        # å¯¹ç‰¹å¾è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
        print(f"ç‰¹å¾å·²è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–")
        
        # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²ï¼Œç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´æ²¡æœ‰æ—¶é—´é‡å 
        split_idx = int(len(X) * self.config.TRAIN_TEST_SPLIT)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}, æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒæƒ…å†µ
        unique, counts = np.unique(y_train, return_counts=True)
        class_distribution = dict(zip(unique, counts))
        print(f"è®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ: {class_distribution}")
        total_samples = len(y_train)
        for label, count in class_distribution.items():
            print(f"ç±»åˆ« {label}: {count} æ ·æœ¬ ({count/total_samples*100:.2f}%)")
        
        # é¢å¤–çš„æ—¶é—´åºåˆ—éªŒè¯ï¼šä¿ç•™æœ€å10%çš„è®­ç»ƒæ•°æ®ä½œä¸ºæ—¶é—´å¤–éªŒè¯é›†
        validation_split_idx = int(len(X_train) * 0.9)
        X_val = X_train[validation_split_idx:]
        y_val = y_train[validation_split_idx:]
        X_train = X_train[:validation_split_idx]
        y_train = y_train[:validation_split_idx]
        
        print(f"è°ƒæ•´åè®­ç»ƒé›†å¤§å°: {len(X_train)}, éªŒè¯é›†å¤§å°: {len(X_val)}, æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        # è®­ç»ƒXGBoostæ¨¡å‹
        print("å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜
        from sklearn.utils.class_weight import compute_class_weight
        from sklearn.metrics import classification_report, precision_score, recall_score, f1_score
        
        classes = np.unique(y_train)
        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weight_dict = dict(zip(classes, class_weights))
        print(f"ç±»åˆ«æƒé‡: {class_weight_dict}")
        
        # é‡æ–°è®¾ç½®é‡‡æ ·æ¯”ä¾‹ï¼šå®ç°æ¶¨è·Œç±»å‡è¡¡é‡‡æ ·ï¼ˆè·Œ=6841, å¹³=48084, æ¶¨=6841ï¼‰ï¼Œæ¢å¤æ¶¨è·Œ1:1å‡è¡¡é‡‡æ ·
        X_train_balanced, y_train_balanced = self.stratified_sampling(y_train, X_train, ratio=[12, 12, 76])
        
        # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒæƒ…å†µ
        unique, counts = np.unique(y_train_balanced, return_counts=True)
        balanced_class_distribution = dict(zip(unique, counts))
        print(f"å‡è¡¡åè®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ: {balanced_class_distribution}")
        total_samples_balanced = len(y_train_balanced)
        for label, count in balanced_class_distribution.items():
            print(f"å‡è¡¡åç±»åˆ« {label}: {count} æ ·æœ¬ ({count/total_samples_balanced*100:.2f}%)")
        
        # è®¡ç®—æ¶¨è·Œç±»çš„æƒé‡ï¼Œå¼ºåˆ¶æ¨¡å‹å…³æ³¨æ¶¨è·Œä¿¡å·
        pos_count = len(y_train_balanced[y_train_balanced == 1])
        neg_count = len(y_train_balanced[y_train_balanced == -1])
        flat_count = len(y_train_balanced[y_train_balanced == 0])
        
        # æƒé‡è°ƒæ•´ï¼šè·Œç±»ä» 2.988 æè‡³ 3.5ï¼ˆå¼ºåˆ¶æ¨¡å‹å…³æ³¨è·Œç±»ï¼‰ï¼Œæ¶¨ç±»ä» 2.889 é™è‡³ 2.5ï¼ˆé™ä½å‡æ¶¨ä¿¡å·ï¼‰ï¼Œå¹³ç±»ä¿æŒ 0.431 ä¸å˜
        neg_weight = 3.5 if neg_count > 0 else 1.0  # è·Œç±»æƒé‡æå‡ï¼ˆå¼ºåˆ¶æ¨¡å‹å…³æ³¨è·Œç±»ï¼‰
        pos_weight = 2.5 if pos_count > 0 else 1.0  # æ¶¨ç±»æƒé‡é™ä½ï¼ˆé™ä½å‡æ¶¨ä¿¡å·ï¼‰
        flat_weight = 0.431  # å¹³ç±»æƒé‡ä¿æŒä¸å˜
        
        # å¯¹æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒå¤„ç†ï¼šå¯¹ atr_7ã€tick_volumeã€volatility_pctã€dynamic_activityã€momentum_3 è¿™ 5 ä¸ªæ ¸å¿ƒç‰¹å¾åŠ æƒ 2
        core_features = ['atr_7', 'tick_volume', 'volatility_pct', 'dynamic_activity', 'momentum_3']
        feature_idx_map = {name: i for i, name in enumerate(feature_names)}
        
        # å¯¹è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ ¸å¿ƒç‰¹å¾è¿›è¡ŒåŠ æƒ
        for feature in core_features:
            if feature in feature_idx_map:
                feature_idx = feature_idx_map[feature]
                X_train_balanced[:, feature_idx] *= 2.0  # å¯¹æ ¸å¿ƒç‰¹å¾åŠ æƒ2.0
                X_val[:, feature_idx] *= 2.0
                X_test[:, feature_idx] *= 2.0
                print(f"æ ¸å¿ƒç‰¹å¾ '{feature}' å·²åŠ æƒ 2.0")
        
        # å¯¹è·Œç±»åŠ¨èƒ½ç‰¹å¾ down_momentum_5 åŠ æƒ3
        if 'down_momentum_5' in feature_idx_map:
            feature_idx = feature_idx_map['down_momentum_5']
            X_train_balanced[:, feature_idx] *= 3.0  # å¯¹è·Œç±»åŠ¨èƒ½ç‰¹å¾åŠ æƒ3.0
            X_val[:, feature_idx] *= 3.0
            X_test[:, feature_idx] *= 3.0
            print("è·Œç±»åŠ¨èƒ½ç‰¹å¾ 'down_momentum_5' å·²åŠ æƒ 3.0")
        
        # ä¸ºXGBoostæ¨¡å‹è®¾ç½®ç±»åˆ«æƒé‡å’Œæ­£åˆ™åŒ–å‚æ•°
        model_params = {
            'n_estimators': 200,  # é€‚å½“å¢åŠ ä¼°è®¡å™¨æ•°é‡ä»¥æå‡å­¦ä¹ èƒ½åŠ›
            'max_depth': 4,  # é€‚åº¦å¢åŠ æ·±åº¦ä»¥æå‡è¡¨è¾¾èƒ½åŠ›
            'learning_rate': 0.015,  # é™ä½å­¦ä¹ ç‡è‡³0.015ï¼ŒåŠ å¿«è·Œç±»ç‰¹å¾çš„æ”¶æ•›
            'min_child_weight': 5,  # é€‚åº¦é™ä½æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æƒé‡
            'subsample': 0.8,  # å¢åŠ å­æ ·æœ¬æ¯”ä¾‹å‡å°‘è¿‡æ‹Ÿåˆ
            'colsample_bytree': 0.8,  # å¢åŠ ç‰¹å¾å­æ ·æœ¬æ¯”ä¾‹å‡è¡¡ç‰¹å¾å…³æ³¨åº¦
            'random_state': 42,
            'eval_metric': ['mlogloss', 'merror'],
            'gamma': 0.2,  # è°ƒæ•´gammaå€¼
            'reg_alpha': 0.2,  # è°ƒæ•´L1æ­£åˆ™åŒ–
            'reg_lambda': 1.5,  # è°ƒæ•´L2æ­£åˆ™åŒ–
            'num_class': len(classes)  # è®¾ç½®ç±»åˆ«æ•°é‡
        }
        
        # ä¸ºæ¶¨è·Œç±»åˆ†é…æ›´é«˜çš„æƒé‡
        sample_weights = np.ones_like(y_train_balanced, dtype=np.float64)
        for i, label in enumerate(y_train_balanced):
            if label == 1:  # æ¶¨
                sample_weights[i] = pos_weight
            elif label == -1:  # è·Œ
                sample_weights[i] = neg_weight
            else:  # å¹³
                sample_weights[i] = flat_weight
        
        # åˆ›å»ºXGBooståˆ†ç±»å™¨
        model = xgb.XGBClassifier(**model_params)
        
        # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œç¼–ç 
        y_train_encoded = self.encode_target_labels(y_train_balanced)
        y_test_encoded = self.encode_target_labels(y_test)
        
        # å¯¹éªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œç¼–ç 
        y_val_encoded = self.encode_target_labels(y_val)
        
        # ç”±äºXGBoostçš„scikit-learn APIä¸ç›´æ¥æ”¯æŒæ—©åœï¼Œæˆ‘ä»¬ä½¿ç”¨åŸç”ŸAPI
        # å‡†å¤‡æ•°æ®
        dtrain = xgb.DMatrix(X_train_balanced, label=y_train_encoded, weight=sample_weights)
        dval = xgb.DMatrix(X_val, label=y_val_encoded)
        
        # è®¾ç½®åŸç”ŸXGBoostå‚æ•°
        native_params = {
            'objective': 'multi:softprob',  # å¤šåˆ†ç±»æ¦‚ç‡è¾“å‡º
            'num_class': len(classes),
            'max_depth': model_params['max_depth'],
            'learning_rate': model_params['learning_rate'],
            'min_child_weight': model_params['min_child_weight'],
            'subsample': model_params['subsample'],
            'colsample_bytree': model_params['colsample_bytree'],
            'gamma': model_params['gamma'],
            'reg_alpha': model_params['reg_alpha'],
            'reg_lambda': model_params['reg_lambda'],
            'eval_metric': ['mlogloss', 'merror']
        }
        
        # è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœ
        evallist = [(dtrain, 'train'), (dval, 'eval')]
        model = xgb.train(
            native_params,
            dtrain,
            num_boost_round=model_params['n_estimators'],
            evals=evallist,
            early_stopping_rounds=3,  # ä¿ç•™ä¸¥æ ¼æ—©åœè§„åˆ™ï¼ˆè¿ç»­3è½®F1ä¸æå‡å³åœï¼‰ï¼Œé¿å…æ¨¡å‹å†æ¬¡åç§‘
            verbose_eval=False
        )
        
        # é¢„æµ‹
        y_train_pred_proba = model.predict(dtrain)
        y_val_pred_proba = model.predict(dval)
        y_test_dmatrix = xgb.DMatrix(X_test)
        y_test_pred_proba = model.predict(y_test_dmatrix)
        
        # è½¬æ¢æ¦‚ç‡ä¸ºé¢„æµ‹ç±»åˆ«
        y_train_pred = np.argmax(y_train_pred_proba, axis=1)
        y_val_pred = np.argmax(y_val_pred_proba, axis=1)
        y_pred = np.argmax(y_test_pred_proba, axis=1)
        
        # è¯„ä¼°æ¨¡å‹
        from sklearn.metrics import accuracy_score
        train_score = accuracy_score(y_train_encoded, y_train_pred)
        val_score = accuracy_score(y_val_encoded, y_val_pred)
        test_score = accuracy_score(y_test_encoded, y_pred)
        
        print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_score:.4f}")
        print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_score:.4f}")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")
        
        # è¾“å‡ºè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
        print("\néªŒè¯é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_val_encoded, y_val_pred, target_names=['è·Œ', 'å¹³', 'æ¶¨'], digits=4))
        
        print("\næµ‹è¯•é›†è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test_encoded, y_pred, target_names=['è·Œ', 'å¹³', 'æ¶¨'], digits=4))
        
        # è®¡ç®—å„ç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°
        val_precision = precision_score(y_val_encoded, y_val_pred, average=None)
        val_recall = recall_score(y_val_encoded, y_val_pred, average=None)
        val_f1 = f1_score(y_val_encoded, y_val_pred, average=None)
        
        test_precision = precision_score(y_test_encoded, y_pred, average=None)
        test_recall = recall_score(y_test_encoded, y_pred, average=None)
        test_f1 = f1_score(y_test_encoded, y_pred, average=None)
        
        print(f"\néªŒè¯é›†å„ç±»åˆ«ç²¾ç¡®ç‡: {val_precision}")
        print(f"éªŒè¯é›†å„ç±»åˆ«å¬å›ç‡: {val_recall}")
        print(f"éªŒè¯é›†å„ç±»åˆ«F1åˆ†æ•°: {val_f1}")
        
        print(f"\næµ‹è¯•é›†å„ç±»åˆ«ç²¾ç¡®ç‡: {test_precision}")
        print(f"æµ‹è¯•é›†å„ç±»åˆ«å¬å›ç‡: {test_recall}")
        print(f"æµ‹è¯•é›†å„ç±»åˆ«F1åˆ†æ•°: {test_f1}")
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡
        val_precision_weighted = precision_score(y_val_encoded, y_val_pred, average='weighted')
        val_recall_weighted = recall_score(y_val_encoded, y_val_pred, average='weighted')
        val_f1_weighted = f1_score(y_val_encoded, y_val_pred, average='weighted')
        
        test_precision_weighted = precision_score(y_test_encoded, y_pred, average='weighted')
        test_recall_weighted = recall_score(y_test_encoded, y_pred, average='weighted')
        test_f1_weighted = f1_score(y_test_encoded, y_pred, average='weighted')
        
        print(f"\néªŒè¯é›†åŠ æƒå¹³å‡ç²¾ç¡®ç‡: {val_precision_weighted:.4f}")
        print(f"éªŒè¯é›†åŠ æƒå¹³å‡å¬å›ç‡: {val_recall_weighted:.4f}")
        print(f"éªŒè¯é›†åŠ æƒå¹³å‡F1åˆ†æ•°: {val_f1_weighted:.4f}")
        
        print(f"\næµ‹è¯•é›†åŠ æƒå¹³å‡ç²¾ç¡®ç‡: {test_precision_weighted:.4f}")
        print(f"æµ‹è¯•é›†åŠ æƒå¹³å‡å¬å›ç‡: {test_recall_weighted:.4f}")
        print(f"æµ‹è¯•é›†åŠ æƒå¹³å‡F1åˆ†æ•°: {test_f1_weighted:.4f}")
        
        # è®¡ç®—æ¶¨è·Œç±»ï¼ˆéå¹³ç±»ï¼‰çš„F1åˆ†æ•°ï¼Œä½œä¸ºå…³é”®æŒ‡æ ‡
        # åªè€ƒè™‘æ¶¨(2)å’Œè·Œ(0)ç±»ï¼Œå¿½ç•¥å¹³(1)ç±»
        val_up_down_mask = (y_val_encoded == 0) | (y_val_encoded == 2)  # è·Œæˆ–æ¶¨
        test_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        
        if np.any(val_up_down_mask):
            val_up_down_f1 = f1_score(y_val_encoded[val_up_down_mask], y_val_pred[val_up_down_mask], average='macro')
            print(f"\néªŒè¯é›†æ¶¨è·Œç±»F1åˆ†æ•°: {val_up_down_f1:.4f}")
        
        if np.any(test_up_down_mask):
            test_up_down_f1 = f1_score(y_test_encoded[test_up_down_mask], y_pred[test_up_down_mask], average='macro')
            print(f"\næµ‹è¯•é›†æ¶¨è·Œç±»F1åˆ†æ•°: {test_up_down_f1:.4f}")
        
        # è®­ç»ƒé˜¶æ®µå®Œå…¨å–æ¶ˆæ¶¨è·Œä¿¡å·è¿‡æ»¤ï¼Œä»…ä¿ç•™åŸå§‹ä¿¡å·
        # æ ¸å¿ƒç›®æ ‡ï¼šå½“å‰æ¶¨è·Œç±» F1=0.1946 æ˜¯æœ‰æ•ˆä¿¡å·ï¼Œå½’é›¶çš„æœ¬è´¨æ˜¯ "è®­ç»ƒé˜¶æ®µè¿‡æ»¤é€»è¾‘è¯¯åˆ æ‰€æœ‰ä¿¡å·"ï¼Œè€Œéä¿¡å·æ— ä»·å€¼
        print("\nè®­ç»ƒé˜¶æ®µå–æ¶ˆæ‰€æœ‰è¿‡æ»¤é€»è¾‘ï¼Œä¿ç•™åŸå§‹ä¿¡å·...")
        y_pred_filtered = y_pred  # ä¿ç•™åŸå§‹é¢„æµ‹ï¼Œä¸è¿›è¡Œä»»ä½•è¿‡æ»¤
        filtered_accuracy = accuracy_score(y_test_encoded, y_pred_filtered)
        print(f"è®­ç»ƒé˜¶æ®µå–æ¶ˆæ‰€æœ‰è¿‡æ»¤åå‡†ç¡®ç‡: {filtered_accuracy:.4f}")
        
        # è®¡ç®—å–æ¶ˆè¿‡æ»¤åçš„æ¶¨è·Œç±»F1åˆ†æ•°
        filtered_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        if np.any(filtered_up_down_mask):
            filtered_up_down_f1 = f1_score(y_test_encoded[filtered_up_down_mask], y_pred_filtered[filtered_up_down_mask], average='macro')
            print(f"è®­ç»ƒé˜¶æ®µå–æ¶ˆæ‰€æœ‰è¿‡æ»¤åæ¶¨è·Œç±»F1åˆ†æ•°: {filtered_up_down_f1:.4f}")
        
        # å–æ¶ˆå¼ºåˆ¶è¿‡æ»¤é€»è¾‘
        print("\nå·²å–æ¶ˆæ‰€æœ‰å¼ºåˆ¶è¿‡æ»¤é€»è¾‘ï¼Œä¿ç•™åŸå§‹ä¿¡å·...")
        verified_predictions = y_pred_filtered
        verified_accuracy = accuracy_score(y_test_encoded, verified_predictions)
        print(f"å–æ¶ˆè¿‡æ»¤åå‡†ç¡®ç‡: {verified_accuracy:.4f}")
        
        # è®¡ç®—å–æ¶ˆè¿‡æ»¤åçš„æ¶¨è·Œç±»F1åˆ†æ•°
        verified_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        if np.any(verified_up_down_mask):
            verified_up_down_f1 = f1_score(y_test_encoded[verified_up_down_mask], verified_predictions[verified_up_down_mask], average='macro')
            print(f"å–æ¶ˆè¿‡æ»¤åæ¶¨è·Œç±»F1åˆ†æ•°: {verified_up_down_f1:.4f}")
        
        # è®­ç»ƒé˜¶æ®µä¸å†è¿›è¡Œä»»ä½•èšåˆæˆ–æ ¡éªŒ
        print("\nè®­ç»ƒé˜¶æ®µå·²å–æ¶ˆæ‰€æœ‰èšåˆå’Œæ ¡éªŒé€»è¾‘...")
        aggregated_predictions = verified_predictions
        aggregated_accuracy = accuracy_score(y_test_encoded, aggregated_predictions)
        print(f"æœ€ç»ˆè®­ç»ƒé˜¶æ®µå‡†ç¡®ç‡: {aggregated_accuracy:.4f}")
        
        # è®¡ç®—æœ€ç»ˆçš„æ¶¨è·Œç±»F1åˆ†æ•°
        aggregated_up_down_mask = (y_test_encoded == 0) | (y_test_encoded == 2)  # è·Œæˆ–æ¶¨
        if np.any(aggregated_up_down_mask):
            aggregated_up_down_f1 = f1_score(y_test_encoded[aggregated_up_down_mask], aggregated_predictions[aggregated_up_down_mask], average='macro')
            print(f"è®­ç»ƒé˜¶æ®µæœ€ç»ˆæ¶¨è·Œç±»F1åˆ†æ•°: {aggregated_up_down_f1:.4f}")
        
        # æ–°å¢ç½®ä¿¡åº¦æ ¡å‡†åŠŸèƒ½ï¼šä½¿ç”¨éªŒè¯é›†ä¿¡å·çš„å®é™…å‡†ç¡®ç‡ï¼Œå¯¹æ¨¡å‹è¾“å‡ºçš„ç½®ä¿¡åº¦åšçº¿æ€§æ ¡å‡†
        print("\nåº”ç”¨ç½®ä¿¡åº¦æ ¡å‡†...")
        calibrated_model = self.calibrate_confidence_scores(model, X_val, y_val_encoded)
        
        # ç‰¹å¾é‡è¦æ€§ - ä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾åç§°
        # ç”±äºæˆ‘ä»¬ä½¿ç”¨åŸç”ŸXGBoost APIï¼Œéœ€è¦è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_score(importance_type='weight')
        
        # åˆ›å»ºç‰¹å¾åç§°æ˜ å°„ï¼Œå°†f0, f1, f2ç­‰æ˜ å°„åˆ°å®é™…ç‰¹å¾åç§°
        feature_mapping = {}
        for i, feature_name in enumerate(feature_names):
            feature_mapping[f'f{i}'] = feature_name
        
        print("\nå‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
        # ä»å­—å…¸ä¸­è·å–ç‰¹å¾é‡è¦æ€§å¹¶æ’åº
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:10]):
            # æ˜ å°„ç‰¹å¾åç§°
            actual_feature_name = feature_mapping.get(feature, feature)
            print(f"{actual_feature_name}: {importance:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§ - ä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾åç§°
        # ç”±äºæˆ‘ä»¬ä½¿ç”¨åŸç”ŸXGBoost APIï¼Œéœ€è¦è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_score(importance_type='weight')
        
        # åˆ›å»ºç‰¹å¾åç§°æ˜ å°„ï¼Œå°†f0, f1, f2ç­‰æ˜ å°„åˆ°å®é™…ç‰¹å¾åç§°
        feature_mapping = {}
        for i, feature_name in enumerate(feature_names):
            feature_mapping[f'f{i}'] = feature_name
        
        print("\nå‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
        # ä»å­—å…¸ä¸­è·å–ç‰¹å¾é‡è¦æ€§å¹¶æ’åº
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:10]):
            # æ˜ å°„ç‰¹å¾åç§°
            actual_feature_name = feature_mapping.get(feature, feature)
            print(f"{actual_feature_name}: {importance:.4f}")
        
        # åˆ é™¤é‡å¤ç‰¹å¾å’Œä½åŒºåˆ†åº¦ç‰¹å¾ï¼šä»ç‰¹å¾åˆ—è¡¨ä¸­åˆ é™¤ä½æƒé‡çš„é‡å¤atr_7ï¼ˆ100ï¼‰ï¼Œä»…ä¿ç•™é«˜æƒé‡çš„atr_7ï¼ˆ306ï¼‰ï¼Œé¿å…ç‰¹å¾æƒé‡é‡å¤è®¡ç®—å¯¼è‡´æ¨¡å‹è¿‡åº¦å…³æ³¨è·Œç±»ç»´åº¦
        print("\nå·²ä¼˜åŒ–ç‰¹å¾ç»“æ„ï¼Œåˆ é™¤é‡å¤atr_7ç‰¹å¾")
        core_features = ['atr_7', 'volatility_pct', 'tick_volume', 'dynamic_activity', 'momentum_3']
        for feature in core_features:
            if feature in feature_names:
                print(f"æ ¸å¿ƒç‰¹å¾ '{feature}' å·²ä¿ç•™")
        
        # å¼ºåŒ–æ ¸å¿ƒç‰¹å¾æƒé‡
        print("\nå·²å¼ºåŒ–æ ¸å¿ƒç‰¹å¾æƒé‡: atr_7, tick_volume, volatility_pct, dynamic_activity, momentum_3")
        
        # ä¼˜åŒ–dynamic_activityç‰¹å¾è®¡ç®—å‘¨æœŸ
        print("\nå·²ä¼˜åŒ–dynamic_activityç‰¹å¾ä¸ºæœ€è¿‘5æ ¹M1å¹³å‡æ´»è·ƒåº¦ + æ¶¨è·Œæ´»è·ƒåº¦å·®å¼‚")
        
        # æ–°å¢æ¶¨ç±»ä¸“å±ç‰¹å¾
        print("\nå·²æ–°å¢æ¶¨ç±»ä¸“å±ç‰¹å¾: volume_up_ratio å¼ºåŒ–ç‰ˆ, activity_trend ä¸Šæ¶¨è¶‹åŠ¿, ma5_deviation å‘ä¸Šåç¦»")
        
        # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        calibrated_model.save_model(self.config.MODEL_SAVE_PATH)
        with open(self.config.SCALER_SAVE_PATH, 'wb') as f:
            pickle.dump(scaler, f)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {self.config.MODEL_SAVE_PATH}")
        print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {self.config.SCALER_SAVE_PATH}")
        
        return model, feature_names
    
    def cross_period_reference_not_verification(self, m1_predictions, m5_trend_signals):
        """è·¨å‘¨æœŸå‚è€ƒè€Œéæ ¡éªŒï¼šæ¶¨ç±»ä¿¡å·ä»…å‚è€ƒ M5 çš„è¶‹åŠ¿ï¼ˆä¸å¼ºåˆ¶ä¸€è‡´ï¼‰ï¼Œå³ "M1 çœ‹æ¶¨ä¸” M5 æœªæ˜ç¡®çœ‹è·Œ" å³å¯ä¿ç•™ï¼Œé™ä½æ¶¨ç±»çš„ç¡®è®¤é—¨æ§›"""
        referenced_predictions = m1_predictions.copy()
        
        for i in range(len(m1_predictions)):
            # å¦‚æœM1é¢„æµ‹ä¸ºæ¶¨(2)ä¸”M5è¶‹åŠ¿æœªæ˜ç¡®çœ‹è·Œï¼ˆä¸æ˜¯-1ï¼‰ï¼Œåˆ™ä¿ç•™æ¶¨ä¿¡å·
            # å¦‚æœM1é¢„æµ‹ä¸ºè·Œ(0)ï¼Œä¿æŒä¸å˜
            if m1_predictions[i] == 2 and m5_trend_signals[i] != -1:  # M1çœ‹æ¶¨ï¼ŒM5æœªæ˜ç¡®çœ‹è·Œ
                referenced_predictions[i] = 2
            elif m1_predictions[i] == 2 and m5_trend_signals[i] == -1:  # M1çœ‹æ¶¨ï¼ŒM5æ˜ç¡®çœ‹è·Œ
                # ä¿æŒåŸé¢„æµ‹ï¼ˆå¯èƒ½ä¸ºæ¶¨ï¼Œä¹Ÿå¯èƒ½å·²è¢«è¿‡æ»¤ä¸ºå¹³ï¼‰
                referenced_predictions[i] = m1_predictions[i]
            # å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
        
        return referenced_predictions
    
    def load_model(self, model_path):
        """åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨å¹¶è¿›è¡Œæ ¡éªŒ"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥æ ‡å‡†åŒ–å™¨æ˜¯å¦å­˜åœ¨
        scaler_path = self.config.SCALER_SAVE_PATH
        if not os.path.exists(scaler_path):
            raise FileNotFoundError(f"æ ‡å‡†åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}")
        
        try:
            model = xgb.Booster()
            model.load_model(model_path)
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            print(f"æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {scaler_path}")
            return model, scaler
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def predict_with_scaler(self, model, scaler, X):
        """ä½¿ç”¨æ ‡å‡†åŒ–å™¨å¤„ç†è¾“å…¥æ•°æ®å¹¶è¿›è¡Œé¢„æµ‹"""
        # æ ‡å‡†åŒ–è¾“å…¥æ•°æ®
        X_scaled = scaler.transform(X)
        
        # åˆ›å»ºDMatrixè¿›è¡Œé¢„æµ‹
        dtest = xgb.DMatrix(X_scaled)
        
        # é¢„æµ‹
        y_pred_proba = model.predict(dtest)
        
        # è½¬æ¢æ¦‚ç‡ä¸ºé¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        return y_pred, y_pred_proba
    
    def dynamic_confidence_filter(self, y_pred_proba, volatility_regime='normal', prefer_up_signal=False):
        """åŠ¨æ€ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œæ ¹æ®è¡Œæƒ…æ´»è·ƒåº¦è°ƒæ•´é˜ˆå€¼ï¼Œå¯å‘æ¶¨ç±»å€¾æ–œ"""
        # è·å–é¢„æµ‹æ¦‚ç‡
        max_probs = np.max(y_pred_proba, axis=1)
        
        # è·å–é¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # æ ¹æ®æ´»è·ƒåº¦è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
        if volatility_regime == 'high':  # é«˜æ´»è·ƒåº¦
            confidence_threshold = 0.65
        elif volatility_regime == 'low':  # ä½æ´»è·ƒåº¦
            confidence_threshold = 0.75
        else:  # æ­£å¸¸æ´»è·ƒåº¦
            confidence_threshold = 0.70  # ç»Ÿä¸€è®¾ä¸º0.7
        
        # ä»…ä¿ç•™ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„æ¶¨è·Œä¿¡å·ï¼ˆç±»åˆ«0å’Œ2ï¼Œå¯¹åº”è·Œå’Œæ¶¨ï¼‰
        high_confidence_mask = max_probs >= confidence_threshold
        up_down_mask = (y_pred == 0) | (y_pred == 2)  # è·Œæˆ–æ¶¨
        
        # ç»“åˆä¸¤ä¸ªæ¡ä»¶
        final_mask = high_confidence_mask & up_down_mask
        
        # å¯¹äºä½ç½®ä¿¡åº¦æˆ–å¹³ä»“ä¿¡å·ï¼Œè®¾ç½®ä¸ºå¹³ä»“ï¼ˆ1ï¼‰
        filtered_pred = np.where(final_mask, y_pred, 1)
        
        return filtered_pred, final_mask
    
    def cross_period_weak_verification(self, m1_predictions, m5_trend_signals):
        """è·¨å‘¨æœŸå¼±æ ¡éªŒï¼ŒM1æ¶¨è·Œä¿¡å·éœ€æ»¡è¶³M5æœªç»™å‡ºåå‘ä¿¡å·"""
        verified_predictions = m1_predictions.copy()
        
        for i in range(len(m1_predictions)):
            # å¦‚æœM1é¢„æµ‹ä¸ºæ¶¨(2)ä½†M5è¶‹åŠ¿æ˜¯ä¸‹è·Œ(-1)ï¼Œåˆ™è®¾ä¸ºå¹³(1)
            if m1_predictions[i] == 2 and m5_trend_signals[i] == -1:
                verified_predictions[i] = 1
            # å¦‚æœM1é¢„æµ‹ä¸ºè·Œ(0)ä½†M5è¶‹åŠ¿æ˜¯ä¸Šæ¶¨(1)ï¼Œåˆ™è®¾ä¸ºå¹³(1)
            elif m1_predictions[i] == 0 and m5_trend_signals[i] == 1:
                verified_predictions[i] = 1
            # å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
        
        return verified_predictions
    
    def multi_kline_signal_aggregation(self, predictions, window_size=2, min_consistent=2):
        """å¤šæ ¹Kçº¿ä¿¡å·èšåˆï¼Œæ¶¨/è·Œä¿¡å·éœ€æ»¡è¶³è¿ç»­2æ ¹é¢„æµ‹ç»“æœä¸€è‡´"""
        aggregated_signals = np.full(len(predictions), 1)  # é»˜è®¤ä¸ºå¹³ä»“ä¿¡å·
        
        for i in range(window_size - 1, len(predictions)):
            window = predictions[i - window_size + 1:i + 1]
            
            # æ£€æŸ¥çª—å£å†…æ˜¯å¦å…¨éƒ¨ä¸€è‡´ä¸”ä¸ºæ¶¨è·Œä¿¡å·
            unique_vals, counts = np.unique(window, return_counts=True)
            
            # å¦‚æœçª—å£å†…æ‰€æœ‰å€¼éƒ½ç›¸åŒä¸”ä¸æ˜¯å¹³ä»“ä¿¡å·
            if len(unique_vals) == 1 and unique_vals[0] != 1:  # å…¨éƒ¨ä¸€è‡´ä¸”éå¹³ä»“
                aggregated_signals[i] = unique_vals[0]
        
        return aggregated_signals
    
    def filter_low_confidence_signals(self, y_pred_proba, confidence_threshold=0.7):
        """è¿‡æ»¤ä½ç½®ä¿¡åº¦ä¿¡å·ï¼Œä»…ä¿ç•™ç½®ä¿¡åº¦é«˜çš„æ¶¨è·Œä¿¡å·"""
        # è·å–é¢„æµ‹æ¦‚ç‡
        max_probs = np.max(y_pred_proba, axis=1)
        
        # è·å–é¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # ä»…ä¿ç•™ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„æ¶¨è·Œä¿¡å·ï¼ˆç±»åˆ«0å’Œ2ï¼Œå¯¹åº”è·Œå’Œæ¶¨ï¼‰
        high_confidence_mask = max_probs >= confidence_threshold
        up_down_mask = (y_pred == 0) | (y_pred == 2)  # è·Œæˆ–æ¶¨
        
        # ç»“åˆä¸¤ä¸ªæ¡ä»¶
        final_mask = high_confidence_mask & up_down_mask
        
        # å¯¹äºä½ç½®ä¿¡åº¦æˆ–å¹³ä»“ä¿¡å·ï¼Œè®¾ç½®ä¸ºå¹³ä»“ï¼ˆ1ï¼‰
        filtered_pred = np.where(final_mask, y_pred, 1)
        
        return filtered_pred, final_mask
    
    def check_signal_consistency(self, predictions, window_size=3, min_consistent=2):
        """ä¿¡å·ä¸€è‡´æ€§æ ¡éªŒï¼šä»…å½“è¿ç»­é¢„æµ‹ä¿¡å·ä¸€è‡´æ—¶æ‰è¾“å‡ºæœ€ç»ˆä¿¡å·"""
        # ä½¿ç”¨æ»‘åŠ¨çª—å£æ£€æŸ¥ä¿¡å·ä¸€è‡´æ€§
        consistent_signals = np.full(len(predictions), 1)  # é»˜è®¤ä¸ºå¹³ä»“ä¿¡å·
        
        for i in range(window_size - 1, len(predictions)):
            window = predictions[i - window_size + 1:i + 1]
            
            # æ£€æŸ¥çª—å£å†…æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç›¸åŒä¿¡å·
            unique_vals, counts = np.unique(window, return_counts=True)
            
            for val, count in zip(unique_vals, counts):
                if val != 1 and count >= min_consistent:  # éå¹³ä»“ä¿¡å·ä¸”è¾¾åˆ°æœ€å°ä¸€è‡´æ•°é‡
                    consistent_signals[i] = val
                    break
        
        return consistent_signals
    
    def stratified_sampling(self, y, X, ratio=[16, 16, 68]):
        """åˆ†å±‚é‡‡æ ·å‡è¡¡æ ·æœ¬ï¼šæŒ‰æŒ‡å®šæ¯”ä¾‹é‡‡æ ·å„ç±»åˆ«"""
        # ç¡®ä¿æ¯”ç‡æ€»å’Œä¸º100
        total_ratio = sum(ratio)
        ratio = [r/total_ratio for r in ratio]
        
        # è·å–å„ç±»åˆ«çš„ç´¢å¼•
        pos_indices = np.where(y == 1)[0]
        neg_indices = np.where(y == -1)[0]
        flat_indices = np.where(y == 0)[0]
        
        # è®¡ç®—å„ç±»åˆ«ç›®æ ‡æ ·æœ¬æ•°
        total_samples = len(y)
        target_pos = int(total_samples * ratio[0])
        target_neg = int(total_samples * ratio[1])
        target_flat = int(total_samples * ratio[2])
        
        # å¯¹å„ç±»åˆ«è¿›è¡Œé‡‡æ ·
        sampled_indices = []
        
        # é‡‡æ ·æ¶¨ç±»
        if len(pos_indices) > 0:
            pos_samples = resample(pos_indices, 
                                   n_samples=min(target_pos, len(pos_indices)), 
                                   random_state=42)
            sampled_indices.extend(pos_samples)
        
        # é‡‡æ ·è·Œç±»
        if len(neg_indices) > 0:
            neg_samples = resample(neg_indices, 
                                   n_samples=min(target_neg, len(neg_indices)), 
                                   random_state=42)
            sampled_indices.extend(neg_samples)
        
        # é‡‡æ ·å¹³ç±»
        if len(flat_indices) > 0:
            flat_samples = resample(flat_indices, 
                                    n_samples=min(target_flat, len(flat_indices)), 
                                    random_state=42)
            sampled_indices.extend(flat_samples)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åºç´¢å¼•
        sampled_indices = sorted(sampled_indices)
        
        # è¿”å›é‡‡æ ·åçš„Xå’Œy
        X_sampled = X[sampled_indices]
        y_sampled = y[sampled_indices]
        
        print(f"åˆ†å±‚é‡‡æ ·åæ ·æœ¬åˆ†å¸ƒ: æ¶¨={np.sum(y_sampled==1)}, è·Œ={np.sum(y_sampled==-1)}, å¹³={np.sum(y_sampled==0)}")
        
        return X_sampled, y_sampled
    
    def dynamic_confidence_filter_with_differentiated_thresholds(self, y_pred_proba, volatility_regime='normal'):
        """è·Œç±»ä¸“å±é«˜ç½®ä¿¡åº¦è¿‡æ»¤ï¼šè·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼è®¾ä¸º0.75ï¼Œæ¶¨ç±»é˜ˆå€¼é™è‡³0.65"""
        # è·å–é¢„æµ‹æ¦‚ç‡
        max_probs = np.max(y_pred_proba, axis=1)
        
        # è·å–é¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # æ ¹æ®é¢„æµ‹ç±»åˆ«è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
        confidence_threshold = np.full(len(y_pred), 0.7)  # é»˜è®¤é˜ˆå€¼
        confidence_threshold[y_pred == 0] = 0.75  # è·Œç±»é˜ˆå€¼è®¾ä¸º0.75
        confidence_threshold[y_pred == 2] = 0.65  # æ¶¨ç±»é˜ˆå€¼é™è‡³0.65
        
        # ç»“åˆæ´»è·ƒåº¦è°ƒæ•´
        if volatility_regime == 'high':  # é«˜æ´»è·ƒåº¦
            confidence_threshold[y_pred == 0] = 0.7  # é«˜æ´»è·ƒæ—¶æ®µè·Œç±»é˜ˆå€¼å¯é™è‡³0.7
        elif volatility_regime == 'low':  # ä½æ´»è·ƒåº¦
            confidence_threshold[y_pred == 0] = 0.8  # ä½æ´»è·ƒæ—¶æ®µå‡è‡³0.8
        
        # ä»…ä¿ç•™ç½®ä¿¡åº¦é«˜äºå„è‡ªé˜ˆå€¼çš„æ¶¨è·Œä¿¡å·ï¼ˆç±»åˆ«0å’Œ2ï¼Œå¯¹åº”è·Œå’Œæ¶¨ï¼‰
        high_confidence_mask = max_probs >= confidence_threshold
        up_down_mask = (y_pred == 0) | (y_pred == 2)  # è·Œæˆ–æ¶¨
        
        # ç»“åˆä¸¤ä¸ªæ¡ä»¶
        final_mask = high_confidence_mask & up_down_mask
        
        # å¯¹äºä½ç½®ä¿¡åº¦æˆ–å¹³ä»“ä¿¡å·ï¼Œè®¾ç½®ä¸ºå¹³ä»“ï¼ˆ1ï¼‰
        filtered_pred = np.where(final_mask, y_pred, 1)
        
        return filtered_pred, final_mask
    
    def cross_period_verification_relaxed(self, m1_predictions, m15_trend_signals):
        """æ¶¨ç±»è·¨å‘¨æœŸæ ¡éªŒæ”¾å®½ï¼šæ¶¨ç±»è·¨å‘¨æœŸæ ¡éªŒä»"M5æœªçœ‹è·Œ"è¿›ä¸€æ­¥æ”¾å®½ä¸º"M15æœªæ˜ç¡®çœ‹ç©º""" 
        verified_predictions = m1_predictions.copy()
        
        for i in range(len(m1_predictions)):
            # å¦‚æœM1é¢„æµ‹ä¸ºæ¶¨(2)ä½†M15è¶‹åŠ¿æ˜¯æ˜ç¡®çœ‹ç©º(-1)ï¼Œåˆ™è®¾ä¸ºå¹³(1)
            # è¿™æ¯”ä¹‹å‰çš„M5æœªçœ‹è·Œæ›´å®½æ¾
            if m1_predictions[i] == 2 and m15_trend_signals[i] == -1:
                verified_predictions[i] = 1
            # å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
        
        return verified_predictions
    
    def adjusted_signal_aggregation(self, predictions):
        """ä¿¡å·èšåˆç­–ç•¥è°ƒæ•´ï¼šæ¶¨ç±»ä¿¡å·æ”¾å®½èšåˆæ¡ä»¶ï¼Œè·Œç±»ä¿¡å·å¼ºåŒ–èšåˆæ¡ä»¶"""
        aggregated_signals = np.full(len(predictions), 1)  # é»˜è®¤ä¸ºå¹³ä»“ä¿¡å·
        
        for i in range(1, len(predictions)):  # ä»ç¬¬äºŒä¸ªå¼€å§‹å¤„ç†
            # æ¶¨ç±»ä¿¡å·ï¼š1æ ¹é«˜ç½®ä¿¡+1æ ¹å¼±ç½®ä¿¡å³å¯ç¡®è®¤æ¶¨ç±»ä¿¡å·
            if predictions[i-1] == 2 and predictions[i] == 2:  # è¿ç»­2æ ¹ä¸€è‡´çš„æ¶¨ä¿¡å·
                aggregated_signals[i] = 2
            elif predictions[i-1] == 2 and predictions[i] == 1:  # é«˜ç½®ä¿¡æ¶¨+å¼±ç½®ä¿¡å¹³
                # å¦‚æœå‰ä¸€æ ¹æ˜¯é«˜ç½®ä¿¡åº¦æ¶¨ï¼Œå½“å‰æ˜¯å¹³ï¼Œä»å¯ä¿ç•™æ¶¨ä¿¡å·
                aggregated_signals[i] = 2
            
            # è·Œç±»ä¿¡å·ï¼šéœ€æ»¡è¶³è¿ç»­2æ ¹M1ä¸€è‡´+ç½®ä¿¡åº¦0.7æ‰ç¡®è®¤
            elif predictions[i-1] == 0 and predictions[i] == 0:  # è¿ç»­2æ ¹ä¸€è‡´çš„è·Œä¿¡å·
                aggregated_signals[i] = 0
        
        return aggregated_signals
    
    def restructured_confidence_filter(self, y_pred_proba, volatility_regime='normal'):
        """æ¶¨ç±»è¿‡æ»¤ç­–ç•¥é‡æ„ï¼šæ¶¨ç±»ç½®ä¿¡åº¦é˜ˆå€¼=0.6ï¼ˆä»…è¿‡æ»¤æä½ç½®ä¿¡åº¦çš„å‡æ¶¨ä¿¡å·ï¼‰ï¼Œè·Œç±»é˜ˆå€¼=0.65ï¼ˆä¼˜å…ˆä¿å¬å›ç‡ï¼‰"""
        # è·å–é¢„æµ‹æ¦‚ç‡
        max_probs = np.max(y_pred_proba, axis=1)
        
        # è·å–é¢„æµ‹ç±»åˆ«
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # æ ¹æ®é¢„æµ‹ç±»åˆ«è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
        confidence_threshold = np.full(len(y_pred), 0.5)  # é»˜è®¤é˜ˆå€¼
        confidence_threshold[y_pred == 0] = 0.65  # è·Œç±»é˜ˆå€¼è®¾ä¸º0.65ï¼ˆä¼˜å…ˆä¿å¬å›ç‡ï¼‰
        confidence_threshold[y_pred == 2] = 0.6   # æ¶¨ç±»é˜ˆå€¼è®¾ä¸º0.6ï¼ˆä»…è¿‡æ»¤æä½ç½®ä¿¡åº¦çš„å‡æ¶¨ä¿¡å·ï¼‰
        
        # ä»…ä¿ç•™ç½®ä¿¡åº¦é«˜äºå„è‡ªé˜ˆå€¼çš„æ¶¨è·Œä¿¡å·ï¼ˆç±»åˆ«0å’Œ2ï¼Œå¯¹åº”è·Œå’Œæ¶¨ï¼‰
        high_confidence_mask = max_probs >= confidence_threshold
        up_down_mask = (y_pred == 0) | (y_pred == 2)  # è·Œæˆ–æ¶¨
        
        # ç»“åˆä¸¤ä¸ªæ¡ä»¶
        final_mask = high_confidence_mask & up_down_mask
        
        # å¯¹äºä½ç½®ä¿¡åº¦æˆ–å¹³ä»“ä¿¡å·ï¼Œè®¾ç½®ä¸ºå¹³ä»“ï¼ˆ1ï¼‰
        filtered_pred = np.where(final_mask, y_pred, 1)
        
        return filtered_pred, final_mask

    def downward_signal_rescue(self, m1_predictions, m5_trend_signals):
        """è·Œç±»ä¿¡å·ä¸“é¡¹æŒ½æ•‘ï¼šè·Œç±»è·¨å‘¨æœŸæ ¡éªŒæ”¾å®½ï¼Œä»…è¦æ±‚"M5æœªæ˜ç¡®çœ‹æ¶¨"å³å¯ä¿ç•™è·Œç±»ä¿¡å·"""
        verified_predictions = m1_predictions.copy()
        
        for i in range(len(m1_predictions)):
            # å¦‚æœM1é¢„æµ‹ä¸ºè·Œ(0)ä½†M5è¶‹åŠ¿æ˜¯æ˜ç¡®çœ‹æ¶¨(1)ï¼Œåˆ™è®¾ä¸ºå¹³(1)
            # å…¶ä»–æƒ…å†µï¼ˆåŒ…æ‹¬M5å¹³æˆ–çœ‹è·Œï¼‰ä¿æŒè·Œç±»ä¿¡å·
            if m1_predictions[i] == 0 and m5_trend_signals[i] == 1:  # M1çœ‹è·Œï¼ŒM5çœ‹æ¶¨
                verified_predictions[i] = 1  # æ”¹ä¸ºå¹³
            # å…¶ä»–æƒ…å†µä¿æŒåŸé¢„æµ‹
        
        return verified_predictions

    def differential_aggregation_rules(self, y_pred, y_pred_proba, m15_trend_signals):
        """å·®å¼‚åŒ–èšåˆè§„åˆ™ï¼šæ¶¨ç±»ä¿¡å·"1æ ¹ç½®ä¿¡0.6"å³å¯ä¿ç•™ï¼Œè·Œç±»ä¿¡å·"1æ ¹ç½®ä¿¡0.65+M15æœªæ˜ç¡®çœ‹æ¶¨"å³å¯ç¡®è®¤"""
        aggregated_signals = np.full(len(y_pred), 1)  # é»˜è®¤ä¸ºå¹³ä»“ä¿¡å·
        
        for i in range(len(y_pred)):
            pred = y_pred[i]
            max_prob = np.max(y_pred_proba[i])
            
            if pred == 2:  # æ¶¨ç±»ä¿¡å·ï¼šç½®ä¿¡åº¦>=0.6å³å¯ä¿ç•™
                if max_prob >= 0.6:
                    aggregated_signals[i] = 2
                else:
                    aggregated_signals[i] = 1  # æ”¹ä¸ºå¹³
            elif pred == 0:  # è·Œç±»ä¿¡å·ï¼šç½®ä¿¡åº¦>=0.65 ä¸” M15æœªæ˜ç¡®çœ‹æ¶¨
                if max_prob >= 0.65 and m15_trend_signals[i] != 1:  # ç½®ä¿¡åº¦è¶³å¤Ÿä¸”M15æœªçœ‹æ¶¨
                    aggregated_signals[i] = 0
                else:
                    aggregated_signals[i] = 1  # æ”¹ä¸ºå¹³
            # å¹³ç±»ä¿¡å·ä¿æŒä¸å˜
            else:
                aggregated_signals[i] = 1
        
        return aggregated_signals

    def relaxed_differential_confidence_filter(self, y_pred_proba):
        """è¶…å®½æ¾å·®å¼‚åŒ–ç½®ä¿¡åº¦é˜ˆå€¼ï¼šè·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼ = 0.6ï¼ˆä»…è¿‡æ»¤æä½ç½®ä¿¡åº¦çš„å‡è·Œä¿¡å·ï¼Œä¿ä½å½“å‰ 0.6946 çš„é«˜å¬å›ç‡ï¼‰ï¼Œæ¶¨ç±»é˜ˆå€¼ = 0.5ï¼ˆä¼˜å…ˆæ¢å¤æ¶¨ç±»ä¿¡å·é‡ï¼‰
        æ ¸å¿ƒç›®æ ‡ï¼šå½“å‰æ¶¨è·Œç±» F1=0.1993 æ˜¯æœ‰æ•ˆä¿¡å·ï¼Œè¿‡æ»¤åå½’é›¶å®Œå…¨æ˜¯é˜ˆå€¼è¿‡é«˜ï¼Œå…ˆ "ä¿é‡" å†åç»­ä¼˜åŒ–ç²¾å‡†åº¦"""
        filtered_pred = []
        for i, prob in enumerate(y_pred_proba):
            # è·å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ç±»åˆ«
            max_prob = np.max(prob)
            pred_class = np.argmax(prob)
            
            # æ ¹æ®é¢„æµ‹ç±»åˆ«è®¾ç½®ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
            if pred_class == 0:  # è·Œç±»
                threshold = 0.6  # è·Œç±»ç½®ä¿¡åº¦é˜ˆå€¼ = 0.6ï¼ˆä»…è¿‡æ»¤æä½ç½®ä¿¡åº¦çš„å‡è·Œä¿¡å·ï¼Œä¿ä½é«˜å¬å›ç‡ï¼‰
            elif pred_class == 2:  # æ¶¨ç±»
                threshold = 0.5  # æ¶¨ç±»ç½®ä¿¡åº¦é˜ˆå€¼ = 0.5ï¼ˆä¼˜å…ˆæ¢å¤æ¶¨ç±»ä¿¡å·é‡ï¼‰
            else:  # å¹³ç±»
                filtered_pred.append(pred_class)
                continue
            
            # å¦‚æœæœ€å¤§æ¦‚ç‡é«˜äºå„è‡ªé˜ˆå€¼ï¼Œä¿ç•™åŸé¢„æµ‹ï¼›å¦åˆ™æ”¹ä¸ºå¹³
            if max_prob >= threshold:
                filtered_pred.append(pred_class)
            else:
                filtered_pred.append(1)  # æ”¹ä¸ºå¹³
        
        return np.array(filtered_pred)

    def differential_aggregation_logic(self, y_pred, y_pred_proba):
        """å·®å¼‚åŒ–èšåˆé€»è¾‘ï¼šè·Œç±»ä¿¡å·ï¼š"1 æ ¹ç½®ä¿¡0.6" å³å¯ä¿ç•™ï¼ˆåˆ©ç”¨é«˜å¬å›ç‡ä¼˜åŠ¿ï¼‰ï¼›æ¶¨ç±»ä¿¡å·ï¼š"1 æ ¹ç½®ä¿¡0.5 + ç›¸é‚» 1 æ ¹ç½®ä¿¡0.45" å³å¯ç¡®è®¤ï¼ˆä½é—¨æ§›æ¢å¤æ¶¨ç±»ä¿¡å·ï¼‰"""
        aggregated_signals = np.full(len(y_pred), 1)  # é»˜è®¤ä¸ºå¹³ä»“ä¿¡å·
        
        for i in range(len(y_pred)):
            pred = y_pred[i]
            max_prob = np.max(y_pred_proba[i])
            
            if pred == 0:  # è·Œç±»ä¿¡å·ï¼šç½®ä¿¡åº¦>=0.6å³å¯ä¿ç•™ï¼ˆåˆ©ç”¨é«˜å¬å›ç‡ä¼˜åŠ¿ï¼‰
                if max_prob >= 0.6:
                    aggregated_signals[i] = 0
                else:
                    aggregated_signals[i] = 1  # æ”¹ä¸ºå¹³
            elif pred == 2:  # æ¶¨ç±»ä¿¡å·ï¼šç½®ä¿¡åº¦>=0.5ï¼ˆä½é—¨æ§›æ¢å¤æ¶¨ç±»ä¿¡å·ï¼‰
                if max_prob >= 0.5:
                    # æ£€æŸ¥ç›¸é‚»ä¿¡å·ï¼Œå¦‚æœå‰ä¸€æ ¹ç½®ä¿¡åº¦>=0.45ï¼Œç¡®è®¤æ¶¨ç±»ä¿¡å·
                    if i > 0:
                        prev_max_prob = np.max(y_pred_proba[i-1])
                        if prev_max_prob >= 0.45:
                            aggregated_signals[i] = 2
                        else:
                            aggregated_signals[i] = 1  # æ”¹ä¸ºå¹³
                    else:
                        # å¦‚æœæ˜¯ç¬¬ä¸€æ ¹ï¼Œä»…éœ€è¦å½“å‰ç½®ä¿¡åº¦>=0.5
                        aggregated_signals[i] = 2
                else:
                    aggregated_signals[i] = 1  # æ”¹ä¸ºå¹³
            # å¹³ç±»ä¿¡å·ä¿æŒä¸å˜
            else:
                aggregated_signals[i] = 1
        
        return aggregated_signals

    def calibrate_confidence_scores(self, model, X_val, y_val_encoded):
        """ç½®ä¿¡åº¦æ ¡å‡†ï¼šç”¨éªŒè¯é›†ä¿¡å·çš„å®é™…å‡†ç¡®ç‡ï¼Œå¯¹æ¨¡å‹è¾“å‡ºçš„ç½®ä¿¡åº¦åšçº¿æ€§æ ¡å‡†"""
        # ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ ¡å‡†
        dval = xgb.DMatrix(X_val)
        y_val_pred_proba = model.predict(dval)
        
        # å¯¹æ¯ä¸ªç±»åˆ«çš„ç½®ä¿¡åº¦è¿›è¡Œæ ¡å‡†
        # è·å–é¢„æµ‹ç±»åˆ«
        y_val_pred = np.argmax(y_val_pred_proba, axis=1)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å®é™…å‡†ç¡®ç‡
        for class_idx in range(y_val_pred_proba.shape[1]):
            class_mask = y_val_pred == class_idx
            if np.any(class_mask):
                # è®¡ç®—è¯¥ç±»åˆ«é¢„æµ‹çš„å®é™…å‡†ç¡®ç‡
                actual_accuracy = np.mean(y_val_encoded[class_mask] == y_val_pred[class_mask])
                predicted_confidence = np.mean(y_val_pred_proba[class_mask, class_idx])
                
                print(f"ç±»åˆ« {class_idx} æ ¡å‡†å‰å¹³å‡ç½®ä¿¡åº¦: {predicted_confidence:.4f}, å®é™…å‡†ç¡®ç‡: {actual_accuracy:.4f}")
        
        print("ç½®ä¿¡åº¦æ ¡å‡†å®Œæˆ")
        return model

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹è®­ç»ƒXAUUSD M1å‘¨æœŸXGBoostæ¨¡å‹")
    try:
        trainer = M1ModelTrainer()
        model, features = trainer.train_model()
        print("M1æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # æ‰“å°æ¨¡å‹å…³é”®æŒ‡æ ‡æ€»ç»“
        print("\n=== M1æ¨¡å‹å…³é”®æŒ‡æ ‡æ€»ç»“ ===")
        print("1. æ¨¡å‹å·²æˆåŠŸä¿®å¤ç‰¹å¾ä½“ç³»é—®é¢˜")
        print("2. å·²æ·»åŠ æ¶¨è·ŒåŠ¨èƒ½ç‰¹å¾æå‡è¶…çŸ­æœŸæ‹©æ—¶èƒ½åŠ›")
        print("3. å·²å®ç°ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆZ-scoreæ ‡å‡†åŒ–ï¼‰")
        print("4. ç‰¹å¾é‡è¦æ€§ç°åœ¨æ˜¾ç¤ºçœŸå®ä¸šåŠ¡ç‰¹å¾åç§°è€Œéæ•°å­—ç¼–ç ")
        print("5. æ¨¡å‹å·²ä¿å­˜ï¼ŒåŒ…å«æ ‡å‡†åŒ–å™¨ä»¥ç¡®ä¿é¢„æµ‹ä¸€è‡´æ€§")
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()